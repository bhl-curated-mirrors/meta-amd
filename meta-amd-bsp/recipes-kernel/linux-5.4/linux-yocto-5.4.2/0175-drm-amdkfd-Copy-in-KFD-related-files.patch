From 34281a6e518354ad857fac85c65dd5ea96663c02 Mon Sep 17 00:00:00 2001
From: Kent Russell <kent.russell@amd.com>
Date: Tue, 21 May 2019 13:55:53 -0400
Subject: [PATCH 0175/1453] drm/amdkfd: Copy in KFD-related files

    This includes kernel configs, includes, and
    amdgpu/*kfd* files

    Change-Id: If18b468aeb974b19f1bad4ea4271fe3a29b1551d
    Signed-off-by: Kent Russell <kent.russell@amd.com>

Change-Id: Ib0f1b9dd7e12d8dff066e6cb21e03a69409d4931
Signed-off-by: Kalyan Alle <kalyan.alle@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c    |   32 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h    |   16 +-
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c |    2 +-
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c |   96 +-
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h |   62 +
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c |  197 ++-
 .../gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c  |  289 +++-
 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c       |   24 +
 drivers/gpu/drm/amd/amdgpu/amdgpu_object.h    |    1 +
 drivers/gpu/drm/amd/amdkfd/Kconfig            |    2 +-
 drivers/gpu/drm/amd/amdkfd/Makefile           |    6 +-
 .../gpu/drm/amd/amdkfd/cwsr_trap_handler.h    |    6 +-
 drivers/gpu/drm/amd/amdkfd/kfd_chardev.c      | 1259 ++++++++++++++++-
 drivers/gpu/drm/amd/amdkfd/kfd_crat.c         |   14 +-
 drivers/gpu/drm/amd/amdkfd/kfd_crat.h         |    2 +
 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c       |  108 +-
 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h       |   30 +
 drivers/gpu/drm/amd/amdkfd/kfd_device.c       |    2 +
 .../drm/amd/amdkfd/kfd_device_queue_manager.c |  388 ++++-
 .../drm/amd/amdkfd/kfd_device_queue_manager.h |   14 +
 drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c     |  116 +-
 drivers/gpu/drm/amd/amdkfd/kfd_events.c       |   41 +-
 drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c  |    9 +-
 drivers/gpu/drm/amd/amdkfd/kfd_ipc.c          |  270 ++++
 drivers/gpu/drm/amd/amdkfd/kfd_ipc.h          |   51 +
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c |   34 +
 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h |   10 +
 .../gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c  |    6 +
 drivers/gpu/drm/amd/amdkfd/kfd_module.c       |    8 +
 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h  |    2 +
 .../gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c  |   33 +
 .../gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c   |   93 +-
 .../gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c   |   47 +-
 drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c   |  523 +++++++
 .../gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h   |   11 +-
 .../gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h  |   97 --
 drivers/gpu/drm/amd/amdkfd/kfd_priv.h         |  150 +-
 drivers/gpu/drm/amd/amdkfd/kfd_process.c      |  389 ++++-
 .../amd/amdkfd/kfd_process_queue_manager.c    |    9 +
 drivers/gpu/drm/amd/amdkfd/kfd_rdma.c         |  297 ++++
 drivers/gpu/drm/amd/amdkfd/kfd_topology.c     |   99 +-
 drivers/gpu/drm/amd/amdkfd/kfd_topology.h     |   12 +-
 drivers/gpu/drm/amd/amdkfd/kfd_trace.c        |   26 +
 drivers/gpu/drm/amd/amdkfd/kfd_trace.h        |  151 ++
 .../gpu/drm/amd/include/kgd_kfd_interface.h   |   17 +
 include/drm/amd_rdma.h                        |   70 +
 include/linux/mmu_notifier.h                  |    3 +
 include/uapi/drm/amdgpu_drm.h                 |    2 +
 include/uapi/linux/kfd_ioctl.h                |  142 +-
 kernel/fork.c                                 |    1 +
 mm/mmu_notifier.c                             |   12 +
 security/device_cgroup.c                      |    1 +
 52 files changed, 4950 insertions(+), 332 deletions(-)
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
 create mode 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/Kconfig
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/Makefile
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_crat.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_crat.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_device.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_events.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_module.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_priv.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_process.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_topology.c
 mode change 100644 => 100755 drivers/gpu/drm/amd/amdkfd/kfd_topology.h
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_trace.c
 create mode 100755 drivers/gpu/drm/amd/amdkfd/kfd_trace.h
 mode change 100644 => 100755 drivers/gpu/drm/amd/include/kgd_kfd_interface.h
 create mode 100755 include/drm/amd_rdma.h
 mode change 100644 => 100755 include/uapi/drm/amdgpu_drm.h
 mode change 100644 => 100755 include/uapi/linux/kfd_ioctl.h

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
old mode 100644
new mode 100755
index 4a66e7ae0343..7dbdb3d3a45b
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.c
@@ -21,7 +21,9 @@
  */
 
 #include "amdgpu_amdkfd.h"
+#include <linux/dma-buf.h>
 #include "amd_shared.h"
+#include <drm/drmP.h>
 
 #include "amdgpu.h"
 #include "amdgpu_gfx.h"
@@ -30,13 +32,15 @@
 #include <linux/dma-buf.h>
 #include "amdgpu_xgmi.h"
 
-static const unsigned int compute_vmid_bitmap = 0xFF00;
+static unsigned int compute_vmid_bitmap = 0xFF00;
 
 /* Total memory size in system memory and all GPU VRAM. Used to
  * estimate worst case amount of memory to reserve for page tables
  */
 uint64_t amdgpu_amdkfd_total_mem_size;
 
+extern bool pcie_p2p;
+
 int amdgpu_amdkfd_init(void)
 {
 	struct sysinfo si;
@@ -204,7 +208,7 @@ void amdgpu_amdkfd_device_init(struct amdgpu_device *adev)
 					adev->doorbell_index.last_non_cp;
 		}
 
-		kgd2kfd_device_init(adev->kfd.dev, &gpu_resources);
+		kgd2kfd_device_init(adev->kfd.dev, adev->ddev, &gpu_resources);
 	}
 }
 
@@ -267,6 +271,14 @@ void amdgpu_amdkfd_gpu_reset(struct kgd_dev *kgd)
 		amdgpu_device_gpu_recover(adev, NULL);
 }
 
+u32 pool_to_domain(enum kgd_memory_pool p)
+{
+	switch (p) {
+	case KGD_POOL_FRAMEBUFFER: return AMDGPU_GEM_DOMAIN_VRAM;
+	default: return AMDGPU_GEM_DOMAIN_GTT;
+	}
+}
+
 int amdgpu_amdkfd_alloc_gtt_mem(struct kgd_dev *kgd, size_t size,
 				void **mem_obj, uint64_t *gpu_addr,
 				void **cpu_ptr, bool mqd_gfx9)
@@ -424,13 +436,17 @@ uint32_t amdgpu_amdkfd_get_fw_version(struct kgd_dev *kgd,
 void amdgpu_amdkfd_get_local_mem_info(struct kgd_dev *kgd,
 				      struct kfd_local_mem_info *mem_info)
 {
+	uint64_t address_mask;
+	resource_size_t aper_limit;
 	struct amdgpu_device *adev = (struct amdgpu_device *)kgd;
-	uint64_t address_mask = adev->dev->dma_mask ? ~*adev->dev->dma_mask :
+
+	address_mask = adev->dev->dma_mask ? ~*adev->dev->dma_mask :
 					     ~((1ULL << 32) - 1);
-	resource_size_t aper_limit = adev->gmc.aper_base + adev->gmc.aper_size;
+	aper_limit = adev->gmc.aper_base + adev->gmc.aper_size;
 
 	memset(mem_info, 0, sizeof(*mem_info));
-	if (!(adev->gmc.aper_base & address_mask || aper_limit & address_mask)) {
+	if (pcie_p2p && !(adev->gmc.aper_base & address_mask ||
+			  aper_limit & address_mask)) {
 		mem_info->local_mem_size_public = adev->gmc.visible_vram_size;
 		mem_info->local_mem_size_private = adev->gmc.real_vram_size -
 				adev->gmc.visible_vram_size;
@@ -686,6 +702,11 @@ bool amdgpu_amdkfd_have_atomics_support(struct kgd_dev *kgd)
 	return adev->have_atomics_support;
 }
 
+void amdgpu_amdkfd_debug_mem_fence(struct kgd_dev *kgd)
+{
+	amdgpu_asic_flush_hdp((struct amdgpu_device *) kgd, NULL);
+}
+
 #ifndef CONFIG_HSA_AMD
 bool amdkfd_fence_check_mm(struct dma_fence *f, struct mm_struct *mm)
 {
@@ -744,6 +765,7 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd, struct pci_dev *pdev,
 }
 
 bool kgd2kfd_device_init(struct kfd_dev *kfd,
+			 struct drm_device *ddev,
 			 const struct kgd2kfd_shared_resources *gpu_resources)
 {
 	return false;
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
old mode 100644
new mode 100755
index 174b2bc40a95..2a6d2a4b4039
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd.h
@@ -143,6 +143,10 @@ struct kfd2kgd_calls *amdgpu_amdkfd_gfx_8_0_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_9_0_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_arcturus_get_functions(void);
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_10_0_get_functions(void);
+int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
+		uint64_t src_offset, struct kgd_mem *dst_mem,
+		uint64_t dest_offset, uint64_t size, struct dma_fence **f,
+		uint64_t *actual_size);
 
 bool amdgpu_amdkfd_is_kfd_vmid(struct amdgpu_device *adev, u32 vmid);
 
@@ -218,7 +222,7 @@ void amdgpu_amdkfd_gpuvm_release_process_vm(struct kgd_dev *kgd, void *vm);
 uint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm);
 int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		struct kgd_dev *kgd, uint64_t va, uint64_t size,
-		void *vm, struct kgd_mem **mem,
+		void *vm, struct sg_table *sg, struct kgd_mem **mem,
 		uint64_t *offset, uint32_t flags);
 int amdgpu_amdkfd_gpuvm_free_memory_of_gpu(
 		struct kgd_dev *kgd, struct kgd_mem *mem);
@@ -236,14 +240,23 @@ int amdgpu_amdkfd_gpuvm_restore_process_bos(void *process_info,
 int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
 					      struct kfd_vm_fault_info *info);
 
+int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
+		struct kgd_mem *mem, uint64_t offset,
+		uint64_t size, struct sg_table **ret_sg);
+void amdgpu_amdkfd_gpuvm_unpin_put_sg_table(
+		struct kgd_mem *mem, struct sg_table *sg);
 int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 				      struct dma_buf *dmabuf,
 				      uint64_t va, void *vm,
 				      struct kgd_mem **mem, uint64_t *size,
 				      uint64_t *mmap_offset);
+int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
+				      struct kgd_mem *mem,
+				      struct dma_buf **dmabuf);
 
 void amdgpu_amdkfd_gpuvm_init_mem_limits(void);
 void amdgpu_amdkfd_unreserve_memory_limit(struct amdgpu_bo *bo);
+void amdgpu_amdkfd_debug_mem_fence(struct kgd_dev *kgd);
 
 /* KGD2KFD callbacks */
 int kgd2kfd_init(void);
@@ -252,6 +265,7 @@ struct kfd_dev *kgd2kfd_probe(struct kgd_dev *kgd, struct pci_dev *pdev,
 			      const struct kfd2kgd_calls *f2g,
 			      unsigned int asic_type, bool vf);
 bool kgd2kfd_device_init(struct kfd_dev *kfd,
+			 struct drm_device *ddev,
 			 const struct kgd2kfd_shared_resources *gpu_resources);
 void kgd2kfd_device_exit(struct kfd_dev *kfd);
 void kgd2kfd_suspend(struct kfd_dev *kfd);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
old mode 100644
new mode 100755
index 5f459bf5f622..c6abcf72e822
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v7.c
@@ -23,7 +23,7 @@
 #include <linux/fdtable.h>
 #include <linux/uaccess.h>
 #include <linux/mmu_context.h>
-
+#include <drm/drmP.h>
 #include "amdgpu.h"
 #include "amdgpu_amdkfd.h"
 #include "cikd.h"
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
old mode 100644
new mode 100755
index 6d2f61449606..d069eab0c137
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.c
@@ -24,10 +24,10 @@
 #include <linux/fdtable.h>
 #include <linux/uaccess.h>
 #include <linux/mmu_context.h>
-
+#include <drm/drmP.h>
 #include "amdgpu.h"
 #include "amdgpu_amdkfd.h"
-#include "gfx_v8_0.h"
+#include "amdgpu_amdkfd_gfx_v8.h"
 #include "gca/gfx_8_0_sh_mask.h"
 #include "gca/gfx_8_0_d.h"
 #include "gca/gfx_8_0_enum.h"
@@ -44,6 +44,13 @@ enum hqd_dequeue_request_type {
 	RESET_WAVES
 };
 
+static const uint32_t watchRegs[MAX_WATCH_ADDRESSES * ADDRESS_WATCH_REG_MAX] = {
+	mmTCP_WATCH0_ADDR_H, mmTCP_WATCH0_ADDR_L, mmTCP_WATCH0_CNTL,
+	mmTCP_WATCH1_ADDR_H, mmTCP_WATCH1_ADDR_L, mmTCP_WATCH1_CNTL,
+	mmTCP_WATCH2_ADDR_H, mmTCP_WATCH2_ADDR_L, mmTCP_WATCH2_CNTL,
+	mmTCP_WATCH3_ADDR_H, mmTCP_WATCH3_ADDR_L, mmTCP_WATCH3_CNTL
+};
+
 /*
  * Register access functions
  */
@@ -693,6 +700,22 @@ static uint16_t get_atc_vmid_pasid_mapping_pasid(struct kgd_dev *kgd,
 
 static int kgd_address_watch_disable(struct kgd_dev *kgd)
 {
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	union TCP_WATCH_CNTL_BITS cntl;
+	unsigned int i;
+
+	cntl.u32All = 0;
+
+	cntl.bitfields.valid = 0;
+	cntl.bitfields.mask = ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK;
+	cntl.bitfields.atc = 1;
+
+	/* Turning off this address until we set all the registers */
+	for (i = 0; i < MAX_WATCH_ADDRESSES; i++)
+		WREG32(watchRegs[i * ADDRESS_WATCH_REG_MAX
+				+ ADDRESS_WATCH_REG_CNTL],
+				cntl.u32All);
+
 	return 0;
 }
 
@@ -702,6 +725,32 @@ static int kgd_address_watch_execute(struct kgd_dev *kgd,
 					uint32_t addr_hi,
 					uint32_t addr_lo)
 {
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	union TCP_WATCH_CNTL_BITS cntl;
+
+	cntl.u32All = cntl_val;
+
+	/* Turning off this watch point until we set all the registers */
+	cntl.bitfields.valid = 0;
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX
+			+ ADDRESS_WATCH_REG_CNTL],
+			cntl.u32All);
+
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX
+			+ ADDRESS_WATCH_REG_ADDR_HI],
+			addr_hi);
+
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX
+			+ ADDRESS_WATCH_REG_ADDR_LO],
+			addr_lo);
+
+	/* Enable the watch point */
+	cntl.bitfields.valid = 1;
+
+	WREG32(watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX
+			+ ADDRESS_WATCH_REG_CNTL],
+			cntl.u32All);
+
 	return 0;
 }
 
@@ -734,7 +783,7 @@ static uint32_t kgd_address_watch_get_offset(struct kgd_dev *kgd,
 					unsigned int watch_point_id,
 					unsigned int reg_offset)
 {
-	return 0;
+	return watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + reg_offset];
 }
 
 static void set_scratch_backing_va(struct kgd_dev *kgd,
@@ -760,6 +809,38 @@ static void set_vm_context_page_table_base(struct kgd_dev *kgd, uint32_t vmid,
 			lower_32_bits(page_table_base));
 }
 
+/*
+ * FIXME: Poliars test failed with this package, FIJI works fine
+ * From the CP spec it does not official support the invalidation
+ * with the specified pasid in the package,  so disable it for V8
+ *
+ */
+#ifdef V8_SUPPORT_IT_OFFICIAL
+static int invalidate_tlbs_with_kiq(struct amdgpu_device *adev, uint16_t pasid)
+{
+	signed long r;
+	uint32_t seq;
+	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
+
+	spin_lock(&adev->gfx.kiq.ring_lock);
+	amdgpu_ring_alloc(ring, 12); /* fence + invalidate_tlbs package*/
+	amdgpu_ring_write(ring, PACKET3(PACKET3_INVALIDATE_TLBS, 0));
+	amdgpu_ring_write(ring,
+			PACKET3_INVALIDATE_TLBS_DST_SEL(1) |
+			PACKET3_INVALIDATE_TLBS_PASID(pasid));
+	amdgpu_fence_emit_polling(ring, &seq);
+	amdgpu_ring_commit(ring);
+	spin_unlock(&adev->gfx.kiq.ring_lock);
+
+	r = amdgpu_fence_wait_polling(ring, seq, adev->usec_timeout);
+	if (r < 1) {
+		DRM_ERROR("wait for kiq fence error: %ld.\n", r);
+		return -ETIME;
+	}
+
+	return 0;
+}
+#endif
 static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 {
 	struct amdgpu_device *adev = (struct amdgpu_device *) kgd;
@@ -769,6 +850,13 @@ static int invalidate_tlbs(struct kgd_dev *kgd, uint16_t pasid)
 	if (adev->in_gpu_reset)
 		return -EIO;
 
+#ifdef V8_SUPPORT_IT_OFFICIAL
+	struct amdgpu_ring *ring = &adev->gfx.kiq.ring;
+
+	if (ring->ready)
+		return invalidate_tlbs_with_kiq(adev, pasid);
+#endif
+
 	for (vmid = 0; vmid < 16; vmid++) {
 		if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid))
 			continue;
@@ -791,7 +879,7 @@ static int invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
 
 	if (!amdgpu_amdkfd_is_kfd_vmid(adev, vmid)) {
 		pr_err("non kfd vmid %d\n", vmid);
-		return -EINVAL;
+		return 0;
 	}
 
 	WREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
new file mode 100755
index 000000000000..3c9491938f0c
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v8.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#ifndef AMDGPU_AMDKFD_GFX_V8_H_INCLUDED
+#define AMDGPU_AMDKFD_GFX_V8_H_INCLUDED
+
+#include <linux/types.h>
+
+enum {
+	MAX_TRAPID = 8,		/* 3 bits in the bitfield. */
+	MAX_WATCH_ADDRESSES = 4
+};
+
+enum {
+	ADDRESS_WATCH_REG_ADDR_HI = 0,
+	ADDRESS_WATCH_REG_ADDR_LO,
+	ADDRESS_WATCH_REG_CNTL,
+	ADDRESS_WATCH_REG_MAX
+};
+
+/*  not defined in the VI reg file  */
+enum {
+	ADDRESS_WATCH_REG_CNTL_ATC_BIT = 0x10000000UL,
+	ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK = 0x00FFFFFF,
+	ADDRESS_WATCH_REG_ADDLOW_MASK_EXTENSION = 0x03000000,
+	/* extend the mask to 26 bits in order to match the low address field */
+	ADDRESS_WATCH_REG_ADDLOW_SHIFT = 6,
+	ADDRESS_WATCH_REG_ADDHIGH_MASK = 0xFFFF
+};
+
+union TCP_WATCH_CNTL_BITS {
+	struct {
+		uint32_t mask:24;
+		uint32_t vmid:4;
+		uint32_t atc:1;
+		uint32_t mode:2;
+		uint32_t valid:1;
+	} bitfields, bits;
+	uint32_t u32All;
+	signed int i32All;
+	float f32All;
+};
+#endif
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
old mode 100644
new mode 100755
index e262f2ac07a3..7822f217b21a
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gfx_v9.c
@@ -26,9 +26,10 @@
 #include <linux/fdtable.h>
 #include <linux/uaccess.h>
 #include <linux/mmu_context.h>
-
+#include <drm/drmP.h>
 #include "amdgpu.h"
 #include "amdgpu_amdkfd.h"
+#include "amdgpu_amdkfd_gfx_v8.h"
 #include "soc15_hw_ip.h"
 #include "gc/gc_9_0_offset.h"
 #include "gc/gc_9_0_sh_mask.h"
@@ -59,6 +60,12 @@ enum hqd_dequeue_request_type {
 	RESET_WAVES
 };
 
+static const uint32_t watchRegs[MAX_WATCH_ADDRESSES * ADDRESS_WATCH_REG_MAX] = {
+	mmTCP_WATCH0_ADDR_H, mmTCP_WATCH0_ADDR_L, mmTCP_WATCH0_CNTL,
+	mmTCP_WATCH1_ADDR_H, mmTCP_WATCH1_ADDR_L, mmTCP_WATCH1_CNTL,
+	mmTCP_WATCH2_ADDR_H, mmTCP_WATCH2_ADDR_L, mmTCP_WATCH2_CNTL,
+	mmTCP_WATCH3_ADDR_H, mmTCP_WATCH3_ADDR_L, mmTCP_WATCH3_CNTL
+};
 
 /* Because of REG_GET_FIELD() being used, we put this function in the
  * asic specific file.
@@ -246,6 +253,16 @@ static uint32_t get_sdma_base_addr(struct amdgpu_device *adev,
 	return retval;
 }
 
+static uint32_t get_watch_base_addr(struct amdgpu_device *adev)
+{
+	uint32_t retval = SOC15_REG_OFFSET(GC, 0, mmTCP_WATCH0_ADDR_H) -
+			mmTCP_WATCH0_ADDR_H;
+
+	pr_debug("kfd: reg watch base address: 0x%x\n", retval);
+
+	return retval;
+}
+
 static inline struct v9_mqd *get_mqd(void *mqd)
 {
 	return (struct v9_mqd *)mqd;
@@ -733,6 +750,25 @@ int kgd_gfx_v9_invalidate_tlbs_vmid(struct kgd_dev *kgd, uint16_t vmid)
 
 int kgd_gfx_v9_address_watch_disable(struct kgd_dev *kgd)
 {
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	union TCP_WATCH_CNTL_BITS cntl;
+	unsigned int i;
+	uint32_t watch_base_addr;
+
+	cntl.u32All = 0;
+
+	cntl.bitfields.valid = 0;
+	cntl.bitfields.mask = ADDRESS_WATCH_REG_CNTL_DEFAULT_MASK;
+	cntl.bitfields.atc = 1;
+
+	watch_base_addr = get_watch_base_addr(adev);
+	/* Turning off this address until we set all the registers */
+	for (i = 0; i < MAX_WATCH_ADDRESSES; i++)
+		WREG32(watch_base_addr +
+				watchRegs[i * ADDRESS_WATCH_REG_MAX +
+						ADDRESS_WATCH_REG_CNTL],
+			cntl.u32All);
+
 	return 0;
 }
 
@@ -742,6 +778,32 @@ int kgd_gfx_v9_address_watch_execute(struct kgd_dev *kgd,
 					uint32_t addr_hi,
 					uint32_t addr_lo)
 {
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	union TCP_WATCH_CNTL_BITS cntl;
+	uint32_t watch_base_addr;
+
+	watch_base_addr = get_watch_base_addr(adev);
+	cntl.u32All = cntl_val;
+
+	/* Turning off this watch point until we set all the registers */
+	cntl.bitfields.valid = 0;
+	WREG32(watch_base_addr + watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_CNTL],
+			cntl.u32All);
+
+	WREG32(watch_base_addr + watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_HI],
+			addr_hi);
+
+	WREG32(watch_base_addr + watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + ADDRESS_WATCH_REG_ADDR_LO],
+			addr_lo);
+
+	/* Enable the watch point */
+	cntl.bitfields.valid = 1;
+
+	WREG32(watch_base_addr +
+			watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX +
+				ADDRESS_WATCH_REG_CNTL],
+			cntl.u32All);
+
 	return 0;
 }
 
@@ -774,6 +836,134 @@ uint32_t kgd_gfx_v9_address_watch_get_offset(struct kgd_dev *kgd,
 					unsigned int watch_point_id,
 					unsigned int reg_offset)
 {
+	return get_watch_base_addr(get_amdgpu_device(kgd)) +
+		watchRegs[watch_point_id * ADDRESS_WATCH_REG_MAX + reg_offset];
+}
+
+uint32_t kgd_gfx_v9_enable_debug_trap(struct kgd_dev *kgd,
+				uint32_t trap_debug_wave_launch_mode,
+				uint32_t vmid)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	uint32_t data = 0;
+	uint32_t orig_wave_cntl_value;
+	uint32_t orig_stall_vmid;
+
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	orig_wave_cntl_value = RREG32(SOC15_REG_OFFSET(GC,
+							0,
+							mmSPI_GDBG_WAVE_CNTL));
+	orig_stall_vmid = REG_GET_FIELD(orig_wave_cntl_value,
+					SPI_GDBG_WAVE_CNTL,
+					STALL_VMID);
+
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA, 1);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA0), 0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA1), 0);
+
+	data = 0;
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_MASK), data);
+
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), orig_stall_vmid);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
+	return 0;
+}
+
+uint32_t kgd_gfx_v9_disable_debug_trap(struct kgd_dev *kgd)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA0), 0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA1), 0);
+
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_MASK), 0);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
+	return 0;
+}
+
+uint32_t kgd_gfx_v9_set_debug_trap_data(struct kgd_dev *kgd,
+					int trap_data0,
+					int trap_data1)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA0), trap_data0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_DATA1), trap_data1);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+	return 0;
+}
+
+uint32_t kgd_gfx_v9_set_wave_launch_trap_override(struct kgd_dev *kgd,
+						uint32_t trap_override,
+						uint32_t trap_mask)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	uint32_t data = 0;
+
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA, 1);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+
+	data = 0;
+	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK,
+		EXCP_EN, trap_mask);
+	data = REG_SET_FIELD(data, SPI_GDBG_TRAP_MASK,
+		REPLACE, trap_override);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_TRAP_MASK), data);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL, STALL_RA, 0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
+	return 0;
+}
+
+uint32_t kgd_gfx_v9_set_wave_launch_mode(struct kgd_dev *kgd,
+					uint8_t wave_launch_mode,
+					uint32_t vmid)
+{
+	struct amdgpu_device *adev = get_amdgpu_device(kgd);
+	uint32_t data = 0;
+	bool is_stall_mode;
+	bool is_mode_set;
+
+
+	is_stall_mode = (wave_launch_mode == 4);
+	is_mode_set = (wave_launch_mode != 0 && wave_launch_mode != 4);
+
+	mutex_lock(&adev->grbm_idx_mutex);
+
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
+		VMID_MASK, is_mode_set ? 1 << vmid : 0);
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL2,
+		MODE, is_mode_set ? wave_launch_mode : 0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL2), data);
+
+	data = RREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL));
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL,
+		STALL_VMID, is_stall_mode ? 1 << vmid : 0);
+	data = REG_SET_FIELD(data, SPI_GDBG_WAVE_CNTL,
+		STALL_RA, is_stall_mode ? 1 : 0);
+	WREG32(SOC15_REG_OFFSET(GC, 0, mmSPI_GDBG_WAVE_CNTL), data);
+
+	mutex_unlock(&adev->grbm_idx_mutex);
+
 	return 0;
 }
 
@@ -837,6 +1027,11 @@ static const struct kfd2kgd_calls kfd2kgd = {
 	.invalidate_tlbs = kgd_gfx_v9_invalidate_tlbs,
 	.invalidate_tlbs_vmid = kgd_gfx_v9_invalidate_tlbs_vmid,
 	.get_hive_id = amdgpu_amdkfd_get_hive_id,
+	.enable_debug_trap = kgd_gfx_v9_enable_debug_trap,
+	.disable_debug_trap = kgd_gfx_v9_disable_debug_trap,
+	.set_debug_trap_data = kgd_gfx_v9_set_debug_trap_data,
+	.set_wave_launch_trap_override = kgd_gfx_v9_set_wave_launch_trap_override,
+	.set_wave_launch_mode = kgd_gfx_v9_set_wave_launch_mode,
 };
 
 struct kfd2kgd_calls *amdgpu_amdkfd_gfx_9_0_get_functions(void)
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
old mode 100644
new mode 100755
index aff97212b15d..4957a4500029
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c
@@ -27,7 +27,7 @@
 #include <linux/pagemap.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/task.h>
-
+#include <drm/drmP.h>
 #include "amdgpu_object.h"
 #include "amdgpu_vm.h"
 #include "amdgpu_amdkfd.h"
@@ -835,6 +835,17 @@ static struct sg_table *create_doorbell_sg(uint64_t addr, uint32_t size)
 	return sg;
 }
 
+static bool check_sg_size(struct sg_table *sgt, uint64_t size)
+{
+	unsigned int count;
+	struct scatterlist *sg;
+
+	for_each_sg(sgt->sgl, sg, sgt->nents, count)
+		size -= sg->length;
+
+	return (size == 0);
+}
+
 static int process_validate_vms(struct amdkfd_process_info *process_info)
 {
 	struct amdgpu_vm *peer_vm;
@@ -1118,13 +1129,12 @@ uint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *vm)
 
 int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		struct kgd_dev *kgd, uint64_t va, uint64_t size,
-		void *vm, struct kgd_mem **mem,
+		void *vm, struct sg_table *sg, struct kgd_mem **mem,
 		uint64_t *offset, uint32_t flags)
 {
 	struct amdgpu_device *adev = get_amdgpu_device(kgd);
 	struct amdgpu_vm *avm = (struct amdgpu_vm *)vm;
 	enum ttm_bo_type bo_type = ttm_bo_type_device;
-	struct sg_table *sg = NULL;
 	uint64_t user_addr = 0;
 	struct amdgpu_bo *bo;
 	struct amdgpu_bo_param bp;
@@ -1144,13 +1154,15 @@ int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 	} else if (flags & ALLOC_MEM_FLAGS_GTT) {
 		domain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;
 		alloc_flags = 0;
+		if (sg && !check_sg_size(sg, size))
+			return -EINVAL;
 	} else if (flags & ALLOC_MEM_FLAGS_USERPTR) {
 		domain = AMDGPU_GEM_DOMAIN_GTT;
 		alloc_domain = AMDGPU_GEM_DOMAIN_CPU;
 		alloc_flags = 0;
 		if (!offset || !*offset)
 			return -EINVAL;
-		user_addr = untagged_addr(*offset);
+		user_addr = *offset;
 	} else if (flags & (ALLOC_MEM_FLAGS_DOORBELL |
 			ALLOC_MEM_FLAGS_MMIO_REMAP)) {
 		domain = AMDGPU_GEM_DOMAIN_GTT;
@@ -1159,6 +1171,7 @@ int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		alloc_flags = 0;
 		if (size > UINT_MAX)
 			return -EINVAL;
+		WARN_ON(sg);
 		sg = create_doorbell_sg(*offset, size);
 		if (!sg)
 			return -ENOMEM;
@@ -1166,6 +1179,10 @@ int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		return -EINVAL;
 	}
 
+	if (sg) {
+		alloc_domain = AMDGPU_GEM_DOMAIN_CPU;
+		bo_type = ttm_bo_type_sg;
+	}
 	*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);
 	if (!*mem) {
 		ret = -ENOMEM;
@@ -1186,6 +1203,7 @@ int amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 
 	amdgpu_sync_create(&(*mem)->sync);
 
+	size = ALIGN(size, PAGE_SIZE);
 	ret = amdgpu_amdkfd_reserve_mem_limit(adev, size, alloc_domain, !!sg);
 	if (ret) {
 		pr_debug("Insufficient system memory\n");
@@ -1626,6 +1644,160 @@ int amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct kgd_dev *kgd,
 	return 0;
 }
 
+static int pin_bo_wo_map(struct kgd_mem *mem)
+{
+	struct amdgpu_bo *bo = mem->bo;
+	int ret = 0;
+
+	ret = amdgpu_bo_reserve(bo, false);
+	if (unlikely(ret))
+		return ret;
+
+	ret = amdgpu_bo_pin(bo, mem->domain);
+	amdgpu_bo_unreserve(bo);
+
+	return ret;
+}
+
+static void unpin_bo_wo_map(struct kgd_mem *mem)
+{
+	struct amdgpu_bo *bo = mem->bo;
+	int ret = 0;
+
+	ret = amdgpu_bo_reserve(bo, false);
+	if (unlikely(ret))
+		return;
+
+	amdgpu_bo_unpin(bo);
+	amdgpu_bo_unreserve(bo);
+}
+
+#define AMD_GPU_PAGE_SHIFT	PAGE_SHIFT
+#define AMD_GPU_PAGE_SIZE (_AC(1, UL) << AMD_GPU_PAGE_SHIFT)
+
+static int get_sg_table(struct amdgpu_device *adev,
+		struct kgd_mem *mem, uint64_t offset,
+		uint64_t size, struct sg_table **ret_sg)
+{
+	struct amdgpu_bo *bo = mem->bo;
+	struct sg_table *sg = NULL;
+	unsigned long bus_addr, aper_limit;
+	unsigned int chunks;
+	unsigned int i;
+	struct scatterlist *s;
+	uint64_t offset_in_page;
+	unsigned int page_size;
+	int ret;
+
+	if (size + offset > amdgpu_bo_size(bo))
+		return -EFAULT;
+	sg = kmalloc(sizeof(*sg), GFP_KERNEL);
+	if (!sg) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM)
+		page_size = AMD_GPU_PAGE_SIZE;
+	else
+		page_size = PAGE_SIZE;
+
+
+	offset_in_page = offset & (page_size - 1);
+	chunks = (size  + offset_in_page + page_size - 1)
+			/ page_size;
+
+	ret = sg_alloc_table(sg, chunks, GFP_KERNEL);
+	if (unlikely(ret))
+		goto out;
+
+	if (bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM) {
+		bus_addr = amdgpu_bo_gpu_offset(bo) - adev->gmc.vram_start
+			   + adev->gmc.aper_base + offset;
+		aper_limit = adev->gmc.aper_base + adev->gmc.aper_size;
+		if (bus_addr + (chunks * page_size) > aper_limit) {
+			pr_err("sg: bus addr not inside pci aperture\n");
+			ret = -EFAULT;
+			goto out_of_range;
+		}
+
+		for_each_sg(sg->sgl, s, sg->orig_nents, i) {
+			uint64_t chunk_size, length;
+
+			chunk_size = page_size - offset_in_page;
+			length = min(size, chunk_size);
+
+			sg_set_page(s, NULL, length, offset_in_page);
+			s->dma_address = bus_addr;
+			s->dma_length = length;
+
+			size -= length;
+			offset_in_page = 0;
+			bus_addr += length;
+		}
+	} else {
+		struct page **pages;
+		unsigned int cur_page;
+
+		pages = bo->tbo.ttm->pages;
+
+		cur_page = offset / page_size;
+		for_each_sg(sg->sgl, s, sg->orig_nents, i) {
+			uint64_t chunk_size, length;
+
+			chunk_size = page_size - offset_in_page;
+			length = min(size, chunk_size);
+
+			sg_set_page(s, pages[cur_page], length, offset_in_page);
+			s->dma_address = page_to_phys(pages[cur_page]);
+			s->dma_length = length;
+
+			size -= length;
+			offset_in_page = 0;
+			cur_page++;
+		}
+	}
+
+	*ret_sg = sg;
+	return 0;
+
+out_of_range:
+	sg_free_table(sg);
+out:
+	kfree(sg);
+	*ret_sg = NULL;
+	return ret;
+}
+
+int amdgpu_amdkfd_gpuvm_pin_get_sg_table(struct kgd_dev *kgd,
+		struct kgd_mem *mem, uint64_t offset,
+		uint64_t size, struct sg_table **ret_sg)
+{
+	int ret;
+	struct amdgpu_device *adev;
+
+	ret = pin_bo_wo_map(mem);
+	if (unlikely(ret))
+		return ret;
+
+	adev = get_amdgpu_device(kgd);
+
+	ret = get_sg_table(adev, mem, offset, size, ret_sg);
+	if (ret)
+		unpin_bo_wo_map(mem);
+
+	return ret;
+}
+
+void amdgpu_amdkfd_gpuvm_unpin_put_sg_table(
+		struct kgd_mem *mem, struct sg_table *sg)
+{
+	sg_free_table(sg);
+	kfree(sg);
+
+	unpin_bo_wo_map(mem);
+}
+
 int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 				      struct dma_buf *dma_buf,
 				      uint64_t va, void *vm,
@@ -1664,10 +1836,14 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 
 	INIT_LIST_HEAD(&(*mem)->bo_va_list);
 	mutex_init(&(*mem)->lock);
-	(*mem)->alloc_flags =
-		((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
-		 ALLOC_MEM_FLAGS_VRAM : ALLOC_MEM_FLAGS_GTT) |
-		ALLOC_MEM_FLAGS_WRITABLE | ALLOC_MEM_FLAGS_EXECUTABLE;
+
+	if (bo->kfd_bo)
+		(*mem)->alloc_flags = bo->kfd_bo->alloc_flags;
+	else
+		(*mem)->alloc_flags =
+			((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?
+			 ALLOC_MEM_FLAGS_VRAM : ALLOC_MEM_FLAGS_GTT) |
+			ALLOC_MEM_FLAGS_WRITABLE | ALLOC_MEM_FLAGS_EXECUTABLE;
 
 	(*mem)->bo = amdgpu_bo_ref(bo);
 	(*mem)->va = va;
@@ -1681,6 +1857,24 @@ int amdgpu_amdkfd_gpuvm_import_dmabuf(struct kgd_dev *kgd,
 	return 0;
 }
 
+int amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_dev *kgd, void *vm,
+				      struct kgd_mem *mem,
+				      struct dma_buf **dmabuf)
+{
+	struct amdgpu_device *adev = NULL;
+
+	if (!dmabuf || !kgd || !vm || !mem)
+		return -EINVAL;
+
+	adev = get_amdgpu_device(kgd);
+
+	*dmabuf = amdgpu_gem_prime_export(&mem->bo->gem_base, 0);
+	if (IS_ERR(*dmabuf))
+		return -EINVAL;
+
+	return 0;
+}
+
 /* Evict a userptr BO by stopping the queues if necessary
  *
  * Runs in MMU notifier, may be in RECLAIM_FS context. This means it
@@ -2244,3 +2438,82 @@ int amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)
 	kfree(mem);
 	return 0;
 }
+
+int amdgpu_amdkfd_copy_mem_to_mem(struct kgd_dev *kgd, struct kgd_mem *src_mem,
+				  uint64_t src_offset, struct kgd_mem *dst_mem,
+				  uint64_t dst_offset, uint64_t size,
+				  struct dma_fence **f, uint64_t *actual_size)
+{
+	struct amdgpu_device *adev = NULL;
+	struct amdgpu_copy_mem src, dst;
+	struct ww_acquire_ctx ticket;
+	struct list_head list, duplicates;
+	struct ttm_validate_buffer resv_list[2];
+	struct dma_fence *fence = NULL;
+	int i, r;
+
+	if (!kgd || !src_mem || !dst_mem || !actual_size)
+		return -EINVAL;
+
+	*actual_size = 0;
+
+	adev = get_amdgpu_device(kgd);
+	INIT_LIST_HEAD(&list);
+	INIT_LIST_HEAD(&duplicates);
+
+	src.bo = &src_mem->bo->tbo;
+	dst.bo = &dst_mem->bo->tbo;
+	src.mem = &src.bo->mem;
+	dst.mem = &dst.bo->mem;
+	src.offset = src_offset;
+	dst.offset = dst_offset;
+
+	resv_list[0].bo = src.bo;
+	resv_list[1].bo = dst.bo;
+
+	for (i = 0; i < 2; i++) {
+		resv_list[i].num_shared = 1;
+		list_add_tail(&resv_list[i].head, &list);
+	}
+
+	r = ttm_eu_reserve_buffers(&ticket, &list, false, &duplicates, true);
+	if (r) {
+		pr_err("Copy buffer failed. Unable to reserve bo (%d)\n", r);
+		return r;
+	}
+
+	/* The process to which the Source and Dest BOs belong to could be
+	 * evicted and the BOs invalidated. So validate BOs before use
+	 */
+	r = amdgpu_amdkfd_bo_validate(src_mem->bo, src_mem->domain, false);
+	if (r) {
+		pr_err("CMA fail: SRC BO validate failed %d\n", r);
+		goto validate_fail;
+	}
+
+
+	r = amdgpu_amdkfd_bo_validate(dst_mem->bo, dst_mem->domain, false);
+	if (r) {
+		pr_err("CMA fail: DST BO validate failed %d\n", r);
+		goto validate_fail;
+	}
+
+
+	r = amdgpu_ttm_copy_mem_to_mem(adev, &src, &dst, size, NULL,
+				       &fence);
+	if (r)
+		pr_err("Copy buffer failed %d\n", r);
+	else
+		*actual_size = size;
+	if (fence) {
+		amdgpu_bo_fence(src_mem->bo, fence, true);
+		amdgpu_bo_fence(dst_mem->bo, fence, true);
+	}
+	if (f)
+		*f = dma_fence_get(fence);
+	dma_fence_put(fence);
+
+validate_fail:
+	ttm_eu_backoff_reservation(&ticket, &list);
+	return r;
+}
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
old mode 100644
new mode 100755
index bb8598eaef9a..309a0d9e8b31
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c
@@ -741,8 +741,32 @@ MODULE_PARM_DESC(hws_gws_support, "MEC FW support gws barriers (false = not supp
 int queue_preemption_timeout_ms = 9000;
 module_param(queue_preemption_timeout_ms, int, 0644);
 MODULE_PARM_DESC(queue_preemption_timeout_ms, "queue preemption timeout in ms (1 = Minimum, 9000 = default)");
+
+/**
+ * DOC: priv_cp_queues (int)
+ * Enable privileged mode for CP queues. Default value: 0 (off)
+ */
+int priv_cp_queues;
+module_param(priv_cp_queues, int, 0644);
+MODULE_PARM_DESC(priv_cp_queues, "Enable privileged mode for CP queues (0 = off (default), 1 = on)");
+
+/**
+ * DOC: keep_udle_process_evicted (bool)
+ * Keep an evicted process evicted if it is idle. Default value: false (off)
+ */
+bool keep_idle_process_evicted;
+module_param(keep_idle_process_evicted, bool, 0444);
+MODULE_PARM_DESC(keep_idle_process_evicted, "Restore evicted process only if queues are active (N = off(default), Y = on)");
 #endif
 
+/**
+ * DOC: pcie_p2p (bool)
+ * Enable PCIe P2P (requires large-BAR). Default value: true (on)
+ */
+bool pcie_p2p = true;
+module_param(pcie_p2p, bool, 0444);
+MODULE_PARM_DESC(pcie_p2p, "Enable PCIe P2P (requires large-BAR). (N = off, Y = on(default))");
+
 /**
  * DOC: dcfeaturemask (uint)
  * Override display features enabled. See enum DC_FEATURE_MASK in drivers/gpu/drm/amd/include/amd_shared.h.
diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
index 2445efc1f096..f5c165e88aee 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_object.h
@@ -95,6 +95,7 @@ struct amdgpu_bo {
 	/* per VM structure for page tables and with virtual addresses */
 	struct amdgpu_vm_bo_base	*vm_bo;
 	/* Constant after initialization */
+	struct drm_gem_object           gem_base;
 	struct amdgpu_bo		*parent;
 	struct amdgpu_bo		*shadow;
 
diff --git a/drivers/gpu/drm/amd/amdkfd/Kconfig b/drivers/gpu/drm/amd/amdkfd/Kconfig
old mode 100644
new mode 100755
index a1a35d4d594b..44d4aa06d441
--- a/drivers/gpu/drm/amd/amdkfd/Kconfig
+++ b/drivers/gpu/drm/amd/amdkfd/Kconfig
@@ -5,7 +5,7 @@
 
 config HSA_AMD
 	bool "HSA kernel driver for AMD GPU devices"
-	depends on DRM_AMDGPU && (X86_64 || ARM64)
+	depends on DRM_AMDGPU && (X86_64 || PPC64 || ARM64)
 	imply AMD_IOMMU_V2 if X86_64
 	select MMU_NOTIFIER
 	help
diff --git a/drivers/gpu/drm/amd/amdkfd/Makefile b/drivers/gpu/drm/amd/amdkfd/Makefile
old mode 100644
new mode 100755
index 48155060a57c..dad236a68a25
--- a/drivers/gpu/drm/amd/amdkfd/Makefile
+++ b/drivers/gpu/drm/amd/amdkfd/Makefile
@@ -55,7 +55,11 @@ AMDKFD_FILES	:= $(AMDKFD_PATH)/kfd_module.o \
 		$(AMDKFD_PATH)/kfd_int_process_v9.o \
 		$(AMDKFD_PATH)/kfd_dbgdev.o \
 		$(AMDKFD_PATH)/kfd_dbgmgr.o \
-		$(AMDKFD_PATH)/kfd_crat.o
+		$(AMDKFD_PATH)/kfd_crat.o \
+		$(AMDKFD_PATH)/kfd_rdma.o \
+		$(AMDKFD_PATH)/kfd_peerdirect.o \
+		$(AMDKFD_PATH)/kfd_ipc.o \
+		$(AMDKFD_PATH)/kfd_trace.o
 
 ifneq ($(CONFIG_AMD_IOMMU_V2),)
 AMDKFD_FILES += $(AMDKFD_PATH)/kfd_iommu.o
diff --git a/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler.h b/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler.h
old mode 100644
new mode 100755
index 901fe3590165..a8cf82d46109
--- a/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler.h
+++ b/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler.h
@@ -694,10 +694,10 @@ static const uint32_t cwsr_trap_gfx10_hex[] = {
 	0x003f8000, 0x8f6f896f,
 	0x88776f77, 0x8a6eff6e,
 	0x023f8000, 0xb9eef807,
-	0xb97af812, 0xb97bf813,
-	0x8ffa887a, 0xf4051bbd,
+	0xb970f812, 0xb971f813,
+	0x8ff08870, 0xf4051bb8,
 	0xfa000000, 0xbf8cc07f,
-	0xf4051ebd, 0xfa000008,
+	0xf4051c38, 0xfa000008,
 	0xbf8cc07f, 0x87ee6e6e,
 	0xbf840001, 0xbe80206e,
 	0xb971f803, 0x8771ff71,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
old mode 100644
new mode 100755
index 1d3cd5c50d5f..70163e92f801
--- a/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c
@@ -26,6 +26,7 @@
 #include <linux/fs.h>
 #include <linux/file.h>
 #include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
 #include <linux/compat.h>
@@ -35,9 +36,15 @@
 #include <linux/mman.h>
 #include <linux/dma-buf.h>
 #include <asm/processor.h>
+#include <linux/ptrace.h>
+#include <linux/pagemap.h>
+
 #include "kfd_priv.h"
 #include "kfd_device_queue_manager.h"
 #include "kfd_dbgmgr.h"
+#include "kfd_ipc.h"
+#include "kfd_trace.h"
+
 #include "amdgpu_amdkfd.h"
 
 static long kfd_ioctl(struct file *, unsigned int, unsigned long);
@@ -58,6 +65,14 @@ static int kfd_char_dev_major = -1;
 static struct class *kfd_class;
 struct device *kfd_device;
 
+static char *kfd_devnode(struct device *dev, umode_t *mode)
+{
+	if (mode && dev->devt == MKDEV(kfd_char_dev_major, 0))
+		*mode = 0666;
+
+	return NULL;
+}
+
 int kfd_chardev_init(void)
 {
 	int err = 0;
@@ -72,6 +87,8 @@ int kfd_chardev_init(void)
 	if (IS_ERR(kfd_class))
 		goto err_class_create;
 
+	kfd_class->devnode = kfd_devnode;
+
 	kfd_device = device_create(kfd_class, NULL,
 					MKDEV(kfd_char_dev_major, 0),
 					NULL, kfd_dev_name);
@@ -561,11 +578,6 @@ static int kfd_ioctl_dbg_register(struct file *filep,
 	if (!dev)
 		return -EINVAL;
 
-	if (dev->device_info->asic_family == CHIP_CARRIZO) {
-		pr_debug("kfd_ioctl_dbg_register not supported on CZ\n");
-		return -EINVAL;
-	}
-
 	mutex_lock(&p->mutex);
 	mutex_lock(kfd_get_dbgmgr_mutex());
 
@@ -612,11 +624,6 @@ static int kfd_ioctl_dbg_unregister(struct file *filep,
 	if (!dev || !dev->dbgmgr)
 		return -EINVAL;
 
-	if (dev->device_info->asic_family == CHIP_CARRIZO) {
-		pr_debug("kfd_ioctl_dbg_unregister not supported on CZ\n");
-		return -EINVAL;
-	}
-
 	mutex_lock(kfd_get_dbgmgr_mutex());
 
 	status = kfd_dbgmgr_unregister(dev->dbgmgr, p);
@@ -657,11 +664,6 @@ static int kfd_ioctl_dbg_address_watch(struct file *filep,
 	if (!dev)
 		return -EINVAL;
 
-	if (dev->device_info->asic_family == CHIP_CARRIZO) {
-		pr_debug("kfd_ioctl_dbg_wave_control not supported on CZ\n");
-		return -EINVAL;
-	}
-
 	cmd_from_user = (void __user *) args->content_ptr;
 
 	/* Validate arguments */
@@ -765,11 +767,6 @@ static int kfd_ioctl_dbg_wave_control(struct file *filep,
 	if (!dev)
 		return -EINVAL;
 
-	if (dev->device_info->asic_family == CHIP_CARRIZO) {
-		pr_debug("kfd_ioctl_dbg_wave_control not supported on CZ\n");
-		return -EINVAL;
-	}
-
 	/* input size must match the computed "compact" size */
 	if (args->buf_size_in_bytes != computed_buff_size) {
 		pr_debug("size mismatch, computed : actual %u : %u\n",
@@ -1255,6 +1252,9 @@ static int kfd_ioctl_alloc_memory_of_gpu(struct file *filep,
 	long err;
 	uint64_t offset = args->mmap_offset;
 	uint32_t flags = args->flags;
+	struct vm_area_struct *vma;
+	uint64_t cpuva = 0;
+	unsigned int mem_type = 0;
 
 	if (args->size == 0)
 		return -EINVAL;
@@ -1270,7 +1270,29 @@ static int kfd_ioctl_alloc_memory_of_gpu(struct file *filep,
 		return -EINVAL;
 	}
 
-	if (flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {
+	if (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {
+		/* Check if the userptr corresponds to another (or third-party)
+		 * device local memory. If so treat is as a doorbell. User
+		 * space will be oblivious of this and will use this doorbell
+		 * BO as a regular userptr BO
+		 */
+		vma = find_vma(current->mm, args->mmap_offset);
+		if (vma && (vma->vm_flags & VM_IO)) {
+			unsigned long pfn;
+
+			follow_pfn(vma, args->mmap_offset, &pfn);
+			flags |= KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL;
+			flags &= ~KFD_IOC_ALLOC_MEM_FLAGS_USERPTR;
+			offset = (pfn << PAGE_SHIFT);
+		} else {
+			if (offset & (PAGE_SIZE - 1)) {
+				pr_debug("Unaligned userptr address:%llx\n",
+					 offset);
+				return -EINVAL;
+			}
+			cpuva = offset;
+		}
+	} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {
 		if (args->size != kfd_doorbell_process_slice(dev))
 			return -EINVAL;
 		offset = kfd_get_process_doorbells(dev, p);
@@ -1292,13 +1314,19 @@ static int kfd_ioctl_alloc_memory_of_gpu(struct file *filep,
 
 	err = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(
 		dev->kgd, args->va_addr, args->size,
-		pdd->vm, (struct kgd_mem **) &mem, &offset,
+		pdd->vm, NULL, (struct kgd_mem **) &mem, &offset,
 		flags);
 
 	if (err)
 		goto err_unlock;
 
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem);
+	mem_type = flags & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM |
+			    KFD_IOC_ALLOC_MEM_FLAGS_GTT |
+			    KFD_IOC_ALLOC_MEM_FLAGS_USERPTR |
+			    KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |
+			    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);
+	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
+			args->va_addr, args->size, cpuva, mem_type, NULL);
 	if (idr_handle < 0) {
 		err = -EFAULT;
 		goto err_free;
@@ -1331,7 +1359,7 @@ static int kfd_ioctl_free_memory_of_gpu(struct file *filep,
 {
 	struct kfd_ioctl_free_memory_of_gpu_args *args = data;
 	struct kfd_process_device *pdd;
-	void *mem;
+	struct kfd_bo *buf_obj;
 	struct kfd_dev *dev;
 	int ret;
 
@@ -1348,15 +1376,15 @@ static int kfd_ioctl_free_memory_of_gpu(struct file *filep,
 		goto err_unlock;
 	}
 
-	mem = kfd_process_device_translate_handle(
-		pdd, GET_IDR_HANDLE(args->handle));
-	if (!mem) {
+	buf_obj = kfd_process_device_find_bo(pdd,
+					GET_IDR_HANDLE(args->handle));
+	if (!buf_obj) {
 		ret = -EINVAL;
 		goto err_unlock;
 	}
+	run_rdma_free_callback(buf_obj);
 
-	ret = amdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->kgd,
-						(struct kgd_mem *)mem);
+	ret = amdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->kgd, buf_obj->mem);
 
 	/* If freeing the buffer failed, leave the handle in place for
 	 * clean-up during process tear-down.
@@ -1381,6 +1409,7 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 	int i;
 	uint32_t *devices_arr = NULL;
 
+	trace_kfd_map_memory_to_gpu_start(p);
 	dev = kfd_device_by_id(GET_GPU_ID(args->handle));
 	if (!dev)
 		return -EINVAL;
@@ -1467,6 +1496,8 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 
 	kfree(devices_arr);
 
+	trace_kfd_map_memory_to_gpu_end(p,
+			args->n_devices * sizeof(*devices_arr), "Success");
 	return err;
 
 bind_process_to_device_failed:
@@ -1476,6 +1507,8 @@ static int kfd_ioctl_map_memory_to_gpu(struct file *filep,
 copy_from_user_failed:
 sync_memory_failed:
 	kfree(devices_arr);
+	trace_kfd_map_memory_to_gpu_end(p,
+		args->n_devices * sizeof(*devices_arr), "Failed");
 
 	return err;
 }
@@ -1567,6 +1600,43 @@ static int kfd_ioctl_unmap_memory_from_gpu(struct file *filep,
 	return err;
 }
 
+static int kfd_ioctl_alloc_queue_gws(struct file *filep,
+		struct kfd_process *p, void *data)
+{
+	int retval;
+	struct kfd_ioctl_alloc_queue_gws_args *args = data;
+	struct queue *q;
+	struct kfd_dev *dev;
+
+	if (!hws_gws_support)
+		return -ENODEV;
+
+	mutex_lock(&p->mutex);
+	q = pqm_get_user_queue(&p->pqm, args->queue_id);
+
+	if (q) {
+		dev = q->device;
+	} else {
+		retval = -EINVAL;
+		goto out_unlock;
+	}
+
+	if (dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {
+		retval = -ENODEV;
+		goto out_unlock;
+	}
+
+	retval = pqm_set_gws(&p->pqm, args->queue_id, args->num_gws ? dev->gws : NULL);
+	mutex_unlock(&p->mutex);
+
+	args->first_gws = 0;
+	return retval;
+
+out_unlock:
+	mutex_unlock(&p->mutex);
+	return retval;
+}
+
 static int kfd_ioctl_get_dmabuf_info(struct file *filep,
 		struct kfd_process *p, void *data)
 {
@@ -1626,53 +1696,1115 @@ static int kfd_ioctl_import_dmabuf(struct file *filep,
 				   struct kfd_process *p, void *data)
 {
 	struct kfd_ioctl_import_dmabuf_args *args = data;
-	struct kfd_process_device *pdd;
-	struct dma_buf *dmabuf;
 	struct kfd_dev *dev;
-	int idr_handle;
-	uint64_t size;
-	void *mem;
 	int r;
 
 	dev = kfd_device_by_id(args->gpu_id);
 	if (!dev)
 		return -EINVAL;
 
-	dmabuf = dma_buf_get(args->dmabuf_fd);
-	if (IS_ERR(dmabuf))
-		return PTR_ERR(dmabuf);
+	r = kfd_ipc_import_dmabuf(dev, p, args->gpu_id, args->dmabuf_fd,
+				  args->va_addr, &args->handle, NULL);
+	if (r)
+		pr_err("Failed to import dmabuf\n");
 
-	mutex_lock(&p->mutex);
+	return r;
+}
 
-	pdd = kfd_bind_process_to_device(dev, p);
-	if (IS_ERR(pdd)) {
-		r = PTR_ERR(pdd);
-		goto err_unlock;
-	}
+static int kfd_ioctl_ipc_export_handle(struct file *filep,
+				       struct kfd_process *p,
+				       void *data)
+{
+	struct kfd_ioctl_ipc_export_handle_args *args = data;
+	struct kfd_dev *dev;
+	int r;
 
-	r = amdgpu_amdkfd_gpuvm_import_dmabuf(dev->kgd, dmabuf,
-					      args->va_addr, pdd->vm,
-					      (struct kgd_mem **)&mem, &size,
-					      NULL);
+	dev = kfd_device_by_id(args->gpu_id);
+	if (!dev)
+		return -EINVAL;
+
+	r = kfd_ipc_export_as_handle(dev, p, args->handle, args->share_handle);
 	if (r)
-		goto err_unlock;
+		pr_err("Failed to export IPC handle\n");
 
-	idr_handle = kfd_process_device_create_obj_handle(pdd, mem);
-	if (idr_handle < 0) {
-		r = -EFAULT;
-		goto err_free;
+	return r;
+}
+
+static int kfd_ioctl_ipc_import_handle(struct file *filep,
+				       struct kfd_process *p,
+				       void *data)
+{
+	struct kfd_ioctl_ipc_import_handle_args *args = data;
+	struct kfd_dev *dev = NULL;
+	int r;
+
+	dev = kfd_device_by_id(args->gpu_id);
+	if (!dev)
+		return -EINVAL;
+
+	r = kfd_ipc_import_handle(dev, p, args->gpu_id, args->share_handle,
+				  args->va_addr, &args->handle,
+				  &args->mmap_offset);
+	if (r)
+		pr_err("Failed to import IPC handle\n");
+
+	return r;
+}
+
+/* Maximum number of entries for process pages array which lives on stack */
+#define MAX_PP_STACK_COUNT 16
+/* Maximum number of pages kmalloc'd to hold struct page's during copy */
+#define MAX_KMALLOC_PAGES (PAGE_SIZE * 2)
+#define MAX_PP_KMALLOC_COUNT (MAX_KMALLOC_PAGES/sizeof(struct page *))
+
+static void kfd_put_sg_table(struct sg_table *sg)
+{
+	unsigned int i;
+	struct scatterlist *s;
+
+	for_each_sg(sg->sgl, s, sg->nents, i)
+		put_page(sg_page(s));
+}
+
+
+/* Create a sg table for the given userptr BO by pinning its system pages
+ * @bo: userptr BO
+ * @offset: Offset into BO
+ * @mm/@task: mm_struct & task_struct of the process that holds the BO
+ * @size: in/out: desired size / actual size which could be smaller
+ * @sg_size: out: Size of sg table. This is ALIGN_UP(@size)
+ * @ret_sg: out sg table
+ */
+static int kfd_create_sg_table_from_userptr_bo(struct kfd_bo *bo,
+					       int64_t offset, int cma_write,
+					       struct mm_struct *mm,
+					       struct task_struct *task,
+					       uint64_t *size,
+					       uint64_t *sg_size,
+					       struct sg_table **ret_sg)
+{
+	int ret, locked = 1;
+	struct sg_table *sg = NULL;
+	unsigned int i, offset_in_page, flags = 0;
+	unsigned long nents, n;
+	unsigned long pa = (bo->cpuva + offset) & PAGE_MASK;
+	unsigned int cur_page = 0;
+	struct scatterlist *s;
+	uint64_t sz = *size;
+	struct page **process_pages;
+
+	*sg_size = 0;
+	sg = kmalloc(sizeof(*sg), GFP_KERNEL);
+	if (!sg)
+		return -ENOMEM;
+
+	offset_in_page = offset & (PAGE_SIZE - 1);
+	nents = (sz + offset_in_page + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	ret = sg_alloc_table(sg, nents, GFP_KERNEL);
+	if (unlikely(ret)) {
+		ret = -ENOMEM;
+		goto sg_alloc_fail;
+	}
+	process_pages = kmalloc_array(nents, sizeof(struct pages *),
+				      GFP_KERNEL);
+	if (!process_pages) {
+		ret = -ENOMEM;
+		goto page_alloc_fail;
+	}
+
+	if (cma_write)
+		flags = FOLL_WRITE;
+	locked = 1;
+	down_read(&mm->mmap_sem);
+	n = get_user_pages_remote(task, mm, pa, nents, flags, process_pages,
+				  NULL, &locked);
+	if (locked)
+		up_read(&mm->mmap_sem);
+	if (n <= 0) {
+		pr_err("CMA: Invalid virtual address 0x%lx\n", pa);
+		ret = -EFAULT;
+		goto get_user_fail;
+	}
+	if (n != nents) {
+		/* Pages pinned < requested. Set the size accordingly */
+		*size = (n * PAGE_SIZE) - offset_in_page;
+		pr_debug("Requested %lx but pinned %lx\n", nents, n);
+	}
+
+	sz = 0;
+	for_each_sg(sg->sgl, s, n, i) {
+		sg_set_page(s, process_pages[cur_page], PAGE_SIZE,
+			    offset_in_page);
+		sg_dma_address(s) = page_to_phys(process_pages[cur_page]);
+		offset_in_page = 0;
+		cur_page++;
+		sz += PAGE_SIZE;
+	}
+	*ret_sg = sg;
+	*sg_size = sz;
+
+	kfree(process_pages);
+	return 0;
+
+get_user_fail:
+	kfree(process_pages);
+page_alloc_fail:
+	sg_free_table(sg);
+sg_alloc_fail:
+	kfree(sg);
+	return ret;
+}
+
+static void kfd_free_cma_bos(struct cma_iter *ci)
+{
+	struct cma_system_bo *cma_bo, *tmp;
+
+	list_for_each_entry_safe(cma_bo, tmp, &ci->cma_list, list) {
+		struct kfd_dev *dev = cma_bo->dev;
+
+		/* sg table is deleted by free_memory_of_gpu */
+		if (cma_bo->sg)
+			kfd_put_sg_table(cma_bo->sg);
+		amdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->kgd, cma_bo->mem);
+		list_del(&cma_bo->list);
+		kfree(cma_bo);
+	}
+}
+
+/* 1 second timeout */
+#define CMA_WAIT_TIMEOUT msecs_to_jiffies(1000)
+
+static int kfd_cma_fence_wait(struct dma_fence *f)
+{
+	int ret;
+
+	ret = dma_fence_wait_timeout(f, false, CMA_WAIT_TIMEOUT);
+	if (likely(ret > 0))
+		return 0;
+	if (!ret)
+		ret = -ETIME;
+	return ret;
+}
+
+/* Put previous (old) fence @pf but it waits for @pf to signal if the context
+ * of the current fence @cf is different.
+ */
+static int kfd_fence_put_wait_if_diff_context(struct dma_fence *cf,
+					      struct dma_fence *pf)
+{
+	int ret = 0;
+
+	if (pf && cf && cf->context != pf->context)
+		ret = kfd_cma_fence_wait(pf);
+	dma_fence_put(pf);
+	return ret;
+}
+
+#define MAX_SYSTEM_BO_SIZE (512*PAGE_SIZE)
+
+/* Create an equivalent system BO for the given @bo. If @bo is a userptr then
+ * create a new system BO by pinning underlying system pages of the given
+ * userptr BO. If @bo is in Local Memory then create an empty system BO and
+ * then copy @bo into this new BO.
+ * @bo: Userptr BO or Local Memory BO
+ * @offset: Offset into bo
+ * @size: in/out: The size of the new BO could be less than requested if all
+ *        the pages couldn't be pinned or size > MAX_SYSTEM_BO_SIZE. This would
+ *        be reflected in @size
+ * @mm/@task: mm/task to which @bo belongs to
+ * @cma_bo: out: new system BO
+ */
+static int kfd_create_cma_system_bo(struct kfd_dev *kdev, struct kfd_bo *bo,
+				    uint64_t *size, uint64_t offset,
+				    int cma_write, struct kfd_process *p,
+				    struct mm_struct *mm,
+				    struct task_struct *task,
+				    struct cma_system_bo **cma_bo)
+{
+	int ret;
+	struct kfd_process_device *pdd = NULL;
+	struct cma_system_bo *cbo;
+	uint64_t bo_size = 0;
+	struct dma_fence *f;
+
+	uint32_t flags = ALLOC_MEM_FLAGS_GTT | ALLOC_MEM_FLAGS_WRITABLE |
+			 ALLOC_MEM_FLAGS_NO_SUBSTITUTE;
+
+	*cma_bo = NULL;
+	cbo = kzalloc(sizeof(**cma_bo), GFP_KERNEL);
+	if (!cbo)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&cbo->list);
+	if (bo->mem_type == KFD_IOC_ALLOC_MEM_FLAGS_VRAM)
+		bo_size = min_t(uint64_t, *size, MAX_SYSTEM_BO_SIZE);
+	else if (bo->cpuva) {
+		ret = kfd_create_sg_table_from_userptr_bo(bo, offset,
+							  cma_write, mm, task,
+							  size, &bo_size,
+							  &cbo->sg);
+		if (ret) {
+			pr_err("CMA: BO create with sg failed %d\n", ret);
+			goto sg_fail;
+		}
+	} else {
+		WARN_ON(1);
+		ret = -EINVAL;
+		goto sg_fail;
+	}
+	mutex_lock(&p->mutex);
+	pdd = kfd_get_process_device_data(kdev, p);
+	if (!pdd) {
+		mutex_unlock(&p->mutex);
+		pr_err("Process device data doesn't exist\n");
+		ret = -EINVAL;
+		goto pdd_fail;
 	}
 
+	ret = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(kdev->kgd, 0ULL, bo_size,
+						      pdd->vm, cbo->sg,
+						      &cbo->mem, NULL, flags);
 	mutex_unlock(&p->mutex);
+	if (ret) {
+		pr_err("Failed to create shadow system BO %d\n", ret);
+		goto pdd_fail;
+	}
+
+	if (bo->mem_type == KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {
+		ret = amdgpu_amdkfd_copy_mem_to_mem(kdev->kgd, bo->mem,
+						    offset, cbo->mem, 0,
+						    bo_size, &f, size);
+		if (ret) {
+			pr_err("CMA: Intermediate copy failed %d\n", ret);
+			goto copy_fail;
+		}
 
-	args->handle = MAKE_HANDLE(args->gpu_id, idr_handle);
+		/* Wait for the copy to finish as subsequent copy will be done
+		 * by different device
+		 */
+		ret = kfd_cma_fence_wait(f);
+		dma_fence_put(f);
+		if (ret) {
+			pr_err("CMA: Intermediate copy timed out %d\n", ret);
+			goto copy_fail;
+		}
+	}
+
+	cbo->dev = kdev;
+	*cma_bo = cbo;
+
+	return ret;
+
+copy_fail:
+	amdgpu_amdkfd_gpuvm_free_memory_of_gpu(kdev->kgd, bo->mem);
+pdd_fail:
+	if (cbo->sg) {
+		kfd_put_sg_table(cbo->sg);
+		sg_free_table(cbo->sg);
+		kfree(cbo->sg);
+	}
+sg_fail:
+	kfree(cbo);
+	return ret;
+}
 
+/* Update cma_iter.cur_bo with KFD BO that is assocaited with
+ * cma_iter.array.va_addr
+ */
+static int kfd_cma_iter_update_bo(struct cma_iter *ci)
+{
+	struct kfd_memory_range *arr = ci->array;
+	uint64_t va_end = arr->va_addr + arr->size - 1;
+
+	mutex_lock(&ci->p->mutex);
+	ci->cur_bo = kfd_process_find_bo_from_interval(ci->p, arr->va_addr,
+						       va_end);
+	mutex_unlock(&ci->p->mutex);
+
+	if (!ci->cur_bo || va_end > ci->cur_bo->it.last) {
+		pr_err("CMA failed. Range out of bounds\n");
+		return -EFAULT;
+	}
 	return 0;
+}
 
-err_free:
-	amdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->kgd, (struct kgd_mem *)mem);
-err_unlock:
+/* Advance iter by @size bytes. */
+static int kfd_cma_iter_advance(struct cma_iter *ci, unsigned long size)
+{
+	int ret = 0;
+
+	ci->offset += size;
+	if (WARN_ON(size > ci->total || ci->offset > ci->array->size))
+		return -EFAULT;
+	ci->total -= size;
+	/* If current range is copied, move to next range if available. */
+	if (ci->offset == ci->array->size) {
+
+		/* End of all ranges */
+		if (!(--ci->nr_segs))
+			return 0;
+
+		ci->array++;
+		ci->offset = 0;
+		ret = kfd_cma_iter_update_bo(ci);
+		if (ret)
+			return ret;
+	}
+	ci->bo_offset = (ci->array->va_addr + ci->offset) -
+			   ci->cur_bo->it.start;
+	return ret;
+}
+
+static int kfd_cma_iter_init(struct kfd_memory_range *arr, unsigned long segs,
+			     struct kfd_process *p, struct mm_struct *mm,
+			     struct task_struct *task, struct cma_iter *ci)
+{
+	int ret;
+	int nr;
+
+	if (!arr || !segs)
+		return -EINVAL;
+
+	memset(ci, 0, sizeof(*ci));
+	INIT_LIST_HEAD(&ci->cma_list);
+	ci->array = arr;
+	ci->nr_segs = segs;
+	ci->p = p;
+	ci->offset = 0;
+	ci->mm = mm;
+	ci->task = task;
+	for (nr = 0; nr < segs; nr++)
+		ci->total += arr[nr].size;
+
+	/* Valid but size is 0. So copied will also be 0 */
+	if (!ci->total)
+		return 0;
+
+	ret = kfd_cma_iter_update_bo(ci);
+	if (!ret)
+		ci->bo_offset = arr->va_addr - ci->cur_bo->it.start;
+	return ret;
+}
+
+static bool kfd_cma_iter_end(struct cma_iter *ci)
+{
+	if (!(ci->nr_segs) || !(ci->total))
+		return true;
+	return false;
+}
+
+/* Copies @size bytes from si->cur_bo to di->cur_bo BO. The function assumes
+ * both source and dest. BOs are userptr BOs. Both BOs can either belong to
+ * current process or one of the BOs can belong to a differnt
+ * process. @Returns 0 on success, -ve on failure
+ *
+ * @si: Source iter
+ * @di: Dest. iter
+ * @cma_write: Indicates if it is write to remote or read from remote
+ * @size: amount of bytes to be copied
+ * @copied: Return number of bytes actually copied.
+ */
+static int kfd_copy_userptr_bos(struct cma_iter *si, struct cma_iter *di,
+				bool cma_write, uint64_t size,
+				uint64_t *copied)
+{
+	int i, ret = 0, locked;
+	unsigned int nents, nl;
+	unsigned int offset_in_page;
+	struct page *pp_stack[MAX_PP_STACK_COUNT];
+	struct page **process_pages = pp_stack;
+	unsigned long rva, lva = 0, flags = 0;
+	uint64_t copy_size, to_copy = size;
+	struct cma_iter *li, *ri;
+
+	if (cma_write) {
+		ri = di;
+		li = si;
+		flags |= FOLL_WRITE;
+	} else {
+		li = di;
+		ri = si;
+	}
+	/* rva: remote virtual address. Page aligned to start page.
+	 * rva + offset_in_page: Points to remote start address
+	 * lva: local virtual address. Points to the start address.
+	 * nents: computes number of remote pages to request
+	 */
+	offset_in_page = ri->bo_offset & (PAGE_SIZE - 1);
+	rva = (ri->cur_bo->cpuva + ri->bo_offset) & PAGE_MASK;
+	lva = li->cur_bo->cpuva + li->bo_offset;
+
+	nents = (size + offset_in_page + PAGE_SIZE - 1) / PAGE_SIZE;
+
+	copy_size = min_t(uint64_t, size, PAGE_SIZE - offset_in_page);
+	*copied = 0;
+
+	if (nents > MAX_PP_STACK_COUNT) {
+		/* For reliability kmalloc only 2 pages worth */
+		process_pages = kmalloc(min_t(size_t, MAX_KMALLOC_PAGES,
+					      sizeof(struct pages *)*nents),
+					GFP_KERNEL);
+
+		if (!process_pages)
+			return -ENOMEM;
+	}
+
+	while (nents && to_copy) {
+		nl = min_t(unsigned int, MAX_PP_KMALLOC_COUNT, nents);
+		locked = 1;
+		down_read(&ri->mm->mmap_sem);
+		nl = get_user_pages_remote(ri->task, ri->mm, rva, nl,
+					   flags, process_pages, NULL,
+					   &locked);
+		if (locked)
+			up_read(&ri->mm->mmap_sem);
+		if (nl <= 0) {
+			pr_err("CMA: Invalid virtual address 0x%lx\n", rva);
+			ret = -EFAULT;
+			break;
+		}
+
+		for (i = 0; i < nl; i++) {
+			unsigned int n;
+			void *kaddr = kmap(process_pages[i]);
+
+			if (cma_write) {
+				n = copy_from_user(kaddr+offset_in_page,
+						   (void *)lva, copy_size);
+				set_page_dirty(process_pages[i]);
+			} else {
+				n = copy_to_user((void *)lva,
+						 kaddr+offset_in_page,
+						 copy_size);
+			}
+			kunmap(kaddr);
+			if (n) {
+				ret = -EFAULT;
+				break;
+			}
+			to_copy -= copy_size;
+			if (!to_copy)
+				break;
+			lva += copy_size;
+			rva += (copy_size + offset_in_page);
+			WARN_ONCE(rva & (PAGE_SIZE - 1),
+				  "CMA: Error in remote VA computation");
+			offset_in_page = 0;
+			copy_size = min_t(uint64_t, to_copy, PAGE_SIZE);
+		}
+
+		for (i = 0; i < nl; i++)
+			put_page(process_pages[i]);
+
+		if (ret)
+			break;
+		nents -= nl;
+	}
+
+	if (process_pages != pp_stack)
+		kfree(process_pages);
+
+	*copied = (size - to_copy);
+	return ret;
+
+}
+
+static int kfd_create_kgd_mem(struct kfd_dev *kdev, uint64_t size,
+			      struct kfd_process *p, struct kgd_mem **mem)
+{
+	int ret;
+	struct kfd_process_device *pdd = NULL;
+	uint32_t flags = ALLOC_MEM_FLAGS_GTT | ALLOC_MEM_FLAGS_WRITABLE |
+			 ALLOC_MEM_FLAGS_NO_SUBSTITUTE;
+
+	if (!mem || !size || !p || !kdev)
+		return -EINVAL;
+
+	*mem = NULL;
+
+	mutex_lock(&p->mutex);
+	pdd = kfd_get_process_device_data(kdev, p);
+	if (!pdd) {
+		mutex_unlock(&p->mutex);
+		pr_err("Process device data doesn't exist\n");
+		return -EINVAL;
+	}
+
+	ret = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(kdev->kgd, 0ULL, size,
+						      pdd->vm, NULL,
+						      mem, NULL, flags);
 	mutex_unlock(&p->mutex);
+	if (ret) {
+		pr_err("Failed to create shadow system BO %d\n", ret);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int kfd_destroy_kgd_mem(struct kgd_mem *mem)
+{
+	if (!mem)
+		return -EINVAL;
+
+	/* param adev is not used*/
+	return amdgpu_amdkfd_gpuvm_free_memory_of_gpu(NULL, mem);
+}
+
+/* Copies @size bytes from si->cur_bo to di->cur_bo starting at their
+ * respective offset.
+ * @si: Source iter
+ * @di: Dest. iter
+ * @cma_write: Indicates if it is write to remote or read from remote
+ * @size: amount of bytes to be copied
+ * @f: Return the last fence if any
+ * @copied: Return number of bytes actually copied.
+ */
+static int kfd_copy_bos(struct cma_iter *si, struct cma_iter *di,
+			int cma_write, uint64_t size,
+			struct dma_fence **f, uint64_t *copied,
+			struct kgd_mem **tmp_mem)
+{
+	int err = 0;
+	struct kfd_bo *dst_bo = di->cur_bo, *src_bo = si->cur_bo;
+	uint64_t src_offset = si->bo_offset, dst_offset = di->bo_offset;
+	struct kgd_mem *src_mem = src_bo->mem, *dst_mem = dst_bo->mem;
+	struct kfd_dev *dev = dst_bo->dev;
+	int d2d = 0;
+
+	*copied = 0;
+	if (f)
+		*f = NULL;
+	if (src_bo->cpuva && dst_bo->cpuva)
+		return kfd_copy_userptr_bos(si, di, cma_write, size, copied);
+
+	/* If either source or dest. is userptr, create a shadow system BO
+	 * by using the underlying userptr BO pages. Then use this shadow
+	 * BO for copy. src_offset & dst_offset are adjusted because the new BO
+	 * is only created for the window (offset, size) requested.
+	 * The shadow BO is created on the other device. This means if the
+	 * other BO is a device memory, the copy will be using that device.
+	 * The BOs are stored in cma_list for deferred cleanup. This minimizes
+	 * fence waiting just to the last fence.
+	 */
+	if (src_bo->cpuva) {
+		dev = dst_bo->dev;
+		err = kfd_create_cma_system_bo(dev, src_bo, &size,
+					       si->bo_offset, cma_write,
+					       si->p, si->mm, si->task,
+					       &si->cma_bo);
+		src_mem = si->cma_bo->mem;
+		src_offset = si->bo_offset & (PAGE_SIZE - 1);
+		list_add_tail(&si->cma_bo->list, &si->cma_list);
+	} else if (dst_bo->cpuva) {
+		dev = src_bo->dev;
+		err = kfd_create_cma_system_bo(dev, dst_bo, &size,
+					       di->bo_offset, cma_write,
+					       di->p, di->mm, di->task,
+					       &di->cma_bo);
+		dst_mem = di->cma_bo->mem;
+		dst_offset = di->bo_offset & (PAGE_SIZE - 1);
+		list_add_tail(&di->cma_bo->list, &di->cma_list);
+	} else if (src_bo->dev->kgd != dst_bo->dev->kgd) {
+		/* This indicates that atleast on of the BO is in local mem.
+		 * If both are in local mem of different devices then create an
+		 * intermediate System BO and do a double copy
+		 * [VRAM]--gpu1-->[System BO]--gpu2-->[VRAM].
+		 * If only one BO is in VRAM then use that GPU to do the copy
+		 */
+		if (src_bo->mem_type == KFD_IOC_ALLOC_MEM_FLAGS_VRAM &&
+		    dst_bo->mem_type == KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {
+			dev = dst_bo->dev;
+			size = min_t(uint64_t, size, MAX_SYSTEM_BO_SIZE);
+			d2d = 1;
+
+			if (*tmp_mem == NULL) {
+				if (kfd_create_kgd_mem(src_bo->dev,
+							MAX_SYSTEM_BO_SIZE,
+							si->p,
+							tmp_mem))
+					return -EINVAL;
+			}
+
+			if (amdgpu_amdkfd_copy_mem_to_mem(src_bo->dev->kgd,
+						src_bo->mem, si->bo_offset,
+						*tmp_mem, 0,
+						size, f, &size))
+				/* tmp_mem will be freed in caller.*/
+				return -EINVAL;
+
+			kfd_cma_fence_wait(*f);
+			dma_fence_put(*f);
+
+			src_mem = *tmp_mem;
+			src_offset = 0;
+		} else if (src_bo->mem_type == KFD_IOC_ALLOC_MEM_FLAGS_VRAM)
+			dev = src_bo->dev;
+		/* else already set to dst_bo->dev */
+	}
+
+	if (err) {
+		pr_err("Failed to create system BO %d", err);
+		return -EINVAL;
+	}
+
+	err = amdgpu_amdkfd_copy_mem_to_mem(dev->kgd, src_mem, src_offset,
+					    dst_mem, dst_offset, size, f,
+					    copied);
+	/* The tmp_bo allocates additional memory. So it is better to wait and
+	 * delete. Also since multiple GPUs are involved the copies are
+	 * currently not pipelined.
+	 */
+	if (*tmp_mem && d2d) {
+		if (!err) {
+			kfd_cma_fence_wait(*f);
+			dma_fence_put(*f);
+			*f = NULL;
+		}
+	}
+	return err;
+}
+
+/* Copy single range from source iterator @si to destination iterator @di.
+ * @si will move to next range and @di will move by bytes copied.
+ * @return : 0 for success or -ve for failure
+ * @f: The last fence if any
+ * @copied: out: number of bytes copied
+ */
+static int kfd_copy_single_range(struct cma_iter *si, struct cma_iter *di,
+				 bool cma_write, struct dma_fence **f,
+				 uint64_t *copied, struct kgd_mem **tmp_mem)
+{
+	int err = 0;
+	uint64_t copy_size, n;
+	uint64_t size = si->array->size;
+	struct kfd_bo *src_bo = si->cur_bo;
+	struct dma_fence *lfence = NULL;
+
+	if (!src_bo || !di || !copied)
+		return -EINVAL;
+	*copied = 0;
+	if (f)
+		*f = NULL;
+
+	while (size && !kfd_cma_iter_end(di)) {
+		struct dma_fence *fence = NULL;
+
+		copy_size = min(size, (di->array->size - di->offset));
+
+		err = kfd_copy_bos(si, di, cma_write, copy_size,
+				&fence, &n, tmp_mem);
+		if (err) {
+			pr_err("CMA %d failed\n", err);
+			break;
+		}
+
+		if (fence) {
+			err = kfd_fence_put_wait_if_diff_context(fence,
+								 lfence);
+			lfence = fence;
+			if (err)
+				break;
+		}
+
+		size -= n;
+		*copied += n;
+		err = kfd_cma_iter_advance(si, n);
+		if (err)
+			break;
+		err = kfd_cma_iter_advance(di, n);
+		if (err)
+			break;
+	}
+
+	if (f)
+		*f = dma_fence_get(lfence);
+	dma_fence_put(lfence);
+
+	return err;
+}
+
+static int kfd_ioctl_cross_memory_copy(struct file *filep,
+				       struct kfd_process *local_p, void *data)
+{
+	struct kfd_ioctl_cross_memory_copy_args *args = data;
+	struct kfd_memory_range *src_array, *dst_array;
+	struct kfd_process *remote_p;
+	struct task_struct *remote_task;
+	struct mm_struct *remote_mm;
+	struct pid *remote_pid;
+	struct dma_fence *lfence = NULL;
+	uint64_t copied = 0, total_copied = 0;
+	struct cma_iter di, si;
+	const char *cma_op;
+	int err = 0;
+	struct kgd_mem *tmp_mem = NULL;
+
+	/* Check parameters */
+	if (args->src_mem_range_array == 0 || args->dst_mem_range_array == 0 ||
+		args->src_mem_array_size == 0 || args->dst_mem_array_size == 0)
+		return -EINVAL;
+	args->bytes_copied = 0;
+
+	/* Allocate space for source and destination arrays */
+	src_array = kmalloc_array((args->src_mem_array_size +
+				  args->dst_mem_array_size),
+				  sizeof(struct kfd_memory_range),
+				  GFP_KERNEL);
+	if (!src_array)
+		return -ENOMEM;
+	dst_array = &src_array[args->src_mem_array_size];
+
+	if (copy_from_user(src_array, (void __user *)args->src_mem_range_array,
+			   args->src_mem_array_size *
+			   sizeof(struct kfd_memory_range))) {
+		err = -EFAULT;
+		goto copy_from_user_fail;
+	}
+	if (copy_from_user(dst_array, (void __user *)args->dst_mem_range_array,
+			   args->dst_mem_array_size *
+			   sizeof(struct kfd_memory_range))) {
+		err = -EFAULT;
+		goto copy_from_user_fail;
+	}
+
+	/* Get remote process */
+	remote_pid = find_get_pid(args->pid);
+	if (!remote_pid) {
+		pr_err("Cross mem copy failed. Invalid PID %d\n", args->pid);
+		err = -ESRCH;
+		goto copy_from_user_fail;
+	}
+
+	remote_task = get_pid_task(remote_pid, PIDTYPE_PID);
+	if (!remote_pid) {
+		pr_err("Cross mem copy failed. Invalid PID or task died %d\n",
+			args->pid);
+		err = -ESRCH;
+		goto get_pid_task_fail;
+	}
+
+	/* Check access permission */
+	remote_mm = mm_access(remote_task, PTRACE_MODE_ATTACH_REALCREDS);
+	if (!remote_mm || IS_ERR(remote_mm)) {
+		err = IS_ERR(remote_mm) ? PTR_ERR(remote_mm) : -ESRCH;
+		if (err == -EACCES) {
+			pr_err("Cross mem copy failed. Permission error\n");
+			err = -EPERM;
+		} else
+			pr_err("Cross mem copy failed. Invalid task %d\n",
+			       err);
+		goto mm_access_fail;
+	}
+
+	remote_p = kfd_get_process(remote_task);
+	if (IS_ERR(remote_p)) {
+		pr_err("Cross mem copy failed. Invalid kfd process %d\n",
+		       args->pid);
+		err = -EINVAL;
+		goto kfd_process_fail;
+	}
+	/* Initialise cma_iter si & @di with source & destination range. */
+	if (KFD_IS_CROSS_MEMORY_WRITE(args->flags)) {
+		cma_op = "WRITE";
+		pr_debug("CMA WRITE: local -> remote\n");
+		err = kfd_cma_iter_init(dst_array, args->dst_mem_array_size,
+					remote_p, remote_mm, remote_task, &di);
+		if (err)
+			goto kfd_process_fail;
+		err = kfd_cma_iter_init(src_array, args->src_mem_array_size,
+					local_p, current->mm, current, &si);
+		if (err)
+			goto kfd_process_fail;
+	} else {
+		cma_op = "READ";
+		pr_debug("CMA READ: remote -> local\n");
+
+		err = kfd_cma_iter_init(dst_array, args->dst_mem_array_size,
+					local_p, current->mm, current, &di);
+		if (err)
+			goto kfd_process_fail;
+		err = kfd_cma_iter_init(src_array, args->src_mem_array_size,
+					remote_p, remote_mm, remote_task, &si);
+		if (err)
+			goto kfd_process_fail;
+	}
+
+	/* Copy one si range at a time into di. After each call to
+	 * kfd_copy_single_range() si will move to next range. di will be
+	 * incremented by bytes copied
+	 */
+	while (!kfd_cma_iter_end(&si) && !kfd_cma_iter_end(&di)) {
+		struct dma_fence *fence = NULL;
+
+		err = kfd_copy_single_range(&si, &di,
+					KFD_IS_CROSS_MEMORY_WRITE(args->flags),
+					&fence, &copied, &tmp_mem);
+		total_copied += copied;
+
+		if (err)
+			break;
+
+		/* Release old fence if a later fence is created. If no
+		 * new fence is created, then keep the preivous fence
+		 */
+		if (fence) {
+			err = kfd_fence_put_wait_if_diff_context(fence,
+								 lfence);
+			lfence = fence;
+			if (err)
+				break;
+		}
+	}
+
+	/* Wait for the last fence irrespective of error condition */
+	if (lfence) {
+		err = kfd_cma_fence_wait(lfence);
+		dma_fence_put(lfence);
+		if (err)
+			pr_err("CMA %s failed. BO timed out\n", cma_op);
+	}
+
+	if (tmp_mem)
+		kfd_destroy_kgd_mem(tmp_mem);
+
+	kfd_free_cma_bos(&si);
+	kfd_free_cma_bos(&di);
+
+kfd_process_fail:
+	mmput(remote_mm);
+mm_access_fail:
+	put_task_struct(remote_task);
+get_pid_task_fail:
+	put_pid(remote_pid);
+copy_from_user_fail:
+	kfree(src_array);
+
+	/* An error could happen after partial copy. In that case this will
+	 * reflect partial amount of bytes copied
+	 */
+	args->bytes_copied = total_copied;
+	return err;
+}
+
+static int kfd_ioctl_dbg_set_debug_trap(struct file *filep,
+				struct kfd_process *p, void *data)
+{
+	struct kfd_ioctl_dbg_trap_args *args = data;
+	struct kfd_process_device *pdd = NULL;
+	int r = 0;
+	struct kfd_dev *dev = NULL;
+	struct kfd_process *target = NULL;
+	struct pid *pid = NULL;
+	uint32_t *queue_id_array = NULL;
+	uint32_t gpu_id;
+	uint32_t debug_trap_action;
+	uint32_t data1;
+	uint32_t data2;
+	uint32_t data3;
+	bool is_suspend_or_resume;
+
+	debug_trap_action = args->op;
+	gpu_id = args->gpu_id;
+	data1 = args->data1;
+	data2 = args->data2;
+	data3 = args->data3;
+
+	if (sched_policy == KFD_SCHED_POLICY_NO_HWS) {
+		pr_err("Unsupported sched_policy: %i", sched_policy);
+		r = -EINVAL;
+		goto out;
+	}
+
+	is_suspend_or_resume =
+		debug_trap_action == KFD_IOC_DBG_TRAP_NODE_SUSPEND ||
+		debug_trap_action == KFD_IOC_DBG_TRAP_NODE_RESUME;
+
+
+	pid = find_get_pid(args->pid);
+	if (!pid) {
+		pr_err("Cannot find pid info for %i\n",
+				args->pid);
+		r =  -ESRCH;
+		goto out;
+	}
+
+	target = kfd_lookup_process_by_pid(pid);
+	if (!target) {
+		pr_err("Cannot find process info info for %i\n",
+				args->pid);
+		r = -ESRCH;
+		goto out;
+	}
+
+	if (target != p) {
+		bool is_debugger_attached = false;
+
+		rcu_read_lock();
+		if (ptrace_parent(target->lead_thread) == current)
+			is_debugger_attached = true;
+		rcu_read_unlock();
+
+		if (!is_debugger_attached) {
+			pr_err("Cannot debug process\n");
+			r = -ESRCH;
+			goto out;
+		}
+	}
+
+	mutex_lock(&target->mutex);
+
+	if (!is_suspend_or_resume) {
+
+		dev = kfd_device_by_id(args->gpu_id);
+		if (!dev) {
+			r = -EINVAL;
+			goto unlock_out;
+		}
+
+		if (dev->device_info->asic_family < CHIP_VEGA10) {
+			r = -EINVAL;
+			goto unlock_out;
+		}
+
+		pdd = kfd_get_process_device_data(dev, target);
+
+		if (!pdd) {
+			r = -EINVAL;
+			goto unlock_out;
+		}
+
+		if ((pdd->is_debugging_enabled == false) &&
+				((debug_trap_action == KFD_IOC_DBG_TRAP_ENABLE
+				  && data1 == 1) ||
+				 (debug_trap_action ==
+				  KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE &&
+				  data1 != 0))) {
+
+			/* We need to reserve the debug trap vmid if we haven't
+			 * yet, and are enabling trap debugging, or we are
+			 * setting the wave launch mode to something other than
+			 * normal==0.
+			 */
+			r = reserve_debug_trap_vmid(dev->dqm);
+			if (r)
+				goto unlock_out;
+
+			pdd->is_debugging_enabled = true;
+		}
+
+		if (!pdd->is_debugging_enabled) {
+			pr_err("Debugging is not enabled for this device\n");
+			r = -EINVAL;
+			goto unlock_out;
+		}
+	} else {
+		/* data 2 has the number of queue IDs */
+		size_t queue_id_array_size = sizeof(uint32_t) * data2;
+
+		queue_id_array = kzalloc(queue_id_array_size, GFP_KERNEL);
+		if (!queue_id_array) {
+			r = -ENOMEM;
+			goto unlock_out;
+		}
+		/* We need to copy the queue IDs from userspace */
+		if (copy_from_user(queue_id_array,
+					(uint32_t *) args->ptr,
+					queue_id_array_size)) {
+			r = -EFAULT;
+			goto unlock_out;
+		}
+	}
+
+
+	switch (debug_trap_action) {
+	case KFD_IOC_DBG_TRAP_ENABLE:
+		switch (data1) {
+		case 0:
+			pdd->debug_trap_enabled = false;
+			r = dev->kfd2kgd->disable_debug_trap(dev->kgd);
+			break;
+		case 1:
+			pdd->debug_trap_enabled = true;
+			r = dev->kfd2kgd->enable_debug_trap(dev->kgd,
+					pdd->trap_debug_wave_launch_mode,
+					dev->vm_info.last_vmid_kfd);
+			break;
+		default:
+			pr_err("Invalid trap enable option: %i\n",
+					data1);
+			r = -EINVAL;
+		}
+		break;
+
+	case KFD_IOC_DBG_TRAP_SET_TRAP_DATA:
+		r = dev->kfd2kgd->set_debug_trap_data(dev->kgd,
+				data1,
+				data2);
+		break;
+
+	case KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_OVERRIDE:
+		r = dev->kfd2kgd->set_wave_launch_trap_override(
+				dev->kgd,
+				data1,
+				data2);
+		break;
+
+	case KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE:
+		pdd->trap_debug_wave_launch_mode = data1;
+		r = dev->kfd2kgd->set_wave_launch_mode(
+				dev->kgd,
+				data1,
+				dev->vm_info.last_vmid_kfd);
+		break;
+
+	case KFD_IOC_DBG_TRAP_NODE_SUSPEND:
+		r = suspend_queues(target,
+				data2, /* Number of queues */
+				data3, /* Grace Period */
+				data1, /* Flags */
+				queue_id_array); /* array of queue ids */
+		if (r)
+			goto unlock_out;
+		break;
+
+	case KFD_IOC_DBG_TRAP_NODE_RESUME:
+		r = resume_queues(target,
+				data2, /* Number of queues */
+				data1, /* Flags */
+				queue_id_array); /* array of queue ids */
+		if (r)
+			goto unlock_out;
+		break;
+	default:
+		pr_err("Invalid option: %i\n", debug_trap_action);
+		r = -EINVAL;
+	}
+
+	if (pdd && pdd->trap_debug_wave_launch_mode == 0 &&
+			!pdd->debug_trap_enabled) {
+		int result;
+
+		result = release_debug_trap_vmid(dev->dqm);
+		if (result) {
+			pr_err("Failed to release debug VMID\n");
+			r = result;
+			goto unlock_out;
+		}
+
+		pdd->is_debugging_enabled = false;
+	}
+
+unlock_out:
+	mutex_unlock(&target->mutex);
+
+out:
+	if (pid)
+		put_pid(pid);
+	if (target)
+		kfd_unref_process(target);
+	kfree(queue_id_array);
 	return r;
 }
 
@@ -1769,6 +2901,21 @@ static const struct amdkfd_ioctl_desc amdkfd_ioctls[] = {
 	AMDKFD_IOCTL_DEF(AMDKFD_IOC_IMPORT_DMABUF,
 				kfd_ioctl_import_dmabuf, 0),
 
+	AMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_QUEUE_GWS,
+			kfd_ioctl_alloc_queue_gws, 0),
+
+        AMDKFD_IOCTL_DEF(AMDKFD_IOC_IPC_IMPORT_HANDLE,
+                                kfd_ioctl_ipc_import_handle, 0),
+
+        AMDKFD_IOCTL_DEF(AMDKFD_IOC_IPC_EXPORT_HANDLE,
+                                kfd_ioctl_ipc_export_handle, 0),
+
+        AMDKFD_IOCTL_DEF(AMDKFD_IOC_CROSS_MEMORY_COPY,
+                                kfd_ioctl_cross_memory_copy, 0),
+
+        AMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_TRAP,
+                        kfd_ioctl_dbg_set_debug_trap, 0),
+
 };
 
 #define AMDKFD_CORE_IOCTL_COUNT	ARRAY_SIZE(amdkfd_ioctls)
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c b/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
old mode 100644
new mode 100755
index 1ef3c32852d9..9ad9e42ddd12
--- a/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_crat.c
@@ -750,6 +750,7 @@ static int kfd_fill_gpu_cache_info(struct kfd_dev *kdev,
  *
  *	Return 0 if successful else return error code
  */
+#ifdef CONFIG_ACPI
 int kfd_create_crat_image_acpi(void **crat_image, size_t *size)
 {
 	struct acpi_table_header *crat_table;
@@ -787,6 +788,7 @@ int kfd_create_crat_image_acpi(void **crat_image, size_t *size)
 
 	return 0;
 }
+#endif
 
 /* Memory required to create Virtual CRAT.
  * Since there is no easy way to predict the amount of memory required, the
@@ -935,8 +937,6 @@ static int kfd_fill_iolink_info_for_cpu(int numa_node_id, int *avail_size,
 static int kfd_create_vcrat_image_cpu(void *pcrat_image, size_t *size)
 {
 	struct crat_header *crat_table = (struct crat_header *)pcrat_image;
-	struct acpi_table_header *acpi_table;
-	acpi_status status;
 	struct crat_subtype_generic *sub_type_hdr;
 	int avail_size = *size;
 	int numa_node_id;
@@ -944,6 +944,10 @@ static int kfd_create_vcrat_image_cpu(void *pcrat_image, size_t *size)
 	uint32_t entries = 0;
 #endif
 	int ret = 0;
+#ifdef CONFIG_ACPI
+	struct acpi_table_header *acpi_table;
+	acpi_status status;
+#endif
 
 	if (!pcrat_image || avail_size < VCRAT_SIZE_FOR_CPU)
 		return -EINVAL;
@@ -960,6 +964,7 @@ static int kfd_create_vcrat_image_cpu(void *pcrat_image, size_t *size)
 			sizeof(crat_table->signature));
 	crat_table->length = sizeof(struct crat_header);
 
+#ifdef CONFIG_ACPI
 	status = acpi_get_table("DSDT", 0, &acpi_table);
 	if (status != AE_OK)
 		pr_warn("DSDT table not found for OEM information\n");
@@ -970,6 +975,11 @@ static int kfd_create_vcrat_image_cpu(void *pcrat_image, size_t *size)
 		memcpy(crat_table->oem_table_id, acpi_table->oem_table_id,
 				CRAT_OEMTABLEID_LENGTH);
 	}
+#else
+	crat_table->oem_revision = 0;
+	memcpy(crat_table->oem_id, "INV", CRAT_OEMID_LENGTH);
+	memcpy(crat_table->oem_table_id, "UNAVAIL", CRAT_OEMTABLEID_LENGTH);
+#endif
 	crat_table->total_entries = 0;
 	crat_table->num_domains = 0;
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_crat.h b/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
old mode 100644
new mode 100755
index d54ceebd346b..6f59fb02e5e3
--- a/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_crat.h
@@ -316,7 +316,9 @@ struct cdit_header {
 
 struct kfd_dev;
 
+#ifdef CONFIG_ACPI
 int kfd_create_crat_image_acpi(void **crat_image, size_t *size);
+#endif
 void kfd_destroy_crat_image(void *crat_image);
 int kfd_parse_crat_table(void *crat_image, struct list_head *device_list,
 			 uint32_t proximity_domain);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
old mode 100644
new mode 100755
index a3441b0e385b..3635e0b4b3b7
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.c
@@ -29,7 +29,7 @@
 #include <linux/mutex.h>
 #include <linux/device.h>
 
-#include "kfd_pm4_headers.h"
+#include "kfd_pm4_headers_vi.h"
 #include "kfd_pm4_headers_diq.h"
 #include "kfd_kernel_queue.h"
 #include "kfd_priv.h"
@@ -46,9 +46,10 @@ static void dbgdev_address_watch_disable_nodiq(struct kfd_dev *dev)
 
 static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 				unsigned int pasid, uint64_t vmid0_address,
-				uint32_t *packet_buff, size_t size_in_bytes)
+				uint32_t *packet_buff, size_t size_in_bytes,
+				bool sync)
 {
-	struct pm4__release_mem *rm_packet;
+	struct pm4_mec_release_mem *rm_packet;
 	struct pm4__indirect_buffer_pasid *ib_packet;
 	struct kfd_mem_obj *mem_obj;
 	size_t pq_packets_size_in_bytes;
@@ -64,8 +65,9 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 
 	kq = dbgdev->kq;
 
-	pq_packets_size_in_bytes = sizeof(struct pm4__release_mem) +
-				sizeof(struct pm4__indirect_buffer_pasid);
+	pq_packets_size_in_bytes = sizeof(struct pm4__indirect_buffer_pasid);
+	if (sync)
+		pq_packets_size_in_bytes += sizeof(struct pm4_mec_release_mem);
 
 	/*
 	 * We acquire a buffer from DIQ
@@ -98,6 +100,11 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 
 	ib_packet->bitfields5.pasid = pasid;
 
+	if (!sync) {
+		kq->ops.submit_packet(kq);
+		return status;
+	}
+
 	/*
 	 * for now we use release mem for GPU-CPU synchronization
 	 * Consider WaitRegMem + WriteData as a better alternative
@@ -106,7 +113,7 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 	 * (a) Sync with HW
 	 * (b) Sync var is written by CP to mem.
 	 */
-	rm_packet = (struct pm4__release_mem *) (ib_packet_buff +
+	rm_packet = (struct pm4_mec_release_mem *) (ib_packet_buff +
 			(sizeof(struct pm4__indirect_buffer_pasid) /
 					sizeof(unsigned int)));
 
@@ -125,7 +132,7 @@ static int dbgdev_diq_submit_ib(struct kfd_dbgdev *dbgdev,
 
 	rm_packet->header.opcode = IT_RELEASE_MEM;
 	rm_packet->header.type = PM4_TYPE_3;
-	rm_packet->header.count = sizeof(struct pm4__release_mem) / 4 - 2;
+	rm_packet->header.count = sizeof(struct pm4_mec_release_mem) / 4 - 2;
 
 	rm_packet->bitfields2.event_type = CACHE_FLUSH_AND_INV_TS_EVENT;
 	rm_packet->bitfields2.event_index =
@@ -231,7 +238,8 @@ static void dbgdev_address_watch_set_registers(
 			union TCP_WATCH_ADDR_H_BITS *addrHi,
 			union TCP_WATCH_ADDR_L_BITS *addrLo,
 			union TCP_WATCH_CNTL_BITS *cntl,
-			unsigned int index, unsigned int vmid)
+			unsigned int index, unsigned int vmid,
+			bool is_apu)
 {
 	union ULARGE_INTEGER addr;
 
@@ -256,9 +264,9 @@ static void dbgdev_address_watch_set_registers(
 
 	cntl->bitfields.mode = adw_info->watch_mode[index];
 	cntl->bitfields.vmid = (uint32_t) vmid;
-	/* for now assume it is an ATC address */
-	cntl->u32All |= ADDRESS_WATCH_REG_CNTL_ATC_BIT;
-
+	/* for APU assume it is an ATC address */
+	if (is_apu)
+		cntl->u32All |= ADDRESS_WATCH_REG_CNTL_ATC_BIT;
 	pr_debug("\t\t%20s %08x\n", "set reg mask :", cntl->bitfields.mask);
 	pr_debug("\t\t%20s %08x\n", "set reg add high :",
 			addrHi->bitfields.addr);
@@ -300,7 +308,8 @@ static int dbgdev_address_watch_nodiq(struct kfd_dbgdev *dbgdev,
 
 	for (i = 0; i < adw_info->num_watch_points; i++) {
 		dbgdev_address_watch_set_registers(adw_info, &addrHi, &addrLo,
-						&cntl, i, pdd->qpd.vmid);
+				&cntl, i, pdd->qpd.vmid,
+				dbgdev->dev->device_info->needs_iommu_device);
 
 		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
 		pr_debug("\t\t%20s %08x\n", "register index :", i);
@@ -339,9 +348,9 @@ static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 	union TCP_WATCH_ADDR_H_BITS addrHi;
 	union TCP_WATCH_ADDR_L_BITS addrLo;
 	union TCP_WATCH_CNTL_BITS cntl;
-	struct kfd_mem_obj *mem_obj;
 	unsigned int aw_reg_add_dword;
 	uint32_t *packet_buff_uint;
+	uint64_t packet_buff_gpu_addr;
 	unsigned int i;
 	int status;
 	size_t ib_size = sizeof(struct pm4__set_config_reg) * 4;
@@ -363,15 +372,13 @@ static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 		return -EINVAL;
 	}
 
-	status = kfd_gtt_sa_allocate(dbgdev->dev, ib_size, &mem_obj);
-
+	status = dbgdev->kq->ops.acquire_inline_ib(dbgdev->kq,
+			ib_size/sizeof(uint32_t),
+			&packet_buff_uint, &packet_buff_gpu_addr);
 	if (status) {
-		pr_err("Failed to allocate GART memory\n");
+		pr_err("Failed to allocate IB from DIQ ring\n");
 		return status;
 	}
-
-	packet_buff_uint = mem_obj->cpu_ptr;
-
 	memset(packet_buff_uint, 0, ib_size);
 
 	packets_vec = (struct pm4__set_config_reg *) (packet_buff_uint);
@@ -390,12 +397,9 @@ static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 	packets_vec[3].bitfields2.insert_vmid = 1;
 
 	for (i = 0; i < adw_info->num_watch_points; i++) {
-		dbgdev_address_watch_set_registers(adw_info,
-						&addrHi,
-						&addrLo,
-						&cntl,
-						i,
-						vmid);
+		dbgdev_address_watch_set_registers(adw_info, &addrHi, &addrLo,
+				&cntl, i, vmid,
+				dbgdev->dev->device_info->needs_iommu_device);
 
 		pr_debug("\t\t%30s\n", "* * * * * * * * * * * * * * * * * *");
 		pr_debug("\t\t%20s %08x\n", "register index :", i);
@@ -468,24 +472,24 @@ static int dbgdev_address_watch_diq(struct kfd_dbgdev *dbgdev,
 		status = dbgdev_diq_submit_ib(
 					dbgdev,
 					adw_info->process->pasid,
-					mem_obj->gpu_addr,
+					packet_buff_gpu_addr,
 					packet_buff_uint,
-					ib_size);
+					ib_size, true);
 
 		if (status) {
 			pr_err("Failed to submit IB to DIQ\n");
-			break;
+			return status;
 		}
 	}
 
-	kfd_gtt_sa_free(dbgdev->dev, mem_obj);
 	return status;
 }
 
 static int dbgdev_wave_control_set_registers(
 				struct dbg_wave_control_info *wac_info,
 				union SQ_CMD_BITS *in_reg_sq_cmd,
-				union GRBM_GFX_INDEX_BITS *in_reg_gfx_index)
+				union GRBM_GFX_INDEX_BITS *in_reg_gfx_index,
+				unsigned int asic_family)
 {
 	int status = 0;
 	union SQ_CMD_BITS reg_sq_cmd;
@@ -543,11 +547,25 @@ static int dbgdev_wave_control_set_registers(
 
 	switch (wac_info->operand) {
 	case HSA_DBG_WAVEOP_HALT:
-		reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_HALT;
+		if (asic_family == CHIP_KAVERI) {
+			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_HALT;
+			pr_debug("Halting KV\n");
+		} else {
+			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
+			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_HALT;
+			pr_debug("Halting CZ\n");
+		}
 		break;
 
 	case HSA_DBG_WAVEOP_RESUME:
-		reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_RESUME;
+		if (asic_family == CHIP_KAVERI) {
+			reg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_RESUME;
+			pr_debug("Resuming KV\n");
+		} else {
+			reg_sq_cmd.bits_sethalt.cmd  = SQ_IND_CMD_NEW_SETHALT;
+			reg_sq_cmd.bits_sethalt.data = SQ_IND_CMD_DATA_RESUME;
+			pr_debug("Resuming CZ\n");
+		}
 		break;
 
 	case HSA_DBG_WAVEOP_KILL:
@@ -587,15 +605,15 @@ static int dbgdev_wave_control_diq(struct kfd_dbgdev *dbgdev,
 	int status;
 	union SQ_CMD_BITS reg_sq_cmd;
 	union GRBM_GFX_INDEX_BITS reg_gfx_index;
-	struct kfd_mem_obj *mem_obj;
 	uint32_t *packet_buff_uint;
+	uint64_t packet_buff_gpu_addr;
 	struct pm4__set_config_reg *packets_vec;
 	size_t ib_size = sizeof(struct pm4__set_config_reg) * 3;
 
 	reg_sq_cmd.u32All = 0;
 
 	status = dbgdev_wave_control_set_registers(wac_info, &reg_sq_cmd,
-							&reg_gfx_index);
+			&reg_gfx_index,	dbgdev->dev->device_info->asic_family);
 	if (status) {
 		pr_err("Failed to set wave control registers\n");
 		return status;
@@ -634,15 +652,13 @@ static int dbgdev_wave_control_diq(struct kfd_dbgdev *dbgdev,
 
 	pr_debug("\t\t %30s\n", "* * * * * * * * * * * * * * * * * *");
 
-	status = kfd_gtt_sa_allocate(dbgdev->dev, ib_size, &mem_obj);
-
-	if (status != 0) {
-		pr_err("Failed to allocate GART memory\n");
+	status = dbgdev->kq->ops.acquire_inline_ib(dbgdev->kq,
+			ib_size / sizeof(uint32_t),
+			&packet_buff_uint, &packet_buff_gpu_addr);
+	if (status) {
+		pr_err("Failed to allocate IB from DIQ ring\n");
 		return status;
 	}
-
-	packet_buff_uint = mem_obj->cpu_ptr;
-
 	memset(packet_buff_uint, 0, ib_size);
 
 	packets_vec =  (struct pm4__set_config_reg *) packet_buff_uint;
@@ -682,15 +698,13 @@ static int dbgdev_wave_control_diq(struct kfd_dbgdev *dbgdev,
 	status = dbgdev_diq_submit_ib(
 			dbgdev,
 			wac_info->process->pasid,
-			mem_obj->gpu_addr,
+			packet_buff_gpu_addr,
 			packet_buff_uint,
-			ib_size);
+			ib_size, false);
 
 	if (status)
 		pr_err("Failed to submit IB to DIQ\n");
 
-	kfd_gtt_sa_free(dbgdev->dev, mem_obj);
-
 	return status;
 }
 
@@ -712,7 +726,7 @@ static int dbgdev_wave_control_nodiq(struct kfd_dbgdev *dbgdev,
 		return -EFAULT;
 	}
 	status = dbgdev_wave_control_set_registers(wac_info, &reg_sq_cmd,
-							&reg_gfx_index);
+			&reg_gfx_index,	dbgdev->dev->device_info->asic_family);
 	if (status) {
 		pr_err("Failed to set wave control registers\n");
 		return status;
@@ -804,7 +818,7 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p)
 		return -EFAULT;
 
 	status = dbgdev_wave_control_set_registers(&wac_info, &reg_sq_cmd,
-			&reg_gfx_index);
+			&reg_gfx_index, dev->device_info->asic_family);
 	if (status != 0)
 		return -EINVAL;
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
old mode 100644
new mode 100755
index 0619c777b47e..65579bc33c95
--- a/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_dbgdev.h
@@ -63,6 +63,22 @@ enum {
 /* SQ_CMD definitions */
 #define SQ_CMD						0x8DEC
 
+enum {
+	SQ_IND_CMD_DATA_RESUME = 0,
+	SQ_IND_CMD_DATA_HALT = 1
+};
+
+enum SQ_IND_CMD_NEW {
+	SQ_IND_CMD_NEW_NULL = 0x00000000,
+	SQ_IND_CMD_NEW_SETHALT = 0x00000001,
+	SQ_IND_CMD_NEW_SAVECTX = 0x00000002,
+	SQ_IND_CMD_NEW_KILL = 0x00000003,
+	SQ_IND_CMD_NEW_DEBUG = 0x00000004,
+	SQ_IND_CMD_NEW_TRAP = 0x00000005,
+	SQ_IND_CMD_NEW_SET_PRIO = 0x00000006
+
+};
+
 enum SQ_IND_CMD_CMD {
 	SQ_IND_CMD_CMD_NULL = 0x00000000,
 	SQ_IND_CMD_CMD_HALT = 0x00000001,
@@ -121,6 +137,20 @@ union SQ_CMD_BITS {
 		 uint32_t:1;
 		uint32_t vm_id:4;
 	} bitfields, bits;
+	struct {
+		uint32_t cmd:3;
+		 uint32_t:1;
+		uint32_t mode:3;
+		uint32_t check_vmid:1;
+		uint32_t data:3;
+		 uint32_t:5;
+		uint32_t wave_id:4;
+		uint32_t simd_id:2;
+		 uint32_t:2;
+		uint32_t queue_id:3;
+		 uint32_t:1;
+		uint32_t vm_id:4;
+	} bitfields_sethalt, bits_sethalt;
 	uint32_t u32All;
 	signed int i32All;
 	float f32All;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device.c b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
old mode 100644
new mode 100755
index f329b82f11d9..06461ac730d4
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device.c
@@ -514,10 +514,12 @@ static void kfd_cwsr_init(struct kfd_dev *kfd)
 }
 
 bool kgd2kfd_device_init(struct kfd_dev *kfd,
+			 struct drm_device *ddev,
 			 const struct kgd2kfd_shared_resources *gpu_resources)
 {
 	unsigned int size;
 
+	kfd->ddev = ddev;
 	kfd->mec_fw_version = amdgpu_amdkfd_get_fw_version(kfd->kgd,
 			KGD_ENGINE_MEC1);
 	kfd->sdma_fw_version = amdgpu_amdkfd_get_fw_version(kfd->kgd,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
old mode 100644
new mode 100755
index f8c400287fb4..10794e5cabda
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -25,6 +25,7 @@
 #include <linux/printk.h>
 #include <linux/slab.h>
 #include <linux/list.h>
+#include <linux/mmu_context.h>
 #include <linux/types.h>
 #include <linux/bitops.h>
 #include <linux/sched.h>
@@ -132,6 +133,31 @@ void program_sh_mem_settings(struct device_queue_manager *dqm,
 						qpd->sh_mem_bases);
 }
 
+bool check_if_queues_active(struct device_queue_manager *dqm,
+		struct qcm_process_device *qpd)
+{
+	bool busy = false;
+	struct queue *q;
+
+	dqm_lock(dqm);
+	list_for_each_entry(q, &qpd->queues_list, list) {
+		struct mqd_manager *mqd_mgr;
+		enum KFD_MQD_TYPE type;
+
+		type = get_mqd_type_from_queue_type(q->properties.type);
+		mqd_mgr = dqm->mqd_mgrs[type];
+		if (!mqd_mgr || !mqd_mgr->check_queue_active)
+			continue;
+
+		busy = mqd_mgr->check_queue_active(q);
+		if (busy)
+			break;
+	}
+	dqm_unlock(dqm);
+
+	return busy;
+}
+
 static int allocate_doorbell(struct qcm_process_device *qpd, struct queue *q)
 {
 	struct kfd_dev *dev = qpd->dqm->dev;
@@ -566,6 +592,66 @@ static int update_queue(struct device_queue_manager *dqm, struct queue *q)
 	return retval;
 }
 
+/* suspend_single_queue does not lock the dqm like the
+ * evict_process_queues_cpsch or evict_process_queues_nocpsch. You should
+ * lock the dqm before calling, and unlock after calling.
+ *
+ * The reason we don't lock the dqm is because this function may be
+ * called on multipe queues in a loop, so rather than locking/unlocking
+ * multiple times, we will just keep the dqm locked for all of the calls.
+ */
+static int suspend_single_queue(struct device_queue_manager *dqm,
+				      struct kfd_process_device *pdd,
+				      struct queue *q)
+{
+	int retval = 0;
+
+	pr_debug("Suspending PASID %u queue [%i]\n",
+			pdd->process->pasid,
+			q->properties.queue_id);
+
+	q->properties.is_suspended = true;
+	if (q->properties.is_active) {
+		dqm->queue_count--;
+		q->properties.is_active = false;
+	}
+
+	return retval;
+}
+
+/* resume_single_queue does not lock the dqm like the functions
+ * restore_process_queues_cpsch or restore_process_queues_nocpsch. You should
+ * lock the dqm before calling, and unlock after calling.
+ *
+ * The reason we don't lock the dqm is because this function may be
+ * called on multipe queues in a loop, so rather than locking/unlocking
+ * multiple times, we will just keep the dqm locked for all of the calls.
+ */
+static int resume_single_queue(struct device_queue_manager *dqm,
+				      struct qcm_process_device *qpd,
+				      struct queue *q)
+{
+	struct kfd_process_device *pdd;
+	uint64_t pd_base;
+	int retval = 0;
+
+	pdd = qpd_to_pdd(qpd);
+	/* Retrieve PD base */
+	pd_base = amdgpu_amdkfd_gpuvm_get_process_page_dir(pdd->vm);
+
+	pr_debug("Restoring from suspend PASID %u queue [%i]\n",
+			    pdd->process->pasid,
+			    q->properties.queue_id);
+
+	q->properties.is_suspended = false;
+
+	if (QUEUE_IS_ACTIVE(q->properties)) {
+		q->properties.is_active = true;
+		dqm->queue_count++;
+	}
+
+	return retval;
+}
 static int evict_process_queues_nocpsch(struct device_queue_manager *dqm,
 					struct qcm_process_device *qpd)
 {
@@ -869,6 +955,7 @@ static int initialize_nocpsch(struct device_queue_manager *dqm)
 	dqm->queue_count = dqm->next_pipe_to_allocate = 0;
 	dqm->sdma_queue_count = 0;
 	dqm->xgmi_sdma_queue_count = 0;
+	dqm->trap_debug_vmid = 0;
 
 	for (pipe = 0; pipe < get_pipes_per_mec(dqm); pipe++) {
 		int pipe_offset = pipe * get_queues_per_pipe(dqm);
@@ -1021,6 +1108,7 @@ static int initialize_cpsch(struct device_queue_manager *dqm)
 	dqm->active_runlist = false;
 	dqm->sdma_bitmap = ~0ULL >> (64 - get_num_sdma_queues(dqm));
 	dqm->xgmi_sdma_bitmap = ~0ULL >> (64 - get_num_xgmi_sdma_queues(dqm));
+	dqm->trap_debug_vmid = 0;
 
 	INIT_WORK(&dqm->hw_exception_work, kfd_process_hw_exception);
 
@@ -1852,6 +1940,289 @@ static void kfd_process_hw_exception(struct work_struct *work)
 	amdgpu_amdkfd_gpu_reset(dqm->dev->kgd);
 }
 
+/*
+ * Reserves a vmid for the trap debugger
+ */
+int reserve_debug_trap_vmid(struct device_queue_manager *dqm)
+{
+	int r;
+	int updated_vmid_mask;
+
+	if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {
+		pr_err("Unsupported on sched_policy: %i\n", dqm->sched_policy);
+		return -EINVAL;
+	}
+
+	dqm_lock(dqm);
+
+	if (dqm->trap_debug_vmid != 0) {
+		pr_err("Trap debug id already reserved\n");
+		r = -EINVAL;
+		goto out_unlock;
+	}
+
+	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+	if (r)
+		goto out_unlock;
+
+	updated_vmid_mask = dqm->dev->shared_resources.compute_vmid_bitmap;
+	updated_vmid_mask &= ~(1 << dqm->dev->vm_info.last_vmid_kfd);
+
+	dqm->dev->shared_resources.compute_vmid_bitmap = updated_vmid_mask;
+	dqm->trap_debug_vmid = dqm->dev->vm_info.last_vmid_kfd;
+	r = set_sched_resources(dqm);
+	if (r)
+		goto out_unlock;
+
+	r = map_queues_cpsch(dqm);
+	if (r)
+		goto out_unlock;
+
+	pr_debug("Reserved VMID for trap debug: %i\n", dqm->trap_debug_vmid);
+
+out_unlock:
+	dqm_unlock(dqm);
+	return r;
+}
+
+/*
+ * Releases vmid for the trap debugger
+ */
+int release_debug_trap_vmid(struct device_queue_manager *dqm)
+{
+	int r;
+	int updated_vmid_mask;
+	uint32_t trap_debug_vmid;
+
+	if (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {
+		pr_err("Unsupported on sched_policy: %i\n", dqm->sched_policy);
+		return -EINVAL;
+	}
+
+	dqm_lock(dqm);
+	trap_debug_vmid = dqm->trap_debug_vmid;
+	if (dqm->trap_debug_vmid == 0) {
+		pr_err("Trap debug id is not reserved\n");
+		r = -EINVAL;
+		goto out_unlock;
+	}
+
+	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+	if (r)
+		goto out_unlock;
+
+	updated_vmid_mask = dqm->dev->shared_resources.compute_vmid_bitmap;
+	updated_vmid_mask |= (1 << dqm->dev->vm_info.last_vmid_kfd);
+
+	dqm->dev->shared_resources.compute_vmid_bitmap = updated_vmid_mask;
+	dqm->trap_debug_vmid = 0;
+	r = set_sched_resources(dqm);
+	if (r)
+		goto out_unlock;
+
+	r = map_queues_cpsch(dqm);
+	if (r)
+		goto out_unlock;
+
+	pr_debug("Released VMID for trap debug: %i\n", trap_debug_vmid);
+
+out_unlock:
+	dqm_unlock(dqm);
+	return r;
+}
+
+bool queue_id_in_array(unsigned int queue_id,
+		uint32_t num_queues,
+		uint32_t *queue_ids)
+{
+	int i;
+
+	for (i = 0; i < num_queues; i++)
+		if (queue_id == queue_ids[i])
+			return true;
+	return false;
+}
+
+struct copy_context_work_handler_workarea {
+	struct work_struct copy_context_work;
+	struct kfd_process *p;
+};
+
+void copy_context_work_handler (struct work_struct *work)
+{
+	struct copy_context_work_handler_workarea *workarea;
+	struct mqd_manager *mqd_mgr;
+	struct kfd_process_device *pdd;
+	struct queue *q;
+	struct mm_struct *mm;
+	struct kfd_process *p;
+	uint32_t tmp_ctl_stack_used_size, tmp_save_area_used_size;
+
+	workarea = container_of(work,
+			struct copy_context_work_handler_workarea,
+			copy_context_work);
+
+	p = workarea->p;
+	mm = get_task_mm(p->lead_thread);
+	use_mm(mm);
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
+		struct device_queue_manager *dqm = pdd->dev->dqm;
+		struct qcm_process_device *qpd = &pdd->qpd;
+
+		dqm_lock(dqm);
+
+
+		list_for_each_entry(q, &qpd->queues_list, list) {
+			mqd_mgr = dqm->mqd_mgrs[KFD_MQD_TYPE_COMPUTE];
+
+			/* We ignore the return value from get_wave_state
+			 * because
+			 * i) right now, it always returns 0, and
+			 * ii) if we hit an error, we would continue to the
+			 *      next queue anyway.
+			 */
+			mqd_mgr->get_wave_state(mqd_mgr,
+					q->mqd,
+					(void __user *)	q->properties.ctx_save_restore_area_address,
+					&tmp_ctl_stack_used_size,
+					&tmp_save_area_used_size);
+		}
+
+		dqm_unlock(dqm);
+	}
+	unuse_mm(mm);
+	mmput(mm);
+}
+
+int suspend_queues(struct kfd_process *p,
+			uint32_t num_queues,
+			uint32_t grace_period,
+			uint32_t flags,
+			uint32_t *queue_ids)
+{
+	int r = -ENODEV;
+	bool any_queues_suspended = false;
+	struct kfd_process_device *pdd;
+	struct queue *q;
+
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
+		bool queues_suspended_on_device = false;
+		struct device_queue_manager *dqm = pdd->dev->dqm;
+		struct qcm_process_device *qpd = &pdd->qpd;
+
+		dqm_lock(dqm);
+
+		/* We need to loop over all of the queues on this
+		 * device, and check if it is in the list passed in,
+		 * and if it is, we will evict it.
+		 */
+		list_for_each_entry(q, &qpd->queues_list, list) {
+			if (queue_id_in_array(q->properties.queue_id,
+						num_queues,
+						queue_ids)) {
+				if (q->properties.is_suspended)
+					continue;
+				r = suspend_single_queue(dqm,
+						pdd,
+						q);
+				if (r) {
+					pr_err("Failed to suspend process queues. queue_id == %i\n",
+							q->properties.queue_id);
+					dqm_unlock(dqm);
+					return r;
+				}
+				queues_suspended_on_device = true;
+				any_queues_suspended = true;
+			}
+		}
+
+		if (queues_suspended_on_device) {
+			r = execute_queues_cpsch(dqm,
+				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+			if (r) {
+				pr_err("Failed to suspend process queues.\n");
+				dqm_unlock(dqm);
+				return r;
+			}
+		}
+
+		dqm_unlock(dqm);
+		amdgpu_amdkfd_debug_mem_fence(dqm->dev->kgd);
+	}
+
+	if (any_queues_suspended) {
+		struct copy_context_work_handler_workarea copy_context_worker;
+
+		INIT_WORK_ONSTACK(
+				&copy_context_worker.copy_context_work,
+				copy_context_work_handler);
+
+		copy_context_worker.p = p;
+
+		schedule_work(&copy_context_worker.copy_context_work);
+
+
+		flush_work(&copy_context_worker.copy_context_work);
+		destroy_work_on_stack(&copy_context_worker.copy_context_work);
+	}
+	return r;
+}
+
+int resume_queues(struct kfd_process *p,
+		uint32_t num_queues,
+		uint32_t flags,
+		uint32_t *queue_ids)
+{
+	int r = -ENODEV;
+	struct kfd_process_device *pdd;
+	struct queue *q;
+
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
+		bool queues_resumed_on_device = false;
+		struct device_queue_manager *dqm = pdd->dev->dqm;
+		struct qcm_process_device *qpd = &pdd->qpd;
+
+		dqm_lock(dqm);
+
+		/* We need to loop over all of the queues on this
+		 * device, and check if it is in the list passed in,
+		 * and if it is, we will restore it.
+		 */
+		list_for_each_entry(q, &qpd->queues_list, list) {
+			if (queue_id_in_array(q->properties.queue_id,
+						num_queues,
+						queue_ids)) {
+				if (!q->properties.is_suspended)
+					continue;
+				r = resume_single_queue(dqm,
+							&pdd->qpd,
+							q);
+				if (r) {
+					pr_err("Failed to resume process queues\n");
+					dqm_unlock(dqm);
+					return r;
+				}
+				queues_resumed_on_device = true;
+			}
+		}
+
+		if (queues_resumed_on_device) {
+			r = execute_queues_cpsch(dqm,
+					KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES,
+					0);
+			if (r) {
+				pr_err("Failed to resume process queues\n");
+				dqm_unlock(dqm);
+				return r;
+			}
+		}
+
+		dqm_unlock(dqm);
+	}
+	return r;
+}
+
+
 #if defined(CONFIG_DEBUG_FS)
 
 static void seq_reg_dump(struct seq_file *m,
@@ -1882,15 +2253,14 @@ int dqm_debugfs_hqds(struct seq_file *m, void *data)
 	int pipe, queue;
 	int r = 0;
 
-	r = dqm->dev->kfd2kgd->hqd_dump(dqm->dev->kgd,
-					KFD_CIK_HIQ_PIPE, KFD_CIK_HIQ_QUEUE,
-					&dump, &n_regs);
-	if (!r) {
-		seq_printf(m, "  HIQ on MEC %d Pipe %d Queue %d\n",
-			   KFD_CIK_HIQ_PIPE/get_pipes_per_mec(dqm)+1,
-			   KFD_CIK_HIQ_PIPE%get_pipes_per_mec(dqm),
-			   KFD_CIK_HIQ_QUEUE);
-		seq_reg_dump(m, dump, n_regs);
+        r = dqm->dev->kfd2kgd->hqd_dump(dqm->dev->kgd,
+                KFD_CIK_HIQ_PIPE, KFD_CIK_HIQ_QUEUE, &dump, &n_regs);
+        if (!r) {
+                seq_printf(m, "  HIQ on MEC %d Pipe %d Queue %d\n",
+                                KFD_CIK_HIQ_PIPE/get_pipes_per_mec(dqm)+1,
+                                KFD_CIK_HIQ_PIPE%get_pipes_per_mec(dqm),
+                                KFD_CIK_HIQ_QUEUE);
+                seq_reg_dump(m, dump, n_regs);
 
 		kfree(dump);
 	}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
old mode 100644
new mode 100755
index 90db2c9275f6..67ae43c1acc9
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -193,6 +193,7 @@ struct device_queue_manager {
 	struct kfd_mem_obj	*fence_mem;
 	bool			active_runlist;
 	int			sched_policy;
+	uint32_t		trap_debug_vmid;
 
 	/* hw exception  */
 	bool			is_hws_hang;
@@ -219,6 +220,19 @@ unsigned int get_queues_per_pipe(struct device_queue_manager *dqm);
 unsigned int get_pipes_per_mec(struct device_queue_manager *dqm);
 unsigned int get_num_sdma_queues(struct device_queue_manager *dqm);
 unsigned int get_num_xgmi_sdma_queues(struct device_queue_manager *dqm);
+bool check_if_queues_active(struct device_queue_manager *dqm,
+		struct qcm_process_device *qpd);
+int reserve_debug_trap_vmid(struct device_queue_manager *dqm);
+int release_debug_trap_vmid(struct device_queue_manager *dqm);
+int suspend_queues(struct kfd_process *p,
+			uint32_t num_queues,
+			uint32_t grace_period,
+			uint32_t flags,
+			uint32_t *queue_ids);
+int resume_queues(struct kfd_process *p,
+		uint32_t num_queues,
+		uint32_t flags,
+		uint32_t *queue_ids);
 
 static inline unsigned int get_sh_mem_bases_32(struct kfd_process_device *pdd)
 {
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
old mode 100644
new mode 100755
index ebe79bf00145..419b57726f8e
--- a/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_doorbell.c
@@ -126,10 +126,91 @@ void kfd_doorbell_fini(struct kfd_dev *kfd)
 		iounmap(kfd->doorbell_kernel_ptr);
 }
 
+static vm_fault_t kfd_doorbell_vm_fault(struct vm_fault *vmf)
+{
+	struct kfd_process *process = vmf->vma->vm_private_data;
+
+	pr_debug("Process %d doorbell vm page fault\n", process->pasid);
+
+	kfd_process_remap_doorbells_locked(process);
+
+	kfd_process_schedule_restore(process);
+
+	return VM_FAULT_NOPAGE;
+}
+
+static const struct vm_operations_struct kfd_doorbell_vm_ops = {
+	.fault = kfd_doorbell_vm_fault,
+};
+
+void kfd_doorbell_unmap_locked(struct kfd_process_device *pdd)
+{
+	struct kfd_process *process = pdd->process;
+	struct vm_area_struct *vma;
+	size_t size;
+
+	vma = pdd->qpd.doorbell_vma;
+	/* If process is evicted before queue is created
+	 * doorbell is not mapped to user space yet
+	 */
+	if (!vma || !pdd->qpd.queue_count) {
+		pdd->qpd.doorbell_mapped = -1;
+		return;
+	}
+
+	pr_debug("Process %d unmapping doorbell 0x%lx\n",
+			process->pasid, vma->vm_start);
+
+	size = kfd_doorbell_process_slice(pdd->dev);
+	zap_vma_ptes(vma, vma->vm_start, size);
+	pdd->qpd.doorbell_mapped = 0;
+}
+
+void kfd_doorbell_unmap(struct kfd_process_device *pdd)
+{
+	mutex_lock(&pdd->qpd.doorbell_lock);
+	kfd_doorbell_unmap_locked(pdd);
+	mutex_unlock(&pdd->qpd.doorbell_lock);
+}
+
+int kfd_doorbell_remap(struct kfd_process_device *pdd)
+{
+	struct kfd_process *process = pdd->process;
+	phys_addr_t address;
+	struct vm_area_struct *vma;
+	size_t size;
+	int ret = 0;
+
+	mutex_lock(&pdd->qpd.doorbell_lock);
+	if (pdd->qpd.doorbell_mapped != 0)
+		goto out_unlock;
+
+	/* Calculate physical address of doorbell */
+	address = kfd_get_process_doorbells(pdd->dev, process);
+	vma = pdd->qpd.doorbell_vma;
+	size = kfd_doorbell_process_slice(pdd->dev);
+
+	pr_debug("Process %d remap doorbell 0x%lx\n", process->pasid,
+		vma->vm_start);
+
+	ret = vm_iomap_memory(vma, address, size);
+	if (ret)
+		pr_err("Process %d failed to remap doorbell 0x%lx\n",
+			process->pasid, vma->vm_start);
+
+out_unlock:
+	pdd->qpd.doorbell_mapped = 1;
+	mutex_unlock(&pdd->qpd.doorbell_lock);
+
+	return ret;
+}
+
 int kfd_doorbell_mmap(struct kfd_dev *dev, struct kfd_process *process,
 		      struct vm_area_struct *vma)
 {
 	phys_addr_t address;
+	struct kfd_process_device *pdd;
+	int ret;
 
 	/*
 	 * For simplicitly we only allow mapping of the entire doorbell
@@ -146,20 +227,47 @@ int kfd_doorbell_mmap(struct kfd_dev *dev, struct kfd_process *process,
 
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 
-	pr_debug("Mapping doorbell page\n"
+	pr_debug("Process %d mapping doorbell page\n"
 		 "     target user address == 0x%08llX\n"
 		 "     physical address    == 0x%08llX\n"
 		 "     vm_flags            == 0x%04lX\n"
 		 "     size                == 0x%04lX\n",
-		 (unsigned long long) vma->vm_start, address, vma->vm_flags,
-		 kfd_doorbell_process_slice(dev));
+		 process->pasid, (unsigned long long) vma->vm_start,
+		 address, vma->vm_flags, kfd_doorbell_process_slice(dev));
+
+	pdd = kfd_get_process_device_data(dev, process);
+	if (WARN_ON_ONCE(!pdd))
+		return 0;
 
+	mutex_lock(&pdd->qpd.doorbell_lock);
 
-	return io_remap_pfn_range(vma,
+	ret = io_remap_pfn_range(vma,
 				vma->vm_start,
 				address >> PAGE_SHIFT,
 				kfd_doorbell_process_slice(dev),
 				vma->vm_page_prot);
+
+	if (!ret && keep_idle_process_evicted) {
+		vma->vm_ops = &kfd_doorbell_vm_ops;
+		vma->vm_private_data = process;
+		pdd->qpd.doorbell_vma = vma;
+
+		/* If process is evicted before the first queue is created,
+		 * process will be restored by the page fault when the
+		 * doorbell is accessed the first time
+		 */
+		if (pdd->qpd.doorbell_mapped == -1) {
+			pr_debug("Process %d evicted, unmapping doorbell\n",
+				process->pasid);
+			kfd_doorbell_unmap_locked(pdd);
+		} else {
+			pdd->qpd.doorbell_mapped = 1;
+		}
+	}
+
+	mutex_unlock(&pdd->qpd.doorbell_lock);
+
+	return ret;
 }
 
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_events.c b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
old mode 100644
new mode 100755
index 2297b6d73ae6..bcb6c20f9ca4
--- a/drivers/gpu/drm/amd/amdkfd/kfd_events.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_events.c
@@ -32,6 +32,7 @@
 #include "kfd_events.h"
 #include "kfd_iommu.h"
 #include <linux/device.h>
+#include <linux/ptrace.h>
 
 /*
  * Wrapper around wait_queue_entry_t
@@ -452,6 +453,36 @@ static void acknowledge_signal(struct kfd_process *p, struct kfd_event *ev)
 	page_slots(p->signal_page)[ev->event_id] = UNSIGNALED_EVENT_SLOT;
 }
 
+/* HACK: Temporary hack to enable signaling to debuggers running in a
+ * separate process. Remove this when the real signaling mechanism is
+ * implemented.
+ */
+static void signal_event_to_debugger(struct kfd_process *p)
+{
+	struct kfd_process_device *pdd;
+	struct task_struct *tracer;
+
+	/* Check that a debugger is attached to the process */
+	rcu_read_lock();
+	tracer = ptrace_parent(p->lead_thread);
+	if (tracer)
+		get_task_struct(tracer);
+	rcu_read_unlock();
+	if (!tracer)
+		return;
+
+	/* Check that GPU debugging is enabled for at least one device
+	 * for this process
+	 */
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list)
+		if (pdd->is_debugging_enabled) {
+			send_sig(SIGUSR2, tracer, 0);
+			break;
+		}
+
+	put_task_struct(tracer);
+}
+
 static void set_event_from_interrupt(struct kfd_process *p,
 					struct kfd_event *ev)
 {
@@ -464,6 +495,7 @@ static void set_event_from_interrupt(struct kfd_process *p,
 void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
 				uint32_t valid_id_bits)
 {
+	bool events_signaled = false;
 	struct kfd_event *ev = NULL;
 
 	/*
@@ -483,6 +515,7 @@ void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
 							 valid_id_bits);
 	if (ev) {
 		set_event_from_interrupt(p, ev);
+		events_signaled = true;
 	} else if (p->signal_page) {
 		/*
 		 * Partial ID lookup failed. Assume that the event ID
@@ -504,8 +537,10 @@ void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
 				if (id >= KFD_SIGNAL_EVENT_LIMIT)
 					break;
 
-				if (slots[id] != UNSIGNALED_EVENT_SLOT)
+				if (slots[id] != UNSIGNALED_EVENT_SLOT) {
 					set_event_from_interrupt(p, ev);
+					events_signaled = true;
+				}
 			}
 		} else {
 			/* With relatively many events, it's faster to
@@ -516,9 +551,12 @@ void kfd_signal_event_interrupt(unsigned int pasid, uint32_t partial_id,
 				if (slots[id] != UNSIGNALED_EVENT_SLOT) {
 					ev = lookup_event_by_id(p, id);
 					set_event_from_interrupt(p, ev);
+					events_signaled = true;
 				}
 		}
 	}
+	if (events_signaled)
+		signal_event_to_debugger(p);
 
 	mutex_unlock(&p->event_mutex);
 	kfd_unref_process(p);
@@ -935,6 +973,7 @@ void kfd_signal_iommu_event(struct kfd_dev *dev, unsigned int pasid,
 
 	/* Workaround on Raven to not kill the process when memory is freed
 	 * before IOMMU is able to finish processing all the excessive PPRs
+	 * triggered due to HW flaws.
 	 */
 	if (dev->device_info->asic_family != CHIP_RAVEN &&
 	    dev->device_info->asic_family != CHIP_RENOIR) {
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c b/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
old mode 100644
new mode 100755
index 481661499c9a..ae950633228c
--- a/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_flat_memory.c
@@ -369,8 +369,13 @@ int kfd_init_apertures(struct kfd_process *process)
 
 	/*Iterating over all devices*/
 	while (kfd_topology_enum_kfd_devices(id, &dev) == 0) {
-		if (!dev) {
-			id++; /* Skip non GPU devices */
+		if (!dev || kfd_devcgroup_check_permission(dev)) {
+			/* Skip non GPU devices and devices to which the
+			 * current process have no access to. Access can be
+			 * limited by placing the process in a specific
+			 * cgroup hierarchy
+			 */
+			id++;
 			continue;
 		}
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
new file mode 100755
index 000000000000..b733f6d09d43
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.c
@@ -0,0 +1,270 @@
+/*
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/slab.h>
+#include <linux/random.h>
+
+#include "kfd_ipc.h"
+#include "kfd_priv.h"
+#include "amdgpu_amdkfd.h"
+
+#define KFD_IPC_HASH_TABLE_SIZE_SHIFT 4
+#define KFD_IPC_HASH_TABLE_SIZE_MASK ((1 << KFD_IPC_HASH_TABLE_SIZE_SHIFT) - 1)
+
+static struct kfd_ipc_handles {
+	DECLARE_HASHTABLE(handles, KFD_IPC_HASH_TABLE_SIZE_SHIFT);
+	struct mutex lock;
+} kfd_ipc_handles;
+
+/* Since, handles are random numbers, it can be used directly as hashing key.
+ * The least 4 bits of the handle are used as key. However, during import all
+ * 128 bits of the handle are checked to prevent handle snooping.
+ */
+#define HANDLE_TO_KEY(sh) ((*(uint64_t *)sh) & KFD_IPC_HASH_TABLE_SIZE_MASK)
+
+static int ipc_store_insert(void *val, void *sh, struct kfd_ipc_obj **ipc_obj)
+{
+	struct kfd_ipc_obj *obj;
+
+	obj = kmalloc(sizeof(*obj), GFP_KERNEL);
+	if (!obj)
+		return -ENOMEM;
+
+	/* The initial ref belongs to the allocator process.
+	 * The IPC object store itself does not hold a ref since
+	 * there is no specific moment in time where that ref should
+	 * be dropped, except "when there are no more userspace processes
+	 * holding a ref to the object". Therefore the removal from IPC
+	 * storage happens at ipc_obj release time.
+	 */
+	kref_init(&obj->ref);
+	obj->data = val;
+	get_random_bytes(obj->share_handle, sizeof(obj->share_handle));
+
+	memcpy(sh, obj->share_handle, sizeof(obj->share_handle));
+
+	mutex_lock(&kfd_ipc_handles.lock);
+	hlist_add_head(&obj->node,
+		&kfd_ipc_handles.handles[HANDLE_TO_KEY(obj->share_handle)]);
+	mutex_unlock(&kfd_ipc_handles.lock);
+
+	if (ipc_obj)
+		*ipc_obj = obj;
+
+	return 0;
+}
+
+static void ipc_obj_release(struct kref *r)
+{
+	struct kfd_ipc_obj *obj;
+
+	obj = container_of(r, struct kfd_ipc_obj, ref);
+
+	mutex_lock(&kfd_ipc_handles.lock);
+	hash_del(&obj->node);
+	mutex_unlock(&kfd_ipc_handles.lock);
+
+	dma_buf_put(obj->data);
+	kfree(obj);
+}
+
+void ipc_obj_get(struct kfd_ipc_obj *obj)
+{
+	kref_get(&obj->ref);
+}
+
+void ipc_obj_put(struct kfd_ipc_obj **obj)
+{
+	kref_put(&(*obj)->ref, ipc_obj_release);
+	*obj = NULL;
+}
+
+int kfd_ipc_init(void)
+{
+	mutex_init(&kfd_ipc_handles.lock);
+	hash_init(kfd_ipc_handles.handles);
+	return 0;
+}
+
+static int kfd_import_dmabuf_create_kfd_bo(struct kfd_dev *dev,
+			  struct kfd_process *p,
+			  uint32_t gpu_id, struct dma_buf *dmabuf,
+			  uint64_t va_addr, uint64_t *handle,
+			  uint64_t *mmap_offset,
+			  struct kfd_ipc_obj *ipc_obj)
+{
+	int r;
+	void *mem;
+	uint64_t size;
+	int idr_handle;
+	struct kfd_process_device *pdd = NULL;
+
+	if (!handle)
+		return -EINVAL;
+
+	if (!dev)
+		return -EINVAL;
+
+	mutex_lock(&p->mutex);
+
+	pdd = kfd_bind_process_to_device(dev, p);
+	if (IS_ERR(pdd)) {
+		r = PTR_ERR(pdd);
+		goto err_unlock;
+	}
+
+	r = amdgpu_amdkfd_gpuvm_import_dmabuf(dev->kgd, dmabuf,
+					va_addr, pdd->vm,
+					(struct kgd_mem **)&mem, &size,
+					mmap_offset);
+	if (r)
+		goto err_unlock;
+
+	idr_handle = kfd_process_device_create_obj_handle(pdd, mem,
+							  va_addr, size, 0, 0,
+							  ipc_obj);
+	if (idr_handle < 0) {
+		r = -EFAULT;
+		goto err_free;
+	}
+
+	mutex_unlock(&p->mutex);
+
+	*handle = MAKE_HANDLE(gpu_id, idr_handle);
+
+	return 0;
+
+err_free:
+	amdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->kgd, (struct kgd_mem *)mem);
+err_unlock:
+	mutex_unlock(&p->mutex);
+	return r;
+}
+
+int kfd_ipc_import_dmabuf(struct kfd_dev *dev,
+					   struct kfd_process *p,
+					   uint32_t gpu_id, int dmabuf_fd,
+					   uint64_t va_addr, uint64_t *handle,
+					   uint64_t *mmap_offset)
+{
+	int r;
+	struct dma_buf *dmabuf = dma_buf_get(dmabuf_fd);
+
+	if (!dmabuf)
+		return -EINVAL;
+
+	r = kfd_import_dmabuf_create_kfd_bo(dev, p, gpu_id, dmabuf,
+					    va_addr, handle, mmap_offset,
+					    NULL);
+	dma_buf_put(dmabuf);
+	return r;
+}
+
+int kfd_ipc_import_handle(struct kfd_dev *dev, struct kfd_process *p,
+			  uint32_t gpu_id, uint32_t *share_handle,
+			  uint64_t va_addr, uint64_t *handle,
+			  uint64_t *mmap_offset)
+{
+	int r;
+	struct kfd_ipc_obj *entry, *found = NULL;
+
+	mutex_lock(&kfd_ipc_handles.lock);
+	/* Convert the user provided handle to hash key and search only in that
+	 * bucket
+	 */
+	hlist_for_each_entry(entry,
+		&kfd_ipc_handles.handles[HANDLE_TO_KEY(share_handle)], node) {
+		if (!memcmp(entry->share_handle, share_handle,
+			    sizeof(entry->share_handle))) {
+			found = entry;
+			break;
+		}
+	}
+	mutex_unlock(&kfd_ipc_handles.lock);
+
+	if (!found)
+		return -EINVAL;
+	ipc_obj_get(found);
+
+	pr_debug("Found ipc_dma_buf: %p\n", found->data);
+
+	r = kfd_import_dmabuf_create_kfd_bo(dev, p, gpu_id, found->data,
+					    va_addr, handle, mmap_offset,
+					    found);
+	if (r)
+		goto error_unref;
+
+	return r;
+
+error_unref:
+	ipc_obj_put(&found);
+	return r;
+}
+
+int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
+			     uint64_t handle, uint32_t *ipc_handle)
+{
+	struct kfd_process_device *pdd = NULL;
+	struct kfd_ipc_obj *obj;
+	struct kfd_bo *kfd_bo = NULL;
+	struct dma_buf *dmabuf;
+	int r;
+
+	if (!dev || !ipc_handle)
+		return -EINVAL;
+
+	mutex_lock(&p->mutex);
+	pdd = kfd_bind_process_to_device(dev, p);
+	if (IS_ERR(pdd)) {
+		mutex_unlock(&p->mutex);
+		pr_err("Failed to get pdd\n");
+		return PTR_ERR(pdd);
+	}
+
+	kfd_bo = kfd_process_device_find_bo(pdd, GET_IDR_HANDLE(handle));
+	mutex_unlock(&p->mutex);
+
+	if (!kfd_bo) {
+		pr_err("Failed to get bo");
+		return -EINVAL;
+	}
+	if (kfd_bo->kfd_ipc_obj) {
+		memcpy(ipc_handle, kfd_bo->kfd_ipc_obj->share_handle,
+		       sizeof(kfd_bo->kfd_ipc_obj->share_handle));
+		return 0;
+	}
+
+	r = amdgpu_amdkfd_gpuvm_export_dmabuf(dev->kgd, pdd->vm,
+					(struct kgd_mem *)kfd_bo->mem,
+					&dmabuf);
+	if (r)
+		return r;
+
+	r = ipc_store_insert(dmabuf, ipc_handle, &obj);
+	if (r)
+		return r;
+
+	kfd_bo->kfd_ipc_obj = obj;
+
+	return r;
+}
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
new file mode 100755
index 000000000000..9ee8627b88b0
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_ipc.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright 2014 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#ifndef KFD_IPC_H_
+#define KFD_IPC_H_
+
+#include <linux/types.h>
+#include "kfd_priv.h"
+
+struct kfd_ipc_obj {
+	struct hlist_node node;
+	struct kref ref;
+	void *data;
+	uint32_t share_handle[4];
+};
+
+int kfd_ipc_import_handle(struct kfd_dev *dev, struct kfd_process *p,
+			  uint32_t gpu_id, uint32_t *share_handle,
+			  uint64_t va_addr, uint64_t *handle,
+			  uint64_t *mmap_offset);
+int kfd_ipc_import_dmabuf(struct kfd_dev *kfd, struct kfd_process *p,
+			  uint32_t gpu_id, int dmabuf_fd,
+			  uint64_t va_addr, uint64_t *handle,
+			  uint64_t *mmap_offset);
+int kfd_ipc_export_as_handle(struct kfd_dev *dev, struct kfd_process *p,
+			     uint64_t handle, uint32_t *ipc_handle);
+
+void ipc_obj_get(struct kfd_ipc_obj *obj);
+void ipc_obj_put(struct kfd_ipc_obj **obj);
+
+#endif /* KFD_IPC_H_ */
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
old mode 100644
new mode 100755
index 990ab545c0f5..9ec62435326e
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.c
@@ -268,6 +268,39 @@ static int acquire_packet_buffer(struct kernel_queue *kq,
 	return -ENOMEM;
 }
 
+static int acquire_inline_ib(struct kernel_queue *kq,
+			     size_t size_in_dwords,
+			     unsigned int **buffer_ptr,
+			     uint64_t *gpu_addr)
+{
+	int ret;
+	unsigned int *buf;
+	union PM4_MES_TYPE_3_HEADER nop;
+
+	if (size_in_dwords >= (1 << 14))
+		return -EINVAL;
+
+	/* Allocate size_in_dwords on the ring, plus an extra dword
+	 * for a NOP packet header
+	 */
+	ret = acquire_packet_buffer(kq, size_in_dwords + 1,  &buf);
+	if (ret)
+		return ret;
+
+	/* Build a NOP packet that contains the IB as "payload". */
+	nop.u32all = 0;
+	nop.opcode = IT_NOP;
+	nop.count = size_in_dwords - 1;
+	nop.type = PM4_TYPE_3;
+
+	*buf = nop.u32all;
+	*buffer_ptr = buf + 1;
+	*gpu_addr = kq->pq_gpu_addr + ((unsigned long)*buffer_ptr -
+				       (unsigned long)kq->pq_kernel_addr);
+
+	return 0;
+}
+
 static void submit_packet(struct kernel_queue *kq)
 {
 #ifdef DEBUG
@@ -307,6 +340,7 @@ struct kernel_queue *kernel_queue_init(struct kfd_dev *dev,
 	kq->ops.initialize = initialize;
 	kq->ops.uninitialize = uninitialize;
 	kq->ops.acquire_packet_buffer = acquire_packet_buffer;
+	kq->ops.acquire_inline_ib = acquire_inline_ib;
 	kq->ops.submit_packet = submit_packet;
 	kq->ops.rollback_packet = rollback_packet;
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
old mode 100644
new mode 100755
index 365fc674fea4..a23927d809c7
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue.h
@@ -42,6 +42,12 @@
  * pending write pointer to that location so subsequent calls to
  * acquire_packet_buffer will get a correct write pointer
  *
+ * @acquire_inline_ib: Returns a pointer to the location in the kernel
+ * queue ring buffer where the calling function can write an inline IB. It is
+ * Guaranteed that there is enough space for that IB. It also updates the
+ * pending write pointer to that location so subsequent calls to
+ * acquire_packet_buffer will get a correct write pointer
+ *
  * @submit_packet: Update the write pointer and doorbell of a kernel queue.
  *
  * @sync_with_hw: Wait until the write pointer and the read pointer of a kernel
@@ -59,6 +65,10 @@ struct kernel_queue_ops {
 	int	(*acquire_packet_buffer)(struct kernel_queue *kq,
 					size_t packet_size_in_dwords,
 					unsigned int **buffer_ptr);
+	int	(*acquire_inline_ib)(struct kernel_queue *kq,
+				     size_t packet_size_in_dwords,
+				     unsigned int **buffer_ptr,
+				     uint64_t *gpu_addr);
 
 	void	(*submit_packet)(struct kernel_queue *kq);
 	void	(*rollback_packet)(struct kernel_queue *kq);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
old mode 100644
new mode 100755
index 9a4bafb2e175..b2a75cf61331
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
@@ -72,6 +72,7 @@ static int pm_map_process_v9(struct packet_manager *pm,
 {
 	struct pm4_mes_map_process *packet;
 	uint64_t vm_page_table_base_addr = qpd->page_table_base;
+	struct kfd_dev *kfd = pm->dqm->dev;
 
 	packet = (struct pm4_mes_map_process *)buffer;
 	memset(buffer, 0, sizeof(struct pm4_mes_map_process));
@@ -88,6 +89,11 @@ static int pm_map_process_v9(struct packet_manager *pm,
 	packet->bitfields14.sdma_enable = 1;
 	packet->bitfields14.num_queues = (qpd->is_debug) ? 0 : qpd->queue_count;
 
+	if (kfd->dqm->trap_debug_vmid) {
+		packet->bitfields2.debug_vmid = kfd->dqm->trap_debug_vmid;
+		packet->bitfields2.new_debug = 1;
+	}
+
 	packet->sh_mem_config = qpd->sh_mem_config;
 	packet->sh_mem_bases = qpd->sh_mem_bases;
 	packet->sq_shader_tba_lo = lower_32_bits(qpd->tba_addr >> 8);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_module.c b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
old mode 100644
new mode 100755
index 986ff52d5750..d5c5d8f98acd
--- a/drivers/gpu/drm/amd/amdkfd/kfd_module.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_module.c
@@ -52,10 +52,16 @@ static int kfd_init(void)
 	if (err < 0)
 		goto err_topology;
 
+	err = kfd_ipc_init();
+	if (err < 0)
+		goto err_ipc;
+
 	err = kfd_process_create_wq();
 	if (err < 0)
 		goto err_create_wq;
 
+	kfd_init_peer_direct();
+
 	/* Ignore the return value, so that we can continue
 	 * to init the KFD, even if procfs isn't craated
 	 */
@@ -66,6 +72,7 @@ static int kfd_init(void)
 	return 0;
 
 err_create_wq:
+err_ipc:
 	kfd_topology_shutdown();
 err_topology:
 	kfd_chardev_exit();
@@ -76,6 +83,7 @@ static int kfd_init(void)
 static void kfd_exit(void)
 {
 	kfd_debugfs_fini();
+	kfd_close_peer_direct();
 	kfd_process_destroy_wq();
 	kfd_procfs_shutdown();
 	kfd_topology_shutdown();
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
old mode 100644
new mode 100755
index fbdb16418847..e1455a921a3b
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h
@@ -98,6 +98,8 @@ struct mqd_manager {
 				  u32 *ctl_stack_used_size,
 				  u32 *save_area_used_size);
 
+	bool	(*check_queue_active)(struct queue *q);
+
 #if defined(CONFIG_DEBUG_FS)
 	int	(*debugfs_show_mqd)(struct seq_file *m, void *data);
 #endif
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
old mode 100644
new mode 100755
index 28876aceb14b..9431dc2ca54b
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c
@@ -30,6 +30,7 @@
 #include "cik_regs.h"
 #include "cik_structs.h"
 #include "oss/oss_2_4_sh_mask.h"
+#include "gca/gfx_7_2_sh_mask.h"
 
 static inline struct cik_mqd *get_mqd(void *mqd)
 {
@@ -41,6 +42,31 @@ static inline struct cik_sdma_rlc_registers *get_sdma_mqd(void *mqd)
 	return (struct cik_sdma_rlc_registers *)mqd;
 }
 
+static bool check_sdma_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	struct cik_sdma_rlc_registers *m = get_sdma_mqd(q->mqd);
+
+	rptr = m->sdma_rlc_rb_rptr;
+	wptr = m->sdma_rlc_rb_wptr;
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+
+	return (rptr != wptr);
+}
+
+static bool check_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	struct cik_mqd *m = get_mqd(q->mqd);
+
+	rptr = m->cp_hqd_pq_rptr;
+	wptr = m->cp_hqd_pq_wptr;
+
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+
+	return (rptr != wptr);
+}
+
 static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 			struct queue_properties *q)
 {
@@ -213,6 +239,9 @@ static void __update_mqd(struct mqd_manager *mm, void *mqd,
 
 	if (q->format == KFD_QUEUE_FORMAT_AQL)
 		m->cp_hqd_pq_control |= NO_UPDATE_RPTR;
+	if (priv_cp_queues)
+		m->cp_hqd_pq_control |=
+			1 << CP_HQD_PQ_CONTROL__PRIV_STATE__SHIFT;
 
 	update_cu_mask(mm, mqd, q);
 	set_priority(m, q);
@@ -382,6 +411,7 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct cik_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -395,6 +425,7 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct cik_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -408,6 +439,7 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct cik_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -421,6 +453,7 @@ struct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_sdma;
 		mqd->destroy_mqd = destroy_mqd_sdma;
 		mqd->is_occupied = is_occupied_sdma;
+		mqd->check_queue_active = check_sdma_queue_active;
 		mqd->mqd_size = sizeof(struct cik_sdma_rlc_registers);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd_sdma;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
old mode 100644
new mode 100755
index d3380c5bdbde..785ceda52c94
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c
@@ -42,6 +42,49 @@ static inline struct v9_sdma_mqd *get_sdma_mqd(void *mqd)
 	return (struct v9_sdma_mqd *)mqd;
 }
 
+static bool check_sdma_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	uint32_t rptr_hi, wptr_hi;
+	struct v9_sdma_mqd *m = get_sdma_mqd(q->mqd);
+
+	rptr = m->sdmax_rlcx_rb_rptr;
+	wptr = m->sdmax_rlcx_rb_wptr;
+	rptr_hi = m->sdmax_rlcx_rb_rptr_hi;
+	wptr_hi = m->sdmax_rlcx_rb_wptr_hi;
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+	pr_debug("rptr_hi=%d, wptr_hi=%d\n", rptr_hi, wptr_hi);
+
+	return (rptr != wptr || rptr_hi != wptr_hi);
+}
+
+static bool check_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	uint32_t cntl_stack_offset, cntl_stack_size;
+	struct v9_mqd *m = get_mqd(q->mqd);
+
+	rptr = m->cp_hqd_pq_rptr;
+	wptr = m->cp_hqd_pq_wptr_lo % q->properties.queue_size;
+	cntl_stack_offset = m->cp_hqd_cntl_stack_offset;
+	cntl_stack_size = m->cp_hqd_cntl_stack_size;
+
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+	pr_debug("m->cp_hqd_cntl_stack_offset=0x%08x\n", cntl_stack_offset);
+	pr_debug("m->cp_hqd_cntl_stack_size=0x%08x\n", cntl_stack_size);
+
+	if ((rptr == 0 && wptr == 0) ||
+		cntl_stack_offset == 0xffffffff ||
+		cntl_stack_size > 0x5000)
+		return false;
+
+	/* Process is idle if both conditions are meet:
+	 * queue's rptr equals to wptr
+	 * control stack is empty, cntl_stack_offset = cntl_stack_size
+	 */
+	return (rptr != wptr || cntl_stack_offset != cntl_stack_size);
+}
+
 static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 			struct queue_properties *q)
 {
@@ -246,6 +289,9 @@ static void update_mqd(struct mqd_manager *mm, void *mqd,
 		m->cp_hqd_pq_doorbell_control |= 1 <<
 			CP_HQD_PQ_DOORBELL_CONTROL__DOORBELL_BIF_DROP__SHIFT;
 	}
+	if (priv_cp_queues)
+		m->cp_hqd_pq_control |=
+			1 << CP_HQD_PQ_CONTROL__PRIV_STATE__SHIFT;
 	if (mm->dev->cwsr_enabled && q->ctx_save_restore_area_address)
 		m->cp_hqd_ctx_save_control = 0;
 
@@ -288,12 +334,38 @@ static bool is_occupied(struct mqd_manager *mm, void *mqd,
 		pipe_id, queue_id);
 }
 
+struct user_context_save_area_header {
+	/* Byte offset from start of user context
+	 * save area to the last saved top (lowest
+	 * address) of control stack data. Must be
+	 * 4 byte aligned.
+	 */
+	uint32_t control_stack_offset;
+
+	/* Byte size of the last saved control stack
+	 * data. Must be 4 byte aligned.
+	 */
+	uint32_t control_stack_size;
+
+	/* Byte offset from start of user context save
+	 * area to the last saved base (lowest address)
+	 * of wave state data. Must be 4 byte aligned.
+	 */
+	uint32_t wave_state_offset;
+
+	/* Byte size of the last saved wave state data.
+	 * Must be 4 byte aligned.
+	 */
+	uint32_t wave_state_size;
+};
+
 static int get_wave_state(struct mqd_manager *mm, void *mqd,
 			  void __user *ctl_stack,
 			  u32 *ctl_stack_used_size,
 			  u32 *save_area_used_size)
 {
 	struct v9_mqd *m;
+	struct user_context_save_area_header header;
 
 	/* Control stack is located one page after MQD. */
 	void *mqd_ctl_stack = (void *)((uintptr_t)mqd + PAGE_SIZE);
@@ -302,9 +374,22 @@ static int get_wave_state(struct mqd_manager *mm, void *mqd,
 
 	*ctl_stack_used_size = m->cp_hqd_cntl_stack_size -
 		m->cp_hqd_cntl_stack_offset;
-	*save_area_used_size = m->cp_hqd_wg_state_offset;
+	*save_area_used_size = m->cp_hqd_wg_state_offset -
+		m->cp_hqd_cntl_stack_size;
+
+	header.control_stack_size = *ctl_stack_used_size;
+	header.wave_state_size = *save_area_used_size;
+
+	header.wave_state_offset = m->cp_hqd_wg_state_offset;
+	header.control_stack_offset = m->cp_hqd_cntl_stack_offset;
+
+	if (copy_to_user(ctl_stack, &header, sizeof(header)))
+		return -EFAULT;
+
+	if (copy_to_user(ctl_stack + m->cp_hqd_cntl_stack_offset,
+				mqd_ctl_stack + m->cp_hqd_cntl_stack_offset,
+				*ctl_stack_used_size))
 
-	if (copy_to_user(ctl_stack, mqd_ctl_stack, m->cp_hqd_cntl_stack_size))
 		return -EFAULT;
 
 	return 0;
@@ -452,6 +537,7 @@ struct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
 		mqd->get_wave_state = get_wave_state;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct v9_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -465,6 +551,7 @@ struct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct v9_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -478,6 +565,7 @@ struct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct v9_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -491,6 +579,7 @@ struct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_sdma;
 		mqd->destroy_mqd = destroy_mqd_sdma;
 		mqd->is_occupied = is_occupied_sdma;
+		mqd->check_queue_active = check_sdma_queue_active;
 		mqd->mqd_size = sizeof(struct v9_sdma_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd_sdma;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
old mode 100644
new mode 100755
index 7d144f56f421..39c9b470e227
--- a/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c
@@ -44,6 +44,45 @@ static inline struct vi_sdma_mqd *get_sdma_mqd(void *mqd)
 	return (struct vi_sdma_mqd *)mqd;
 }
 
+static bool check_sdma_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	struct vi_sdma_mqd *m = get_sdma_mqd(q->mqd);
+
+	rptr = m->sdmax_rlcx_rb_rptr;
+	wptr = m->sdmax_rlcx_rb_wptr;
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+
+	return (rptr != wptr);
+}
+
+static bool check_queue_active(struct queue *q)
+{
+	uint32_t rptr, wptr;
+	uint32_t cntl_stack_offset, cntl_stack_size;
+	struct vi_mqd *m = get_mqd(q->mqd);
+
+	rptr = m->cp_hqd_pq_rptr;
+	wptr = m->cp_hqd_pq_wptr;
+	cntl_stack_offset = m->cp_hqd_cntl_stack_offset;
+	cntl_stack_size = m->cp_hqd_cntl_stack_size;
+
+	pr_debug("rptr=%d, wptr=%d\n", rptr, wptr);
+	pr_debug("m->cp_hqd_cntl_stack_offset=0x%08x\n", cntl_stack_offset);
+	pr_debug("m->cp_hqd_cntl_stack_size=0x%08x\n", cntl_stack_size);
+
+	if ((rptr == 0 && wptr == 0) ||
+		cntl_stack_offset == 0xffffffff ||
+		cntl_stack_size > 0x5000)
+		return false;
+
+	/* Process is idle if both conditions are meet:
+	 * queue's rptr equals to wptr
+	 * control stack is empty, cntl_stack_offset = cntl_stack_size
+	 */
+	return (rptr != wptr || cntl_stack_offset != cntl_stack_size);
+}
+
 static void update_cu_mask(struct mqd_manager *mm, void *mqd,
 			struct queue_properties *q)
 {
@@ -224,7 +263,9 @@ static void __update_mqd(struct mqd_manager *mm, void *mqd,
 		m->cp_hqd_pq_control |= CP_HQD_PQ_CONTROL__NO_UPDATE_RPTR_MASK |
 				2 << CP_HQD_PQ_CONTROL__SLOT_BASED_WPTR__SHIFT;
 	}
-
+	if (priv_cp_queues)
+		m->cp_hqd_pq_control |=
+			1 << CP_HQD_PQ_CONTROL__PRIV_STATE__SHIFT;
 	if (mm->dev->cwsr_enabled && q->ctx_save_restore_area_address)
 		m->cp_hqd_ctx_save_control =
 			atc_bit << CP_HQD_CTX_SAVE_CONTROL__ATC__SHIFT |
@@ -434,6 +475,7 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
 		mqd->get_wave_state = get_wave_state;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct vi_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -447,6 +489,7 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct vi_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -460,6 +503,7 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_hiq;
 		mqd->destroy_mqd = destroy_mqd;
 		mqd->is_occupied = is_occupied;
+		mqd->check_queue_active = check_queue_active;
 		mqd->mqd_size = sizeof(struct vi_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd;
@@ -473,6 +517,7 @@ struct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,
 		mqd->update_mqd = update_mqd_sdma;
 		mqd->destroy_mqd = destroy_mqd_sdma;
 		mqd->is_occupied = is_occupied_sdma;
+		mqd->check_queue_active = check_sdma_queue_active;
 		mqd->mqd_size = sizeof(struct vi_sdma_mqd);
 #if defined(CONFIG_DEBUG_FS)
 		mqd->debugfs_show_mqd = debugfs_show_mqd_sdma;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c b/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
new file mode 100755
index 000000000000..5d1ae20da9be
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_peerdirect.c
@@ -0,0 +1,523 @@
+/*
+ * Copyright 2016 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+
+/* NOTE:
+ *
+ * This file contains logic to dynamically detect and enable PeerDirect
+ * suppor. PeerDirect support is delivered e.g. as part of OFED
+ * from Mellanox. Because we are not able to rely on the fact that the
+ * corresponding OFED will be installed we should:
+ *  - copy PeerDirect definitions locally to avoid dependency on
+ *    corresponding header file
+ *  - try dynamically detect address of PeerDirect function
+ *    pointers.
+ *
+ * If dynamic detection failed then PeerDirect support should be
+ * enabled using the standard PeerDirect bridge driver from:
+ * https://github.com/RadeonOpenCompute/ROCnRDMA
+ *
+ *
+ * Logic to support PeerDirect relies only on official public API to be
+ * non-intrusive as much as possible.
+ *
+ **/
+
+#include <linux/device.h>
+#include <linux/export.h>
+#include <linux/pid.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/scatterlist.h>
+#include <linux/module.h>
+#include <drm/amd_rdma.h>
+
+#include "kfd_priv.h"
+
+
+
+/* ----------------------- PeerDirect interface ------------------------------*/
+
+/*
+ * Copyright (c) 2013,  Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#define IB_PEER_MEMORY_NAME_MAX 64
+#define IB_PEER_MEMORY_VER_MAX 16
+
+struct peer_memory_client {
+	char	name[IB_PEER_MEMORY_NAME_MAX];
+	char	version[IB_PEER_MEMORY_VER_MAX];
+	/* acquire return code: 1-mine, 0-not mine */
+	int (*acquire)(unsigned long addr, size_t size,
+			void *peer_mem_private_data,
+					char *peer_mem_name,
+					void **client_context);
+	int (*get_pages)(unsigned long addr,
+			  size_t size, int write, int force,
+			  struct sg_table *sg_head,
+			  void *client_context, void *core_context);
+	int (*dma_map)(struct sg_table *sg_head, void *client_context,
+			struct device *dma_device, int dmasync, int *nmap);
+	int (*dma_unmap)(struct sg_table *sg_head, void *client_context,
+			   struct device  *dma_device);
+	void (*put_pages)(struct sg_table *sg_head, void *client_context);
+	unsigned long (*get_page_size)(void *client_context);
+	void (*release)(void *client_context);
+	void* (*get_context_private_data)(u64 peer_id);
+	void (*put_context_private_data)(void *context);
+};
+
+typedef int (*invalidate_peer_memory)(void *reg_handle,
+					  void *core_context);
+
+void *ib_register_peer_memory_client(struct peer_memory_client *peer_client,
+				  invalidate_peer_memory *invalidate_callback);
+void ib_unregister_peer_memory_client(void *reg_handle);
+
+
+/*------------------- PeerDirect bridge driver ------------------------------*/
+
+#define AMD_PEER_BRIDGE_DRIVER_VERSION	"1.0"
+#define AMD_PEER_BRIDGE_DRIVER_NAME	"amdkfd"
+
+
+static void* (*pfn_ib_register_peer_memory_client)(struct peer_memory_client
+							*peer_client,
+					invalidate_peer_memory
+							*invalidate_callback);
+
+static void (*pfn_ib_unregister_peer_memory_client)(void *reg_handle);
+
+static const struct amd_rdma_interface *rdma_interface;
+
+static void *ib_reg_handle;
+
+struct amd_mem_context {
+	uint64_t	va;
+	uint64_t	size;
+	struct pid	*pid;
+
+	struct amd_p2p_info  *p2p_info;
+
+	/* Flag that free callback was called */
+	int free_callback_called;
+
+	/* Context received from PeerDirect call */
+	void *core_context;
+};
+
+
+static void free_callback(void *client_priv)
+{
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_priv;
+
+	pr_debug("data 0x%p\n", mem_context);
+
+	if (!mem_context) {
+		pr_warn("Invalid client context\n");
+		return;
+	}
+
+	pr_debug("mem_context->core_context 0x%p\n", mem_context->core_context);
+
+	/* amdkfd will free resources when we return from this callback.
+	 * Set flag to inform that there is nothing to do on "put_pages", etc.
+	 */
+	WRITE_ONCE(mem_context->free_callback_called, 1);
+}
+
+
+static int amd_acquire(unsigned long addr, size_t size,
+			void *peer_mem_private_data,
+			char *peer_mem_name, void **client_context)
+{
+	int ret;
+	struct amd_mem_context *mem_context;
+	struct pid *pid;
+	unsigned long page_size;
+
+	/* Get pointer to structure describing current process */
+	pid = get_task_pid(current, PIDTYPE_PID);
+
+	pr_debug("addr:0x%lx,size:0x%x, pid 0x%p\n",
+					addr, (unsigned int)size, pid);
+
+	/* Check if address is handled by AMD GPU driver */
+	ret = rdma_interface->is_gpu_address(addr, pid);
+
+	if (!ret) {
+		pr_debug("Not GPU Address\n");
+		/* This is not GPU address */
+		return 0;
+	}
+
+	pr_debug("GPU address\n");
+
+	/* Initialize context used for operation with given address */
+	mem_context = kzalloc(sizeof(*mem_context), GFP_KERNEL);
+
+	if (!mem_context)
+		return 0;	/* Error case handled as not GPU address  */
+
+	mem_context->free_callback_called = 0;
+	mem_context->va   = addr;
+
+	/* Workaround: Currently, Mellanox drivers seem to be supporting only at
+	 * page granularity. This is causing failures when an application tries
+	 * to register size < page_size. Fix it temporarily by aligning the size
+	 * to page size
+	 */
+	rdma_interface->get_page_size(addr, size, pid, &page_size);
+	mem_context->size = ALIGN(size, page_size);
+
+	/* Save PID. It is guaranteed that the function will be
+	 * called in the correct process context as opposite to others.
+	 */
+	mem_context->pid  = pid;
+
+	pr_debug("Client context %p\n", mem_context);
+
+	/* Return pointer to allocated context */
+	*client_context = mem_context;
+
+	/* Return 1 to inform that this address which will be handled
+	 * by AMD GPU driver
+	 */
+	return 1;
+}
+
+static int amd_get_pages(unsigned long addr, size_t size, int write, int force,
+			  struct sg_table *sg_head,
+			  void *client_context, void *core_context)
+{
+	int ret;
+	unsigned long page_size;
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("addr:0x%lx,size:0x%x, core_context:%p\n",
+		addr, (unsigned int)size, core_context);
+
+	if (!mem_context) {
+		pr_warn("Invalid client context");
+		return -EINVAL;
+	}
+
+	pr_debug("pid :0x%p\n", mem_context->pid);
+
+
+	if (addr != mem_context->va) {
+		pr_warn("Context address (0x%llx) is not the same\n",
+			mem_context->va);
+		return -EINVAL;
+	}
+
+	/* Workaround: see amd_acquire */
+	rdma_interface->get_page_size(addr, size, mem_context->pid,
+				      &page_size);
+	if (ALIGN(size, page_size) != mem_context->size) {
+		pr_warn("Context size (0x%llx) is not the same\n",
+			mem_context->size);
+		return -EINVAL;
+	}
+
+	ret = rdma_interface->get_pages(addr,
+					mem_context->size,
+					mem_context->pid,
+					&mem_context->p2p_info,
+					free_callback,
+					mem_context);
+
+	if (ret || !mem_context->p2p_info) {
+		pr_err("Could not rdma::get_pages failure: %d\n", ret);
+		return ret;
+	}
+
+	mem_context->core_context = core_context;
+
+	/* Note: At this stage it is OK not to fill sg_table */
+	return 0;
+}
+
+
+static int amd_dma_map(struct sg_table *sg_head, void *client_context,
+			struct device *dma_device, int dmasync, int *nmap)
+{
+	/*
+	 * NOTE/TODO:
+	 * We could have potentially three cases for real memory
+	 *	location:
+	 *		- all memory in the local
+	 *		- all memory in the system (RAM)
+	 *		- memory is spread (s/g) between local and system.
+	 *
+	 *	In the case of all memory in the system we could use
+	 *	iommu driver to build DMA addresses but not in the case
+	 *	of local memory because currently iommu driver doesn't
+	 *	deal with local/device memory addresses (it requires "struct
+	 *	page").
+	 *
+	 *	Accordingly returning assumes that iommu funcutionality
+	 *	should be disabled so we can assume that sg_table already
+	 *	contains DMA addresses.
+	 *
+	 */
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("Context 0x%p, sg_head 0x%p\n",
+			client_context, sg_head);
+
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
+			mem_context->pid,
+			mem_context->va,
+			mem_context->size);
+
+	if (!mem_context->p2p_info) {
+		pr_err("No sg table were allocated\n");
+		return -EINVAL;
+	}
+
+	/* Copy information about previosly allocated sg_table */
+	*sg_head = *mem_context->p2p_info->pages;
+
+	/* Return number of pages */
+	*nmap = mem_context->p2p_info->pages->nents;
+
+	return 0;
+}
+
+static int amd_dma_unmap(struct sg_table *sg_head, void *client_context,
+			   struct device  *dma_device)
+{
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("Context 0x%p, sg_table 0x%p\n",
+			client_context, sg_head);
+
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
+			mem_context->pid,
+			mem_context->va,
+			mem_context->size);
+
+	/* Assume success */
+	return 0;
+}
+static void amd_put_pages(struct sg_table *sg_head, void *client_context)
+{
+	int ret = 0;
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("sg_head %p client_context: 0x%p\n",
+			sg_head, client_context);
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
+			mem_context->pid,
+			mem_context->va,
+			mem_context->size);
+
+	pr_debug("mem_context->p2p_info %p\n",
+				mem_context->p2p_info);
+
+	if (mem_context->free_callback_called) {
+		READ_ONCE(mem_context->free_callback_called);
+		pr_debug("Free callback was called\n");
+		return;
+	}
+
+	if (mem_context->p2p_info) {
+		ret = rdma_interface->put_pages(&mem_context->p2p_info);
+		mem_context->p2p_info = NULL;
+
+		if (ret)
+			pr_err("Failure: %d (callback status %d)\n",
+					ret, mem_context->free_callback_called);
+	} else
+		pr_err("Pointer to p2p info is null\n");
+}
+static unsigned long amd_get_page_size(void *client_context)
+{
+	unsigned long page_size;
+	int result;
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("context: %p\n", client_context);
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
+			mem_context->pid,
+			mem_context->va,
+			mem_context->size);
+
+
+	result = rdma_interface->get_page_size(
+				mem_context->va,
+				mem_context->size,
+				mem_context->pid,
+				&page_size);
+
+	if (result) {
+		pr_err("Could not get page size. %d\n", result);
+		/* If we failed to get page size then do not know what to do.
+		 * Let's return some default value
+		 */
+		return PAGE_SIZE;
+	}
+
+	return page_size;
+}
+
+static void amd_release(void *client_context)
+{
+	struct amd_mem_context *mem_context =
+		(struct amd_mem_context *)client_context;
+
+	pr_debug("context: 0x%p\n", client_context);
+	pr_debug("pid 0x%p, address 0x%llx, size:0x%llx\n",
+			mem_context->pid,
+			mem_context->va,
+			mem_context->size);
+
+	kfree(mem_context);
+}
+
+
+static struct peer_memory_client amd_mem_client = {
+	.acquire = amd_acquire,
+	.get_pages = amd_get_pages,
+	.dma_map = amd_dma_map,
+	.dma_unmap = amd_dma_unmap,
+	.put_pages = amd_put_pages,
+	.get_page_size = amd_get_page_size,
+	.release = amd_release,
+	.get_context_private_data = NULL,
+	.put_context_private_data = NULL,
+};
+
+/** Initialize PeerDirect interface with RDMA Network stack.
+ *
+ *  Because network stack could potentially be loaded later we check
+ *  presence of PeerDirect when HSA process is created. If PeerDirect was
+ *  already initialized we do nothing otherwise try to detect and register.
+ */
+void kfd_init_peer_direct(void)
+{
+	int result;
+
+	if (pfn_ib_unregister_peer_memory_client) {
+		pr_debug("PeerDirect support was already initialized\n");
+		return;
+	}
+
+	pr_debug("Try to initialize PeerDirect support\n");
+
+	pfn_ib_register_peer_memory_client =
+		(void *(*)(struct peer_memory_client *,
+			  invalidate_peer_memory *))
+		symbol_request(ib_register_peer_memory_client);
+
+	pfn_ib_unregister_peer_memory_client = (void (*)(void *))
+		symbol_request(ib_unregister_peer_memory_client);
+
+	if (!pfn_ib_register_peer_memory_client ||
+		!pfn_ib_unregister_peer_memory_client) {
+		pr_debug("PeerDirect interface was not detected\n");
+		/* Do cleanup */
+		kfd_close_peer_direct();
+		return;
+	}
+
+	result = amdkfd_query_rdma_interface(&rdma_interface);
+
+	if (result < 0) {
+		pr_err("Cannot get RDMA Interface (result = %d)\n", result);
+		return;
+	}
+
+	strcpy(amd_mem_client.name,    AMD_PEER_BRIDGE_DRIVER_NAME);
+	strcpy(amd_mem_client.version, AMD_PEER_BRIDGE_DRIVER_VERSION);
+
+	ib_reg_handle = pfn_ib_register_peer_memory_client(&amd_mem_client,
+							   NULL);
+
+	if (!ib_reg_handle) {
+		pr_err("Cannot register peer memory client\n");
+		/* Do cleanup */
+		kfd_close_peer_direct();
+		return;
+	}
+
+	pr_info("PeerDirect support was initialized successfully\n");
+}
+
+/**
+ * Close connection with PeerDirect interface with RDMA Network stack.
+ *
+ */
+void kfd_close_peer_direct(void)
+{
+	if (pfn_ib_unregister_peer_memory_client) {
+		if (ib_reg_handle)
+			pfn_ib_unregister_peer_memory_client(ib_reg_handle);
+
+		symbol_put(ib_unregister_peer_memory_client);
+	}
+
+	if (pfn_ib_register_peer_memory_client)
+		symbol_put(ib_register_peer_memory_client);
+
+
+	/* Reset pointers to be safe */
+	pfn_ib_unregister_peer_memory_client = NULL;
+	pfn_ib_register_peer_memory_client   = NULL;
+	ib_reg_handle = NULL;
+}
+
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h
old mode 100644
new mode 100755
index 4d7add843746..e2b86bae4f9b
--- a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_ai.h
@@ -144,10 +144,13 @@ struct pm4_mes_map_process {
 
 	union {
 		struct {
-			uint32_t pasid:16;
-			uint32_t reserved1:8;
-			uint32_t diq_enable:1;
-			uint32_t process_quantum:7;
+			uint32_t pasid:16;	    /* 0 - 15  */
+			uint32_t reserved1:2;	    /* 16 - 17 */
+			uint32_t debug_vmid:4;	    /* 18 - 21 */
+			uint32_t new_debug:1;	    /* 22      */
+			uint32_t tmz:1;		    /* 23      */
+			uint32_t diq_enable:1;      /* 24      */
+			uint32_t process_quantum:7; /* 25 - 31 */
 		} bitfields2;
 		uint32_t ordinal2;
 	};
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
old mode 100644
new mode 100755
index a0ff34878163..0b314a8c8f4b
--- a/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_pm4_headers_diq.h
@@ -77,103 +77,6 @@ struct pm4__indirect_buffer_pasid {
 
 #endif
 
-/*--------------------_RELEASE_MEM-------------------- */
-
-#ifndef _PM4__RELEASE_MEM_DEFINED
-#define _PM4__RELEASE_MEM_DEFINED
-enum _RELEASE_MEM_event_index_enum {
-	event_index___release_mem__end_of_pipe = 5,
-	event_index___release_mem__shader_done = 6
-};
-
-enum _RELEASE_MEM_cache_policy_enum {
-	cache_policy___release_mem__lru = 0,
-	cache_policy___release_mem__stream = 1,
-	cache_policy___release_mem__bypass = 2
-};
-
-enum _RELEASE_MEM_dst_sel_enum {
-	dst_sel___release_mem__memory_controller = 0,
-	dst_sel___release_mem__tc_l2 = 1,
-	dst_sel___release_mem__queue_write_pointer_register = 2,
-	dst_sel___release_mem__queue_write_pointer_poll_mask_bit = 3
-};
-
-enum _RELEASE_MEM_int_sel_enum {
-	int_sel___release_mem__none = 0,
-	int_sel___release_mem__send_interrupt_only = 1,
-	int_sel___release_mem__send_interrupt_after_write_confirm = 2,
-	int_sel___release_mem__send_data_after_write_confirm = 3
-};
-
-enum _RELEASE_MEM_data_sel_enum {
-	data_sel___release_mem__none = 0,
-	data_sel___release_mem__send_32_bit_low = 1,
-	data_sel___release_mem__send_64_bit_data = 2,
-	data_sel___release_mem__send_gpu_clock_counter = 3,
-	data_sel___release_mem__send_cp_perfcounter_hi_lo = 4,
-	data_sel___release_mem__store_gds_data_to_memory = 5
-};
-
-struct pm4__release_mem {
-	union {
-		union PM4_MES_TYPE_3_HEADER header;	/*header */
-		unsigned int ordinal1;
-	};
-
-	union {
-		struct {
-			unsigned int event_type:6;
-			unsigned int reserved1:2;
-			enum _RELEASE_MEM_event_index_enum event_index:4;
-			unsigned int tcl1_vol_action_ena:1;
-			unsigned int tc_vol_action_ena:1;
-			unsigned int reserved2:1;
-			unsigned int tc_wb_action_ena:1;
-			unsigned int tcl1_action_ena:1;
-			unsigned int tc_action_ena:1;
-			unsigned int reserved3:6;
-			unsigned int atc:1;
-			enum _RELEASE_MEM_cache_policy_enum cache_policy:2;
-			unsigned int reserved4:5;
-		} bitfields2;
-		unsigned int ordinal2;
-	};
-
-	union {
-		struct {
-			unsigned int reserved5:16;
-			enum _RELEASE_MEM_dst_sel_enum dst_sel:2;
-			unsigned int reserved6:6;
-			enum _RELEASE_MEM_int_sel_enum int_sel:3;
-			unsigned int reserved7:2;
-			enum _RELEASE_MEM_data_sel_enum data_sel:3;
-		} bitfields3;
-		unsigned int ordinal3;
-	};
-
-	union {
-		struct {
-			unsigned int reserved8:2;
-			unsigned int address_lo_32b:30;
-		} bitfields4;
-		struct {
-			unsigned int reserved9:3;
-			unsigned int address_lo_64b:29;
-		} bitfields5;
-		unsigned int ordinal4;
-	};
-
-	unsigned int address_hi;
-
-	unsigned int data_lo;
-
-	unsigned int data_hi;
-
-};
-#endif
-
-
 /*--------------------_SET_CONFIG_REG-------------------- */
 
 #ifndef _PM4__SET_CONFIG_REG_DEFINED
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
old mode 100644
new mode 100755
index c89326125d71..0afd8634202b
--- a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
@@ -35,7 +35,10 @@
 #include <linux/kfifo.h>
 #include <linux/seq_file.h>
 #include <linux/kref.h>
-#include <linux/sysfs.h>
+#include <linux/pid.h>
+#include <linux/interval_tree.h>
+#include <linux/device_cgroup.h>
+#include <drm/drmP.h>
 #include <kgd_kfd_interface.h>
 
 #include "amd_shared.h"
@@ -47,6 +50,8 @@
 /* GPU ID hash width in bits */
 #define KFD_GPU_ID_HASH_WIDTH 16
 
+struct drm_device;
+
 /* Use upper bits of mmap offset to store KFD driver specific information.
  * BITS[63:62] - Encode MMAP type
  * BITS[61:46] - Encode gpu_id. To identify to which GPU the offset belongs to
@@ -159,6 +164,11 @@ extern int ignore_crat;
  */
 extern int amdgpu_noretry;
 
+/*
+ * Enable privileged mode for all CP queues including user queues
+ */
+extern int priv_cp_queues;
+
 /*
  * Halt if HWS hang is detected
  */
@@ -174,6 +184,11 @@ extern bool hws_gws_support;
  */
 extern int queue_preemption_timeout_ms;
 
+/*
+ * Restore evicted process only if queues are active
+ */
+extern bool keep_idle_process_evicted;
+
 enum cache_policy {
 	cache_policy_coherent,
 	cache_policy_noncoherent
@@ -230,6 +245,7 @@ struct kfd_dev {
 
 	const struct kfd_device_info *device_info;
 	struct pci_dev *pdev;
+	struct drm_device *ddev;
 
 	unsigned int id;		/* topology stub index */
 
@@ -271,6 +287,7 @@ struct kfd_dev {
 	struct device_queue_manager *dqm;
 
 	bool init_complete;
+
 	/*
 	 * Interrupts of interest to KFD are copied
 	 * from the HW ring into a SW ring.
@@ -307,6 +324,51 @@ struct kfd_dev {
 	void *gws;
 };
 
+struct kfd_ipc_obj;
+
+struct kfd_bo {
+	void *mem;
+	struct interval_tree_node it;
+	struct kfd_dev *dev;
+	struct list_head cb_data_head;
+	struct kfd_ipc_obj *kfd_ipc_obj;
+	/* page-aligned VA address */
+	uint64_t cpuva;
+	unsigned int mem_type;
+};
+
+struct cma_system_bo {
+	struct kgd_mem *mem;
+	struct sg_table *sg;
+	struct kfd_dev *dev;
+	struct list_head list;
+};
+
+/* Similar to iov_iter */
+struct cma_iter {
+	/* points to current entry of range array */
+	struct kfd_memory_range *array;
+	/* total number of entries in the initial array */
+	unsigned long nr_segs;
+	/* total amount of data pointed by kfd array*/
+	unsigned long total;
+	/* offset into the entry pointed by cma_iter.array */
+	unsigned long offset;
+	struct kfd_process *p;
+	struct mm_struct *mm;
+	struct task_struct *task;
+	/* current kfd_bo associated with cma_iter.array.va_addr */
+	struct kfd_bo *cur_bo;
+	/* offset w.r.t cur_bo */
+	unsigned long bo_offset;
+	/* If cur_bo is a userptr BO, then a shadow system BO is created
+	 * using its underlying pages. cma_bo holds this BO. cma_list is a
+	 * list cma_bos created in one session
+	 */
+	struct cma_system_bo *cma_bo;
+	struct list_head cma_list;
+};
+
 enum kfd_mempool {
 	KFD_MEMPOOL_SYSTEM_CACHEABLE = 1,
 	KFD_MEMPOOL_SYSTEM_WRITECOMBINE = 2,
@@ -428,6 +490,7 @@ struct queue_properties {
 	uint32_t doorbell_off;
 	bool is_interop;
 	bool is_evicted;
+	bool is_suspended;
 	bool is_active;
 	/* Not relevant for user mode queues in cp scheduling */
 	unsigned int vmid;
@@ -451,7 +514,8 @@ struct queue_properties {
 #define QUEUE_IS_ACTIVE(q) ((q).queue_size > 0 &&	\
 			    (q).queue_address != 0 &&	\
 			    (q).queue_percent > 0 &&	\
-			    !(q).is_evicted)
+			    !(q).is_evicted && \
+			    !(q).is_suspended)
 
 /**
  * struct queue
@@ -523,6 +587,13 @@ enum KFD_PIPE_PRIORITY {
 	KFD_PIPE_PRIORITY_CS_HIGH
 };
 
+enum KFD_SPI_PRIORITY {
+	KFD_SPI_PRIORITY_EXTRA_LOW = 0,
+	KFD_SPI_PRIORITY_LOW,
+	KFD_SPI_PRIORITY_MEDIUM,
+	KFD_SPI_PRIORITY_HIGH
+};
+
 struct scheduling_resources {
 	unsigned int vmid_mask;
 	enum kfd_queue_type type;
@@ -585,6 +656,17 @@ struct qcm_process_device {
 
 	/* doorbell resources per process per device */
 	unsigned long *doorbell_bitmap;
+	/* doorbell user mmap vma */
+	struct vm_area_struct *doorbell_vma;
+	/* lock to serialize doorbell unmap and remap */
+	struct mutex doorbell_lock;
+
+	/* Indicate if doorbell is mapped or unmapped
+	 * -1 means doorbells need to be unmapped because queue is evicted
+	 *  0 means doorbells are unmapped
+	 *  1 means doorbells are mapped
+	 */
+	int doorbell_mapped;
 };
 
 /* KFD Memory Eviction */
@@ -596,6 +678,9 @@ struct qcm_process_device {
 /* Approx. time before evicting the process again */
 #define PROCESS_ACTIVE_TIME_MS 10
 
+void kfd_process_schedule_restore(struct kfd_process *p);
+int kfd_process_remap_doorbells_locked(struct kfd_process *p);
+
 /* 8 byte handle containing GPU ID in the most significant 4 bytes and
  * idr_handle in the least significant 4 bytes
  */
@@ -649,6 +734,22 @@ struct kfd_process_device {
 	 */
 	bool already_dequeued;
 
+	/* Flag to indicate if debugging is active on this device for this
+	 * process.  This is for the new GFX9+ debugging, and indicates that
+	 * any of the debug features are enabled, ie: wave launch mode,
+	 * address watch, or trap debug.  It also indicates that a debug
+	 * VMID has been allocated.
+	 */
+	bool is_debugging_enabled;
+
+	/* Flag to indicate if trap debugging is active on this device for
+	 * this process.  This is for the GFX9_ debugging features
+	 */
+	bool debug_trap_enabled;
+
+	/* Value of the wave launch mode if debugging is enabled */
+	uint32_t trap_debug_wave_launch_mode;
+
 	/* Is this process/pasid bound to this device? (amd_iommu_bind_pasid) */
 	enum kfd_pdd_bound bound;
 };
@@ -687,6 +788,9 @@ struct kfd_process {
 	/* We want to receive a notification when the mm_struct is destroyed */
 	struct mmu_notifier mmu_notifier;
 
+	/* Use for delayed freeing of kfd_process structure */
+	struct rcu_head	rcu;
+
 	unsigned int pasid;
 	unsigned int doorbell_index;
 
@@ -711,6 +815,8 @@ struct kfd_process {
 	size_t signal_event_count;
 	bool signal_event_limit_reached;
 
+	struct rb_root_cached bo_interval_tree;
+
 	/* Information used for memory eviction */
 	void *kgd_process_info;
 	/* Eviction fence that is attached to all the BOs of this process. The
@@ -728,6 +834,7 @@ struct kfd_process {
 	 * restored after an eviction
 	 */
 	unsigned long last_restore_timestamp;
+	unsigned long last_evict_timestamp;
 
 	/* Kobj for our procfs */
 	struct kobject *kobj;
@@ -783,12 +890,23 @@ int kfd_reserved_mem_mmap(struct kfd_dev *dev, struct kfd_process *process,
 
 /* KFD process API for creating and translating handles */
 int kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,
-					void *mem);
+					void *mem, uint64_t start,
+					uint64_t length, uint64_t cpuva,
+					unsigned int mem_type,
+					struct kfd_ipc_obj *ipc_obj);
 void *kfd_process_device_translate_handle(struct kfd_process_device *p,
 					int handle);
+struct kfd_bo *kfd_process_device_find_bo(struct kfd_process_device *pdd,
+					int handle);
+void *kfd_process_find_bo_from_interval(struct kfd_process *p,
+					uint64_t start_addr,
+					uint64_t last_addr);
 void kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,
 					int handle);
 
+void run_rdma_free_callback(struct kfd_bo *buf_obj);
+struct kfd_process *kfd_lookup_process_by_pid(struct pid *pid);
+
 /* Process device data iterator */
 struct kfd_process_device *kfd_get_first_process_device_data(
 							struct kfd_process *p);
@@ -811,6 +929,8 @@ int kfd_doorbell_init(struct kfd_dev *kfd);
 void kfd_doorbell_fini(struct kfd_dev *kfd);
 int kfd_doorbell_mmap(struct kfd_dev *dev, struct kfd_process *process,
 		      struct vm_area_struct *vma);
+void kfd_doorbell_unmap(struct kfd_process_device *pdd);
+int kfd_doorbell_remap(struct kfd_process_device *pdd);
 void __iomem *kfd_get_kernel_doorbell(struct kfd_dev *kfd,
 					unsigned int *doorbell_off);
 void kfd_release_kernel_doorbell(struct kfd_dev *kfd, u32 __iomem *db_addr);
@@ -913,6 +1033,8 @@ int pqm_set_gws(struct process_queue_manager *pqm, unsigned int qid,
 			void *gws);
 struct kernel_queue *pqm_get_kernel_queue(struct process_queue_manager *pqm,
 						unsigned int qid);
+struct queue *pqm_get_user_queue(struct process_queue_manager *pqm,
+						unsigned int qid);
 int pqm_get_wave_state(struct process_queue_manager *pqm,
 		       unsigned int qid,
 		       void __user *ctl_stack,
@@ -1036,6 +1158,28 @@ int dbgdev_wave_reset_wavefronts(struct kfd_dev *dev, struct kfd_process *p);
 
 bool kfd_is_locked(void);
 
+/* PeerDirect support */
+void kfd_init_peer_direct(void);
+void kfd_close_peer_direct(void);
+
+/* IPC Support */
+int kfd_ipc_init(void);
+
+/* Cgroup Support */
+/* Check with device cgroup if @kfd device is accessible */
+static inline int kfd_devcgroup_check_permission(struct kfd_dev *kfd)
+{
+#if defined(CONFIG_CGROUP_DEVICE)
+	struct drm_device *ddev = kfd->ddev;
+
+	return devcgroup_check_permission(DEVCG_DEV_CHAR, DRM_MAJOR,
+					  ddev->render->index,
+					  DEVCG_ACC_WRITE | DEVCG_ACC_READ);
+#else
+	return 0;
+#endif
+}
+
 /* Compute profile */
 void kfd_inc_compute_active(struct kfd_dev *dev);
 void kfd_dec_compute_active(struct kfd_dev *dev);
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process.c b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
old mode 100644
new mode 100755
index 40e3fc0c6942..4cc2a7c49dbe
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process.c
@@ -31,6 +31,8 @@
 #include <linux/compat.h>
 #include <linux/mman.h>
 #include <linux/file.h>
+#include <asm/page.h>
+#include "kfd_ipc.h"
 #include "amdgpu_amdkfd.h"
 
 struct mm_struct;
@@ -39,6 +41,7 @@ struct mm_struct;
 #include "kfd_device_queue_manager.h"
 #include "kfd_dbgmgr.h"
 #include "kfd_iommu.h"
+#include "kfd_trace.h"
 
 /*
  * List of struct kfd_process (field kfd_process).
@@ -60,10 +63,11 @@ static struct workqueue_struct *kfd_process_wq;
  */
 static struct workqueue_struct *kfd_restore_wq;
 
-static struct kfd_process *find_process(const struct task_struct *thread);
+static struct kfd_process *find_process(const struct task_struct *thread,
+		bool ref);
 static void kfd_process_ref_release(struct kref *ref);
-static struct kfd_process *create_process(const struct task_struct *thread);
-static int kfd_process_init_cwsr_apu(struct kfd_process *p, struct file *filep);
+static struct kfd_process *create_process(const struct task_struct *thread,
+					struct file *filep);
 
 static void evict_process_worker(struct work_struct *work);
 static void restore_process_worker(struct work_struct *work);
@@ -181,9 +185,11 @@ static int kfd_process_alloc_gpuvm(struct kfd_process_device *pdd,
 	struct kgd_mem *mem = NULL;
 	int handle;
 	int err;
+	unsigned int mem_type;
 
 	err = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(kdev->kgd, gpu_va, size,
-						 pdd->vm, &mem, NULL, flags);
+						 pdd->vm, NULL, &mem, NULL,
+						 flags);
 	if (err)
 		goto err_alloc_mem;
 
@@ -197,12 +203,19 @@ static int kfd_process_alloc_gpuvm(struct kfd_process_device *pdd,
 		goto sync_memory_failed;
 	}
 
+	mem_type = flags & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM |
+			    KFD_IOC_ALLOC_MEM_FLAGS_GTT |
+			    KFD_IOC_ALLOC_MEM_FLAGS_USERPTR |
+			    KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |
+			    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);
+
 	/* Create an obj handle so kfd_process_device_remove_obj_handle
 	 * will take care of the bo removal when the process finishes.
 	 * We do not need to take p->mutex, because the process is just
 	 * created and the ioctls have not had the chance to run.
 	 */
-	handle = kfd_process_device_create_obj_handle(pdd, mem);
+	handle = kfd_process_device_create_obj_handle(
+			pdd, mem, gpu_va, size, 0, mem_type, NULL);
 
 	if (handle < 0) {
 		err = handle;
@@ -285,19 +298,11 @@ struct kfd_process *kfd_create_process(struct file *filep)
 	mutex_lock(&kfd_processes_mutex);
 
 	/* A prior open of /dev/kfd could have already created the process. */
-	process = find_process(thread);
+	process = find_process(thread, false);
 	if (process) {
 		pr_debug("Process already found\n");
 	} else {
-		process = create_process(thread);
-		if (IS_ERR(process))
-			goto out;
-
-		ret = kfd_process_init_cwsr_apu(process, filep);
-		if (ret) {
-			process = ERR_PTR(ret);
-			goto out;
-		}
+		process = create_process(thread, filep);
 
 		if (!procfs.kobj)
 			goto out;
@@ -340,7 +345,7 @@ struct kfd_process *kfd_get_process(const struct task_struct *thread)
 	if (thread->group_leader->mm != thread->mm)
 		return ERR_PTR(-EINVAL);
 
-	process = find_process(thread);
+	process = find_process(thread, false);
 	if (!process)
 		return ERR_PTR(-EINVAL);
 
@@ -359,13 +364,16 @@ static struct kfd_process *find_process_by_mm(const struct mm_struct *mm)
 	return NULL;
 }
 
-static struct kfd_process *find_process(const struct task_struct *thread)
+static struct kfd_process *find_process(const struct task_struct *thread,
+		bool ref)
 {
 	struct kfd_process *p;
 	int idx;
 
 	idx = srcu_read_lock(&kfd_processes_srcu);
 	p = find_process_by_mm(thread->mm);
+	if (p && ref)
+		kref_get(&p->ref);
 	srcu_read_unlock(&kfd_processes_srcu, idx);
 
 	return p;
@@ -376,17 +384,38 @@ void kfd_unref_process(struct kfd_process *p)
 	kref_put(&p->ref, kfd_process_ref_release);
 }
 
+/* This increments the process->ref counter. */
+struct kfd_process *kfd_lookup_process_by_pid(struct pid *pid)
+{
+	struct task_struct *task = NULL;
+	struct kfd_process *p    = NULL;
+
+	if (!pid) {
+		task = current;
+		get_task_struct(task);
+	} else {
+		task = get_pid_task(pid, PIDTYPE_PID);
+	}
+
+	if (task) {
+		p = find_process(task, true);
+		put_task_struct(task);
+	}
+
+	return p;
+}
+
 static void kfd_process_device_free_bos(struct kfd_process_device *pdd)
 {
 	struct kfd_process *p = pdd->process;
-	void *mem;
+	struct kfd_bo *buf_obj;
 	int id;
 
 	/*
 	 * Remove all handles from idr and release appropriate
 	 * local memory object
 	 */
-	idr_for_each_entry(&pdd->alloc_idr, mem, id) {
+	idr_for_each_entry(&pdd->alloc_idr, buf_obj, id) {
 		struct kfd_process_device *peer_pdd;
 
 		list_for_each_entry(peer_pdd, &p->per_device_data,
@@ -394,10 +423,12 @@ static void kfd_process_device_free_bos(struct kfd_process_device *pdd)
 			if (!peer_pdd->vm)
 				continue;
 			amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(
-				peer_pdd->dev->kgd, mem, peer_pdd->vm);
+				peer_pdd->dev->kgd, buf_obj->mem, peer_pdd->vm);
 		}
 
-		amdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->kgd, mem);
+		run_rdma_free_callback(buf_obj);
+		amdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->kgd,
+						      buf_obj->mem);
 		kfd_process_device_remove_obj_handle(pdd, id);
 	}
 }
@@ -436,7 +467,7 @@ static void kfd_process_destroy_pdds(struct kfd_process *p)
 
 		kfree(pdd->qpd.doorbell_bitmap);
 		idr_destroy(&pdd->alloc_idr);
-
+		mutex_destroy(&pdd->qpd.doorbell_lock);
 		kfree(pdd);
 	}
 }
@@ -486,9 +517,11 @@ static void kfd_process_ref_release(struct kref *ref)
 	queue_work(kfd_process_wq, &p->release_work);
 }
 
-static void kfd_process_free_notifier(struct mmu_notifier *mn)
+static void kfd_process_destroy_delayed(struct rcu_head *rcu)
 {
-	kfd_unref_process(container_of(mn, struct kfd_process, mmu_notifier));
+	struct kfd_process *p = container_of(rcu, struct kfd_process, rcu);
+
+	kfd_unref_process(p);
 }
 
 static void kfd_process_notifier_release(struct mmu_notifier *mn,
@@ -522,6 +555,7 @@ static void kfd_process_notifier_release(struct mmu_notifier *mn,
 	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
 		struct kfd_dev *dev = pdd->dev;
 
+		/* Old (deprecated) debugger for GFXv8 and older */
 		mutex_lock(kfd_get_dbgmgr_mutex());
 		if (dev && dev->dbgmgr && dev->dbgmgr->pasid == p->pasid) {
 			if (!kfd_dbgmgr_unregister(dev->dbgmgr, p)) {
@@ -530,6 +564,22 @@ static void kfd_process_notifier_release(struct mmu_notifier *mn,
 			}
 		}
 		mutex_unlock(kfd_get_dbgmgr_mutex());
+
+		/* New debugger for GFXv9 and later */
+		if (pdd->is_debugging_enabled) {
+			if (pdd->debug_trap_enabled) {
+				dev->kfd2kgd->disable_debug_trap(dev->kgd);
+				pdd->debug_trap_enabled = false;
+			}
+			if (pdd->trap_debug_wave_launch_mode != 0) {
+				dev->kfd2kgd->set_wave_launch_mode(
+					dev->kgd, 0,
+					dev->vm_info.last_vmid_kfd);
+				pdd->trap_debug_wave_launch_mode = 0;
+			}
+			release_debug_trap_vmid(dev->dqm);
+			pdd->is_debugging_enabled = false;
+		}
 	}
 
 	kfd_process_dequeue_from_all_devices(p);
@@ -540,12 +590,12 @@ static void kfd_process_notifier_release(struct mmu_notifier *mn,
 
 	mutex_unlock(&p->mutex);
 
-	mmu_notifier_put(&p->mmu_notifier);
+	mmu_notifier_unregister_no_release(&p->mmu_notifier, mm);
+	mmu_notifier_call_srcu(&p->rcu, &kfd_process_destroy_delayed);
 }
 
 static const struct mmu_notifier_ops kfd_process_mmu_notifier_ops = {
 	.release = kfd_process_notifier_release,
-	.free_notifier = kfd_process_free_notifier,
 };
 
 static int kfd_process_init_cwsr_apu(struct kfd_process *p, struct file *filep)
@@ -615,29 +665,18 @@ static int kfd_process_device_init_cwsr_dgpu(struct kfd_process_device *pdd)
 	return 0;
 }
 
-/*
- * On return the kfd_process is fully operational and will be freed when the
- * mm is released
- */
-static struct kfd_process *create_process(const struct task_struct *thread)
+static struct kfd_process *create_process(const struct task_struct *thread,
+					struct file *filep)
 {
 	struct kfd_process *process;
 	int err = -ENOMEM;
 
 	process = kzalloc(sizeof(*process), GFP_KERNEL);
+
 	if (!process)
 		goto err_alloc_process;
 
-	kref_init(&process->ref);
-	mutex_init(&process->mutex);
-	process->mm = thread->mm;
-	process->lead_thread = thread->group_leader;
-	INIT_LIST_HEAD(&process->per_device_data);
-	INIT_DELAYED_WORK(&process->eviction_work, evict_process_worker);
-	INIT_DELAYED_WORK(&process->restore_work, restore_process_worker);
-	process->last_restore_timestamp = get_jiffies_64();
-	kfd_event_init_process(process);
-	process->is_32bit_user_mode = in_compat_syscall();
+	process->bo_interval_tree = RB_ROOT_CACHED;
 
 	process->pasid = kfd_pasid_alloc();
 	if (process->pasid == 0)
@@ -646,38 +685,68 @@ static struct kfd_process *create_process(const struct task_struct *thread)
 	if (kfd_alloc_process_doorbells(process) < 0)
 		goto err_alloc_doorbells;
 
+	kref_init(&process->ref);
+
+	mutex_init(&process->mutex);
+
+	process->mm = thread->mm;
+
+	/* register notifier */
+	process->mmu_notifier.ops = &kfd_process_mmu_notifier_ops;
+	err = mmu_notifier_register(&process->mmu_notifier, process->mm);
+	if (err)
+		goto err_mmu_notifier;
+
+	hash_add_rcu(kfd_processes_table, &process->kfd_processes,
+			(uintptr_t)process->mm);
+
+	process->lead_thread = thread->group_leader;
+	get_task_struct(process->lead_thread);
+
+	INIT_LIST_HEAD(&process->per_device_data);
+
+	kfd_event_init_process(process);
+
 	err = pqm_init(&process->pqm, process);
 	if (err != 0)
 		goto err_process_pqm_init;
 
 	/* init process apertures*/
+	process->is_32bit_user_mode = in_compat_syscall();
 	err = kfd_init_apertures(process);
 	if (err != 0)
 		goto err_init_apertures;
 
-	/* Must be last, have to use release destruction after this */
-	process->mmu_notifier.ops = &kfd_process_mmu_notifier_ops;
-	err = mmu_notifier_register(&process->mmu_notifier, process->mm);
+	INIT_DELAYED_WORK(&process->eviction_work, evict_process_worker);
+	INIT_DELAYED_WORK(&process->restore_work, restore_process_worker);
+	process->last_restore_timestamp = get_jiffies_64();
+
+	err = kfd_process_init_cwsr_apu(process, filep);
 	if (err)
-		goto err_register_notifier;
+		goto err_init_cwsr;
 
-	get_task_struct(process->lead_thread);
-	hash_add_rcu(kfd_processes_table, &process->kfd_processes,
-			(uintptr_t)process->mm);
+	/* If PeerDirect interface was not detected try to detect it again
+	 * in case if network driver was loaded later.
+	 */
+	kfd_init_peer_direct();
 
 	return process;
 
-err_register_notifier:
+err_init_cwsr:
 	kfd_process_free_outstanding_kfd_bos(process);
 	kfd_process_destroy_pdds(process);
 err_init_apertures:
 	pqm_uninit(&process->pqm);
 err_process_pqm_init:
+	hash_del_rcu(&process->kfd_processes);
+	synchronize_rcu();
+	mmu_notifier_unregister_no_release(&process->mmu_notifier, process->mm);
+err_mmu_notifier:
+	mutex_destroy(&process->mutex);
 	kfd_free_process_doorbells(process);
 err_alloc_doorbells:
 	kfd_pasid_free(process->pasid);
 err_alloc_pasid:
-	mutex_destroy(&process->mutex);
 	kfree(process);
 err_alloc_process:
 	return ERR_PTR(err);
@@ -745,9 +814,13 @@ struct kfd_process_device *kfd_create_process_device_data(struct kfd_dev *dev,
 	pdd->qpd.dqm = dev->dqm;
 	pdd->qpd.pqm = &p->pqm;
 	pdd->qpd.evicted = 0;
+	mutex_init(&pdd->qpd.doorbell_lock);
 	pdd->process = p;
 	pdd->bound = PDD_UNBOUND;
 	pdd->already_dequeued = false;
+	pdd->is_debugging_enabled = false;
+	pdd->debug_trap_enabled = false;
+	pdd->trap_debug_wave_launch_mode = 0;
 	list_add(&pdd->per_device_list, &p->per_device_data);
 
 	/* Init idr used for memory handle translation */
@@ -874,9 +947,49 @@ bool kfd_has_process_device_data(struct kfd_process *p)
  * Assumes that the process lock is held.
  */
 int kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,
-					void *mem)
+					void *mem, uint64_t start,
+					uint64_t length, uint64_t cpuva,
+					unsigned int mem_type,
+					struct kfd_ipc_obj *ipc_obj)
+{
+	int handle;
+	struct kfd_bo *buf_obj;
+	struct kfd_process *p;
+
+	p = pdd->process;
+
+	buf_obj = kzalloc(sizeof(*buf_obj), GFP_KERNEL);
+
+	if (!buf_obj)
+		return -ENOMEM;
+
+	buf_obj->it.start = start;
+	buf_obj->it.last = start + length - 1;
+	interval_tree_insert(&buf_obj->it, &p->bo_interval_tree);
+
+	buf_obj->mem = mem;
+	buf_obj->dev = pdd->dev;
+	buf_obj->kfd_ipc_obj = ipc_obj;
+	buf_obj->cpuva = cpuva;
+	buf_obj->mem_type = mem_type;
+
+	INIT_LIST_HEAD(&buf_obj->cb_data_head);
+
+	handle = idr_alloc(&pdd->alloc_idr, buf_obj, 0, 0, GFP_KERNEL);
+
+	if (handle < 0)
+		kfree(buf_obj);
+
+	return handle;
+}
+
+struct kfd_bo *kfd_process_device_find_bo(struct kfd_process_device *pdd,
+					int handle)
 {
-	return idr_alloc(&pdd->alloc_idr, mem, 0, 0, GFP_KERNEL);
+	if (handle < 0)
+		return NULL;
+
+	return (struct kfd_bo *)idr_find(&pdd->alloc_idr, handle);
 }
 
 /* Translate specific handle from process local memory idr
@@ -885,10 +998,37 @@ int kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,
 void *kfd_process_device_translate_handle(struct kfd_process_device *pdd,
 					int handle)
 {
-	if (handle < 0)
+	struct kfd_bo *buf_obj;
+
+	buf_obj = kfd_process_device_find_bo(pdd, handle);
+
+	return buf_obj->mem;
+}
+
+void *kfd_process_find_bo_from_interval(struct kfd_process *p,
+					uint64_t start_addr,
+					uint64_t last_addr)
+{
+	struct interval_tree_node *it_node;
+	struct kfd_bo *buf_obj;
+
+	it_node = interval_tree_iter_first(&p->bo_interval_tree,
+			start_addr, last_addr);
+	if (!it_node) {
+		pr_err("0x%llx-0x%llx does not relate to an existing buffer\n",
+				start_addr, last_addr);
+		return NULL;
+	}
+
+	if (interval_tree_iter_next(it_node, start_addr, last_addr)) {
+		pr_err("0x%llx-0x%llx spans more than a single BO\n",
+				start_addr, last_addr);
 		return NULL;
+	}
 
-	return idr_find(&pdd->alloc_idr, handle);
+	buf_obj = container_of(it_node, struct kfd_bo, it);
+
+	return buf_obj;
 }
 
 /* Remove specific handle from process local memory idr
@@ -897,8 +1037,24 @@ void *kfd_process_device_translate_handle(struct kfd_process_device *pdd,
 void kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,
 					int handle)
 {
-	if (handle >= 0)
-		idr_remove(&pdd->alloc_idr, handle);
+	struct kfd_bo *buf_obj;
+	struct kfd_process *p;
+
+	p = pdd->process;
+
+	if (handle < 0)
+		return;
+
+	buf_obj = kfd_process_device_find_bo(pdd, handle);
+
+	if (buf_obj->kfd_ipc_obj)
+		ipc_obj_put(&buf_obj->kfd_ipc_obj);
+
+	idr_remove(&pdd->alloc_idr, handle);
+
+	interval_tree_remove(&buf_obj->it, &p->bo_interval_tree);
+
+	kfree(buf_obj);
 }
 
 /* This increments the process->ref counter. */
@@ -978,7 +1134,7 @@ int kfd_process_evict_queues(struct kfd_process *p)
 	return r;
 }
 
-/* process_restore_queues - Restore all user queues of a process */
+/* kfd_process_restore_queues - Restore all user queues of a process */
 int kfd_process_restore_queues(struct kfd_process *p)
 {
 	struct kfd_process_device *pdd;
@@ -997,6 +1153,95 @@ int kfd_process_restore_queues(struct kfd_process *p)
 	return ret;
 }
 
+void kfd_process_schedule_restore(struct kfd_process *p)
+{
+	int ret;
+	unsigned long evicted_jiffies;
+	unsigned long delay_jiffies = msecs_to_jiffies(PROCESS_RESTORE_TIME_MS);
+
+	/* wait at least PROCESS_RESTORE_TIME_MS before attempting to restore
+	 */
+	evicted_jiffies = get_jiffies_64() - p->last_evict_timestamp;
+	if (delay_jiffies > evicted_jiffies)
+		delay_jiffies -= evicted_jiffies;
+	else
+		delay_jiffies = 0;
+
+	pr_debug("Process %d schedule restore work\n", p->pasid);
+	ret = queue_delayed_work(kfd_restore_wq, &p->restore_work,
+				delay_jiffies);
+	WARN(!ret, "Schedule restore work failed\n");
+}
+
+static void kfd_process_unmap_doorbells(struct kfd_process *p)
+{
+	struct kfd_process_device *pdd;
+	struct mm_struct *mm = p->mm;
+
+	down_write(&mm->mmap_sem);
+
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list)
+		kfd_doorbell_unmap(pdd);
+
+	up_write(&mm->mmap_sem);
+}
+
+int kfd_process_remap_doorbells_locked(struct kfd_process *p)
+{
+	struct kfd_process_device *pdd;
+	int ret = 0;
+
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list)
+		ret = kfd_doorbell_remap(pdd);
+
+	return ret;
+}
+
+static int kfd_process_remap_doorbells(struct kfd_process *p)
+{
+	struct mm_struct *mm = p->mm;
+	int ret = 0;
+
+	down_write(&mm->mmap_sem);
+	ret = kfd_process_remap_doorbells_locked(p);
+	up_write(&mm->mmap_sem);
+
+	return ret;
+}
+
+/**
+ * kfd_process_unmap_doorbells_if_idle - Check if queues are active
+ *
+ * Returns true if queues are idle, and unmap doorbells.
+ * Returns false if queues are active
+ */
+static bool kfd_process_unmap_doorbells_if_idle(struct kfd_process *p)
+{
+	struct kfd_process_device *pdd;
+	bool busy = false;
+
+	if (!keep_idle_process_evicted)
+		return false;
+
+	/* Unmap doorbell first to avoid race conditions. Otherwise while the
+	 * second queue is checked, the first queue may get more work, but we
+	 * won't detect that since it has been checked
+	 */
+	kfd_process_unmap_doorbells(p);
+
+	list_for_each_entry(pdd, &p->per_device_data, per_device_list) {
+		busy = check_if_queues_active(pdd->qpd.dqm, &pdd->qpd);
+		if (busy)
+			break;
+	}
+
+	/* Remap doorbell if process queue is not idle */
+	if (busy)
+		kfd_process_remap_doorbells(p);
+
+	return !busy;
+}
+
 static void evict_process_worker(struct work_struct *work)
 {
 	int ret;
@@ -1009,6 +1254,7 @@ static void evict_process_worker(struct work_struct *work)
 	 * lifetime of this thread, kfd_process p will be valid
 	 */
 	p = container_of(dwork, struct kfd_process, eviction_work);
+	trace_kfd_evict_process_worker_start(p);
 	WARN_ONCE(p->last_eviction_seqno != p->ef->seqno,
 		  "Eviction fence mismatch\n");
 
@@ -1020,18 +1266,25 @@ static void evict_process_worker(struct work_struct *work)
 	 */
 	flush_delayed_work(&p->restore_work);
 
-	pr_debug("Started evicting pasid %d\n", p->pasid);
+	p->last_evict_timestamp = get_jiffies_64();
+
+	pr_info("Started evicting pasid %d\n", p->pasid);
 	ret = kfd_process_evict_queues(p);
 	if (!ret) {
 		dma_fence_signal(p->ef);
 		dma_fence_put(p->ef);
 		p->ef = NULL;
-		queue_delayed_work(kfd_restore_wq, &p->restore_work,
-				msecs_to_jiffies(PROCESS_RESTORE_TIME_MS));
 
-		pr_debug("Finished evicting pasid %d\n", p->pasid);
+		if (!kfd_process_unmap_doorbells_if_idle(p))
+			kfd_process_schedule_restore(p);
+		else
+			pr_debug("Process %d queues idle, doorbell unmapped\n",
+				p->pasid);
+
+		pr_info("Finished evicting pasid %d\n", p->pasid);
 	} else
 		pr_err("Failed to evict queues of pasid %d\n", p->pasid);
+	trace_kfd_evict_process_worker_end(p, ret ? "Failed" : "Success");
 }
 
 static void restore_process_worker(struct work_struct *work)
@@ -1046,6 +1299,9 @@ static void restore_process_worker(struct work_struct *work)
 	 * lifetime of this thread, kfd_process p will be valid
 	 */
 	p = container_of(dwork, struct kfd_process, restore_work);
+
+	trace_kfd_restore_process_worker_start(p);
+
 	pr_debug("Started restoring pasid %d\n", p->pasid);
 
 	/* Setting last_restore_timestamp before successful restoration.
@@ -1062,17 +1318,22 @@ static void restore_process_worker(struct work_struct *work)
 	ret = amdgpu_amdkfd_gpuvm_restore_process_bos(p->kgd_process_info,
 						     &p->ef);
 	if (ret) {
-		pr_debug("Failed to restore BOs of pasid %d, retry after %d ms\n",
+		pr_info("Failed to restore BOs of pasid %d, retry after %d ms\n",
 			 p->pasid, PROCESS_BACK_OFF_TIME_MS);
+
 		ret = queue_delayed_work(kfd_restore_wq, &p->restore_work,
 				msecs_to_jiffies(PROCESS_BACK_OFF_TIME_MS));
-		WARN(!ret, "reschedule restore work failed\n");
+		WARN(!ret, "Reschedule restore work failed\n");
+		trace_kfd_restore_process_worker_end(p, ret ?
+					"Rescheduled restore" :
+					"Failed to reschedule restore");
 		return;
 	}
 
 	ret = kfd_process_restore_queues(p);
+	trace_kfd_restore_process_worker_end(p,	ret ? "Failed" : "Success");
 	if (!ret)
-		pr_debug("Finished restoring pasid %d\n", p->pasid);
+		pr_info("Finished restoring pasid %d\n", p->pasid);
 	else
 		pr_err("Failed to restore queues of pasid %d\n", p->pasid);
 }
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
old mode 100644
new mode 100755
index 7e6c3ee82f5b..7a61a5b09ed8
--- a/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_process_queue_manager.c
@@ -473,6 +473,15 @@ struct kernel_queue *pqm_get_kernel_queue(
 	return NULL;
 }
 
+struct queue *pqm_get_user_queue(struct process_queue_manager *pqm,
+					unsigned int qid)
+{
+	struct process_queue_node *pqn;
+
+	pqn = get_queue_by_qid(pqm, qid);
+	return pqn ? pqn->q : NULL;
+}
+
 int pqm_get_wave_state(struct process_queue_manager *pqm,
 		       unsigned int qid,
 		       void __user *ctl_stack,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c b/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
new file mode 100755
index 000000000000..58a11fab2bc7
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_rdma.c
@@ -0,0 +1,297 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+#include <linux/device.h>
+#include <linux/export.h>
+#include <linux/pid.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <drm/amd_rdma.h>
+#include "kfd_priv.h"
+#include "amdgpu_amdkfd.h"
+
+struct rdma_cb {
+	struct list_head node;
+	struct amd_p2p_info amd_p2p_data;
+	void  (*free_callback)(void *client_priv);
+	void  *client_priv;
+};
+
+/**
+ * This function makes the pages underlying a range of GPU virtual memory
+ * accessible for DMA operations from another PCIe device
+ *
+ * \param   address       - The start address in the Unified Virtual Address
+ *			    space in the specified process
+ * \param   length        - The length of requested mapping
+ * \param   pid           - Pointer to structure pid to which address belongs.
+ *			    Could be NULL for current process address space.
+ * \param   p2p_data    - On return: Pointer to structure describing
+ *			    underlying pages/locations
+ * \param   free_callback - Pointer to callback which will be called when access
+ *			    to such memory must be stopped immediately: Memory
+ *			    was freed, GECC events, etc.
+ *			    Client should  immediately stop any transfer
+ *			    operations and returned as soon as possible.
+ *			    After return all resources associated with address
+ *			    will be release and no access will be allowed.
+ * \param   client_priv   - Pointer to be passed as parameter on
+ *			    'free_callback;
+ *
+ * \return  0 if operation was successful
+ */
+static int get_pages(uint64_t address, uint64_t length, struct pid *pid,
+		struct amd_p2p_info **amd_p2p_data,
+		void  (*free_callback)(void *client_priv),
+		void  *client_priv)
+{
+	struct kfd_bo *buf_obj;
+	struct kgd_mem *mem;
+	struct sg_table *sg_table_tmp;
+	struct kfd_dev *dev;
+	uint64_t last = address + length - 1;
+	uint64_t offset;
+	struct kfd_process *p;
+	struct rdma_cb *rdma_cb_data;
+	int ret = 0;
+
+	p = kfd_lookup_process_by_pid(pid);
+	if (!p) {
+		pr_err("Could not find the process\n");
+		return -EINVAL;
+	}
+	mutex_lock(&p->mutex);
+
+	buf_obj = kfd_process_find_bo_from_interval(p, address, last);
+	if (!buf_obj) {
+		pr_err("Cannot find a kfd_bo for the range\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	rdma_cb_data = kmalloc(sizeof(*rdma_cb_data), GFP_KERNEL);
+	if (!rdma_cb_data) {
+		*amd_p2p_data = NULL;
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	mem = buf_obj->mem;
+	dev = buf_obj->dev;
+	offset = address - buf_obj->it.start;
+
+	ret = amdgpu_amdkfd_gpuvm_pin_get_sg_table(dev->kgd, mem,
+			offset, length, &sg_table_tmp);
+
+	if (ret) {
+		pr_err("amdgpu_amdkfd_gpuvm_pin_get_sg_table failed.\n");
+		*amd_p2p_data = NULL;
+		goto free_mem;
+	}
+
+	rdma_cb_data->amd_p2p_data.va = address;
+	rdma_cb_data->amd_p2p_data.size = length;
+	rdma_cb_data->amd_p2p_data.pid = pid;
+	rdma_cb_data->amd_p2p_data.priv = buf_obj;
+	rdma_cb_data->amd_p2p_data.pages = sg_table_tmp;
+
+	rdma_cb_data->free_callback = free_callback;
+	rdma_cb_data->client_priv = client_priv;
+
+	list_add(&rdma_cb_data->node, &buf_obj->cb_data_head);
+
+	kfd_inc_compute_active(dev);
+	*amd_p2p_data = &rdma_cb_data->amd_p2p_data;
+
+	goto out;
+
+free_mem:
+	kfree(rdma_cb_data);
+out:
+	mutex_unlock(&p->mutex);
+	kfd_unref_process(p);
+
+	return ret;
+}
+
+static int put_pages_helper(struct amd_p2p_info *p2p_data)
+{
+	struct kfd_bo *buf_obj;
+	struct kfd_dev *dev;
+	struct sg_table *sg_table_tmp;
+	struct rdma_cb *rdma_cb_data;
+
+	if (!p2p_data) {
+		pr_err("amd_p2p_info pointer is invalid.\n");
+		return -EINVAL;
+	}
+
+	rdma_cb_data = container_of(p2p_data, struct rdma_cb, amd_p2p_data);
+
+	buf_obj = p2p_data->priv;
+	dev = buf_obj->dev;
+	sg_table_tmp = p2p_data->pages;
+
+	list_del(&rdma_cb_data->node);
+	kfree(rdma_cb_data);
+
+	amdgpu_amdkfd_gpuvm_unpin_put_sg_table(buf_obj->mem, sg_table_tmp);
+	kfd_dec_compute_active(dev);
+
+	return 0;
+}
+
+void run_rdma_free_callback(struct kfd_bo *buf_obj)
+{
+	struct rdma_cb *tmp, *rdma_cb_data;
+
+	list_for_each_entry_safe(rdma_cb_data, tmp,
+			&buf_obj->cb_data_head, node) {
+		if (rdma_cb_data->free_callback)
+			rdma_cb_data->free_callback(
+					rdma_cb_data->client_priv);
+	}
+	list_for_each_entry_safe(rdma_cb_data, tmp,
+			&buf_obj->cb_data_head, node)
+		put_pages_helper(&rdma_cb_data->amd_p2p_data);
+}
+
+/**
+ *
+ * This function release resources previously allocated by get_pages() call.
+ *
+ * \param   p_p2p_data - A pointer to pointer to amd_p2p_info entries
+ *			allocated by get_pages() call.
+ *
+ * \return  0 if operation was successful
+ */
+static int put_pages(struct amd_p2p_info **p_p2p_data)
+{
+	struct kfd_process *p = NULL;
+	int ret = 0;
+
+	if (!(*p_p2p_data)) {
+		pr_err("amd_p2p_info pointer is invalid.\n");
+		return -EINVAL;
+	}
+
+	p = kfd_lookup_process_by_pid((*p_p2p_data)->pid);
+	if (!p) {
+		pr_err("Could not find the process\n");
+		return -EINVAL;
+	}
+
+	ret = put_pages_helper(*p_p2p_data);
+
+	if (!ret)
+		*p_p2p_data = NULL;
+
+	kfd_unref_process(p);
+
+	return ret;
+}
+
+/**
+ * Check if given address belongs to GPU address space.
+ *
+ * \param   address - Address to check
+ * \param   pid     - Process to which given address belongs.
+ *		      Could be NULL if current one.
+ *
+ * \return  0  - This is not GPU address managed by AMD driver
+ *	    1  - This is GPU address managed by AMD driver
+ */
+static int is_gpu_address(uint64_t address, struct pid *pid)
+{
+	struct kfd_bo *buf_obj;
+	struct kfd_process *p;
+
+	p = kfd_lookup_process_by_pid(pid);
+	if (!p) {
+		pr_debug("Could not find the process\n");
+		return 0;
+	}
+
+	buf_obj = kfd_process_find_bo_from_interval(p, address, address);
+
+	kfd_unref_process(p);
+	if (!buf_obj)
+		return 0;
+
+	return 1;
+}
+
+/**
+ * Return the single page size to be used when building scatter/gather table
+ * for given range.
+ *
+ * \param   address   - Address
+ * \param   length    - Range length
+ * \param   pid       - Process id structure. Could be NULL if current one.
+ * \param   page_size - On return: Page size
+ *
+ * \return  0 if operation was successful
+ */
+static int get_page_size(uint64_t address, uint64_t length, struct pid *pid,
+			unsigned long *page_size)
+{
+	/*
+	 * As local memory is always consecutive, we can assume the local
+	 * memory page size to be arbitrary.
+	 * Currently we assume the local memory page size to be the same
+	 * as system memory, which is 4KB.
+	 */
+	*page_size = PAGE_SIZE;
+
+	return 0;
+}
+
+
+/**
+ * Singleton object: rdma interface function pointers
+ */
+static const struct amd_rdma_interface  rdma_ops = {
+	.get_pages = get_pages,
+	.put_pages = put_pages,
+	.is_gpu_address = is_gpu_address,
+	.get_page_size = get_page_size,
+};
+
+/**
+ * amdkfd_query_rdma_interface - Return interface (function pointers table) for
+ *				 rdma interface
+ *
+ *
+ * \param interace     - OUT: Pointer to interface
+ *
+ * \return 0 if operation was successful.
+ */
+int amdkfd_query_rdma_interface(const struct amd_rdma_interface **ops)
+{
+	*ops  = &rdma_ops;
+
+	return 0;
+}
+EXPORT_SYMBOL(amdkfd_query_rdma_interface);
+
+
+
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
old mode 100644
new mode 100755
index f2170f0e4334..772b02326f70
--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.c
@@ -218,6 +218,8 @@ struct kfd_topology_device *kfd_create_topology_device(
 		sysfs_show_gen_prop(buffer, "%s %llu\n", name, value)
 #define sysfs_show_32bit_val(buffer, value) \
 		sysfs_show_gen_prop(buffer, "%u\n", value)
+#define sysfs_show_64bit_val(buffer, value) \
+		sysfs_show_gen_prop(buffer, "%llu\n", value)
 #define sysfs_show_str_val(buffer, value) \
 		sysfs_show_gen_prop(buffer, "%s\n", value)
 
@@ -269,6 +271,8 @@ static ssize_t iolink_show(struct kobject *kobj, struct attribute *attr,
 	buffer[0] = 0;
 
 	iolink = container_of(attr, struct kfd_iolink_properties, attr);
+	if (iolink->gpu && kfd_devcgroup_check_permission(iolink->gpu))
+		return -EPERM;
 	sysfs_show_32bit_prop(buffer, "type", iolink->iolink_type);
 	sysfs_show_32bit_prop(buffer, "version_major", iolink->ver_maj);
 	sysfs_show_32bit_prop(buffer, "version_minor", iolink->ver_min);
@@ -300,11 +304,27 @@ static ssize_t mem_show(struct kobject *kobj, struct attribute *attr,
 {
 	ssize_t ret;
 	struct kfd_mem_properties *mem;
+	uint64_t used_mem;
 
 	/* Making sure that the buffer is an empty string */
 	buffer[0] = 0;
 
-	mem = container_of(attr, struct kfd_mem_properties, attr);
+	if (strcmp(attr->name, "used_memory") == 0) {
+		mem = container_of(attr, struct kfd_mem_properties,
+				attr_used);
+		if (mem->gpu) {
+			if (kfd_devcgroup_check_permission(mem->gpu))
+				return -EPERM;
+			used_mem = amdgpu_amdkfd_get_vram_usage(mem->gpu->kgd);
+			return sysfs_show_64bit_val(buffer, used_mem);
+		}
+		/* TODO: Report APU/CPU-allocated memory; For now return 0 */
+		return 0;
+	}
+
+	mem = container_of(attr, struct kfd_mem_properties, attr_props);
+	if (mem->gpu && kfd_devcgroup_check_permission(mem->gpu))
+		return -EPERM;
 	sysfs_show_32bit_prop(buffer, "heap_type", mem->heap_type);
 	sysfs_show_64bit_prop(buffer, "size_in_bytes", mem->size_in_bytes);
 	sysfs_show_32bit_prop(buffer, "flags", mem->flags);
@@ -334,6 +354,8 @@ static ssize_t kfd_cache_show(struct kobject *kobj, struct attribute *attr,
 	buffer[0] = 0;
 
 	cache = container_of(attr, struct kfd_cache_properties, attr);
+	if (cache->gpu && kfd_devcgroup_check_permission(cache->gpu))
+		return -EPERM;
 	sysfs_show_32bit_prop(buffer, "processor_id_low",
 			cache->processor_id_low);
 	sysfs_show_32bit_prop(buffer, "level", cache->cache_level);
@@ -407,6 +429,7 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 {
 	struct kfd_topology_device *dev;
 	uint32_t log_max_watch_addr;
+	struct kfd_local_mem_info local_mem_info;
 
 	/* Making sure that the buffer is an empty string */
 	buffer[0] = 0;
@@ -414,18 +437,24 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 	if (strcmp(attr->name, "gpu_id") == 0) {
 		dev = container_of(attr, struct kfd_topology_device,
 				attr_gpuid);
+		if (dev->gpu && kfd_devcgroup_check_permission(dev->gpu))
+			return -EPERM;
 		return sysfs_show_32bit_val(buffer, dev->gpu_id);
 	}
 
 	if (strcmp(attr->name, "name") == 0) {
 		dev = container_of(attr, struct kfd_topology_device,
 				attr_name);
+		if (dev->gpu && kfd_devcgroup_check_permission(dev->gpu))
+			return -EPERM;
 
 		return sysfs_show_str_val(buffer, dev->node_props.name);
 	}
 
 	dev = container_of(attr, struct kfd_topology_device,
 			attr_props);
+	if (dev->gpu && kfd_devcgroup_check_permission(dev->gpu))
+		return -EPERM;
 	sysfs_show_32bit_prop(buffer, "cpu_cores_count",
 			dev->node_props.cpu_cores_count);
 	sysfs_show_32bit_prop(buffer, "simd_count",
@@ -496,13 +525,26 @@ static ssize_t node_show(struct kobject *kobj, struct attribute *attr,
 		sysfs_show_32bit_prop(buffer, "max_engine_clk_fcompute",
 			dev->node_props.max_engine_clk_fcompute);
 
-		sysfs_show_64bit_prop(buffer, "local_mem_size",
-				(unsigned long long int) 0);
+		/*
+		 * If the ASIC is APU except Kaveri, set local memory size
+		 * to 0 to disable local memory support
+		 */
+		if (!dev->gpu->device_info->needs_iommu_device
+			|| dev->gpu->device_info->asic_family == CHIP_KAVERI) {
+			amdgpu_amdkfd_get_local_mem_info(dev->gpu->kgd,
+				&local_mem_info);
+			sysfs_show_64bit_prop(buffer, "local_mem_size",
+					local_mem_info.local_mem_size_private +
+					local_mem_info.local_mem_size_public);
+		} else
+			sysfs_show_64bit_prop(buffer, "local_mem_size", 0ULL);
 
 		sysfs_show_32bit_prop(buffer, "fw_version",
 				dev->gpu->mec_fw_version);
 		sysfs_show_32bit_prop(buffer, "capability",
 				dev->node_props.capability);
+		sysfs_show_64bit_prop(buffer, "debug_prop",
+				dev->node_props.debug_prop);
 		sysfs_show_32bit_prop(buffer, "sdma_fw_version",
 				dev->gpu->sdma_fw_version);
 	}
@@ -561,7 +603,12 @@ static void kfd_remove_sysfs_node_entry(struct kfd_topology_device *dev)
 	if (dev->kobj_mem) {
 		list_for_each_entry(mem, &dev->mem_props, list)
 			if (mem->kobj) {
-				kfd_remove_sysfs_file(mem->kobj, &mem->attr);
+				/* TODO: Remove when CPU/APU supported */
+				if (dev->node_props.cpu_cores_count == 0)
+					sysfs_remove_file(mem->kobj,
+							&mem->attr_used);
+				kfd_remove_sysfs_file(mem->kobj,
+						&mem->attr_props);
 				mem->kobj = NULL;
 			}
 		kobject_del(dev->kobj_mem);
@@ -663,12 +710,23 @@ static int kfd_build_sysfs_node_entry(struct kfd_topology_device *dev,
 		if (ret < 0)
 			return ret;
 
-		mem->attr.name = "properties";
-		mem->attr.mode = KFD_SYSFS_FILE_MODE;
-		sysfs_attr_init(&mem->attr);
-		ret = sysfs_create_file(mem->kobj, &mem->attr);
+		mem->attr_props.name = "properties";
+		mem->attr_props.mode = KFD_SYSFS_FILE_MODE;
+		sysfs_attr_init(&mem->attr_props);
+		ret = sysfs_create_file(mem->kobj, &mem->attr_props);
 		if (ret < 0)
 			return ret;
+
+		/* TODO: Support APU/CPU memory usage */
+		if (dev->node_props.cpu_cores_count == 0) {
+			mem->attr_used.name = "used_memory";
+			mem->attr_used.mode = KFD_SYSFS_FILE_MODE;
+			sysfs_attr_init(&mem->attr_used);
+			ret = sysfs_create_file(mem->kobj, &mem->attr_used);
+			if (ret < 0)
+				return ret;
+		}
+
 		i++;
 	}
 
@@ -927,6 +985,7 @@ static void kfd_add_non_crat_information(struct kfd_topology_device *kdev)
 	/* TODO: For GPU node, rearrange code from kfd_topology_add_device */
 }
 
+#ifdef CONFIG_ACPI
 /* kfd_is_acpi_crat_invalid - CRAT from ACPI is valid only for AMD APU devices.
  *	Ignore CRAT for all other devices. AMD APU is identified if both CPU
  *	and GPU cores are present.
@@ -945,6 +1004,7 @@ static bool kfd_is_acpi_crat_invalid(struct list_head *device_list)
 	pr_info("Ignoring ACPI CRAT on non-APU system\n");
 	return true;
 }
+#endif
 
 int kfd_topology_init(void)
 {
@@ -982,6 +1042,7 @@ int kfd_topology_init(void)
 	 * NOTE: The current implementation expects all AMD APUs to have
 	 *	CRAT. If no CRAT is available, it is assumed to be a CPU
 	 */
+#ifdef CONFIG_ACPI
 	ret = kfd_create_crat_image_acpi(&crat_image, &image_size);
 	if (!ret) {
 		ret = kfd_parse_crat_table(crat_image,
@@ -995,7 +1056,7 @@ int kfd_topology_init(void)
 			crat_image = NULL;
 		}
 	}
-
+#endif
 	if (!crat_image) {
 		ret = kfd_create_crat_image_virtual(&crat_image, &image_size,
 						    COMPUTE_UNIT_CPU, NULL,
@@ -1098,6 +1159,9 @@ static struct kfd_topology_device *kfd_assign_gpu(struct kfd_dev *gpu)
 {
 	struct kfd_topology_device *dev;
 	struct kfd_topology_device *out_dev = NULL;
+	struct kfd_mem_properties *mem;
+	struct kfd_cache_properties *cache;
+	struct kfd_iolink_properties *iolink;
 
 	down_write(&topology_lock);
 	list_for_each_entry(dev, &topology_device_list, list) {
@@ -1111,6 +1175,14 @@ static struct kfd_topology_device *kfd_assign_gpu(struct kfd_dev *gpu)
 		if (!dev->gpu && (dev->node_props.simd_count > 0)) {
 			dev->gpu = gpu;
 			out_dev = dev;
+
+			/* Assign mem->gpu */
+			list_for_each_entry(mem, &dev->mem_props, list)
+				mem->gpu = dev->gpu;
+			list_for_each_entry(cache, &dev->cache_props, list)
+				cache->gpu = dev->gpu;
+			list_for_each_entry(iolink, &dev->io_link_props, list)
+				iolink->gpu = dev->gpu;
 			break;
 		}
 	}
@@ -1275,7 +1347,8 @@ int kfd_topology_add_device(struct kfd_dev *gpu)
 
 	dev->node_props.vendor_id = gpu->pdev->vendor;
 	dev->node_props.device_id = gpu->pdev->device;
-	dev->node_props.location_id = pci_dev_id(gpu->pdev);
+	dev->node_props.location_id = PCI_DEVID(gpu->pdev->bus->number,
+		gpu->pdev->devfn);
 	dev->node_props.max_engine_clk_fcompute =
 		amdgpu_amdkfd_get_max_engine_clock_in_mhz(dev->gpu->kgd);
 	dev->node_props.max_engine_clk_ccompute =
@@ -1324,6 +1397,12 @@ int kfd_topology_add_device(struct kfd_dev *gpu)
 		dev->node_props.capability |= ((HSA_CAP_DOORBELL_TYPE_2_0 <<
 			HSA_CAP_DOORBELL_TYPE_TOTALBITS_SHIFT) &
 			HSA_CAP_DOORBELL_TYPE_TOTALBITS_MASK);
+
+		dev->node_props.capability |= HSA_CAP_TRAP_DEBUG_SUPPORT |
+			HSA_CAP_TRAP_DEBUG_WAVE_LAUNCH_TRAP_OVERRIDE_SUPPORTED |
+			HSA_CAP_TRAP_DEBUG_WAVE_LAUNCH_MODE_SUPPORTED;
+		dev->node_props.debug_prop |=
+					HSA_DBG_TRAP_DEBUG_TRAP_DATA_COUNT;
 		break;
 	default:
 		WARN(1, "Unexpected ASIC family %u",
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_topology.h b/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
old mode 100644
new mode 100755
index d4718d58d0f2..ba0c62084cc6
--- a/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_topology.h
@@ -47,6 +47,11 @@
 #define HSA_CAP_DOORBELL_TYPE_1_0		0x1
 #define HSA_CAP_DOORBELL_TYPE_2_0		0x2
 #define HSA_CAP_AQL_QUEUE_DOUBLE_MAP		0x00004000
+#define HSA_CAP_TRAP_DEBUG_SUPPORT				0x00008000
+#define HSA_CAP_TRAP_DEBUG_WAVE_LAUNCH_TRAP_OVERRIDE_SUPPORTED	0x00010000
+#define HSA_CAP_TRAP_DEBUG_WAVE_LAUNCH_MODE_SUPPORTED		0x00020000
+
+#define HSA_DBG_TRAP_DEBUG_TRAP_DATA_COUNT			0x00008000
 
 #define HSA_CAP_SRAM_EDCSUPPORTED		0x00080000
 #define HSA_CAP_MEM_EDCSUPPORTED		0x00100000
@@ -62,6 +67,7 @@ struct kfd_node_properties {
 	uint32_t cpu_core_id_base;
 	uint32_t simd_id_base;
 	uint32_t capability;
+	uint64_t debug_prop;
 	uint32_t max_waves_per_simd;
 	uint32_t lds_size_in_kb;
 	uint32_t gds_size_in_kb;
@@ -103,7 +109,9 @@ struct kfd_mem_properties {
 	uint32_t		width;
 	uint32_t		mem_clk_max;
 	struct kobject		*kobj;
-	struct attribute	attr;
+	struct kfd_dev		*gpu;
+	struct attribute	attr_props;
+	struct attribute	attr_used;
 };
 
 #define HSA_CACHE_TYPE_DATA		0x00000001
@@ -124,6 +132,7 @@ struct kfd_cache_properties {
 	uint32_t		cache_type;
 	uint8_t			sibling_map[CRAT_SIBLINGMAP_SIZE];
 	struct kobject		*kobj;
+	struct kfd_dev		*gpu;
 	struct attribute	attr;
 };
 
@@ -142,6 +151,7 @@ struct kfd_iolink_properties {
 	uint32_t		rec_transfer_size;
 	uint32_t		flags;
 	struct kobject		*kobj;
+	struct kfd_dev		*gpu;
 	struct attribute	attr;
 };
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_trace.c b/drivers/gpu/drm/amd/amdkfd/kfd_trace.c
new file mode 100755
index 000000000000..805a1da90bb1
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_trace.c
@@ -0,0 +1,26 @@
+/*
+ * Copyright 2018 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+
+#define CREATE_TRACE_POINTS
+#include "kfd_trace.h"
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_trace.h b/drivers/gpu/drm/amd/amdkfd/kfd_trace.h
new file mode 100755
index 000000000000..345cded4a11a
--- /dev/null
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_trace.h
@@ -0,0 +1,151 @@
+/*
+ * Copyright 2018 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+
+#if !defined(_AMDKFD_TRACE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _KFD_TRACE_H_
+
+
+#include <linux/stringify.h>
+#include <linux/types.h>
+#include <linux/tracepoint.h>
+
+#include "kfd_priv.h"
+#include <linux/kfd_ioctl.h>
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM amdkfd
+#define TRACE_INCLUDE_FILE kfd_trace
+
+
+TRACE_EVENT(kfd_map_memory_to_gpu_start,
+	    TP_PROTO(struct kfd_process *p),
+	    TP_ARGS(p),
+	    TP_STRUCT__entry(
+			    __field(unsigned int, pasid)
+			    ),
+	    TP_fast_assign(
+			   __entry->pasid = p->pasid;
+			   ),
+	    TP_printk("pasid =%u", __entry->pasid)
+);
+
+
+TRACE_EVENT(kfd_map_memory_to_gpu_end,
+	    TP_PROTO(struct kfd_process *p, u32 array_size, char *pStatusMsg),
+	    TP_ARGS(p, array_size, pStatusMsg),
+	    TP_STRUCT__entry(
+				__field(unsigned int, pasid)
+				__field(unsigned int, array_size)
+				__string(pStatusMsg, pStatusMsg)
+			    ),
+	    TP_fast_assign(
+			   __entry->pasid = p->pasid;
+				__entry->array_size = array_size;
+				__assign_str(pStatusMsg, pStatusMsg);
+			   ),
+	    TP_printk("pasid = %u, array_size =	%u, StatusMsg=%s",
+				__entry->pasid,
+				 __entry->array_size,
+				 __get_str(pStatusMsg))
+);
+
+
+TRACE_EVENT(kfd_kgd2kfd_schedule_evict_and_restore_process,
+	    TP_PROTO(struct kfd_process *p, u32 delay_jiffies),
+	    TP_ARGS(p, delay_jiffies),
+	    TP_STRUCT__entry(
+				__field(unsigned int, pasid)
+				__field(unsigned int, delay_jiffies)
+			    ),
+	    TP_fast_assign(
+			   __entry->pasid = p->pasid;
+			   __entry->delay_jiffies = delay_jiffies;
+			),
+	    TP_printk("pasid = %u, delay_jiffies = %u",
+		      __entry->pasid,
+		      __entry->delay_jiffies)
+);
+
+
+TRACE_EVENT(kfd_evict_process_worker_start,
+	    TP_PROTO(struct kfd_process *p),
+	    TP_ARGS(p),
+	    TP_STRUCT__entry(
+				__field(unsigned int, pasid)
+			    ),
+	    TP_fast_assign(
+			   __entry->pasid = p->pasid;
+			   ),
+	    TP_printk("pasid=%u", __entry->pasid)
+);
+
+
+TRACE_EVENT(kfd_evict_process_worker_end,
+	    TP_PROTO(struct kfd_process *p, char *pStatusMsg),
+	    TP_ARGS(p, pStatusMsg),
+	    TP_STRUCT__entry(
+			    __field(unsigned int, pasid)
+			    __string(pStatusMsg, pStatusMsg)
+			    ),
+	    TP_fast_assign(
+			    __entry->pasid = p->pasid;
+			    __assign_str(pStatusMsg, pStatusMsg);
+			   ),
+	    TP_printk("pasid=%u, StatusMsg=%s",
+			    __entry->pasid, __get_str(pStatusMsg))
+);
+
+
+TRACE_EVENT(kfd_restore_process_worker_start,
+	    TP_PROTO(struct kfd_process *p),
+	    TP_ARGS(p),
+	    TP_STRUCT__entry(
+				__field(unsigned int, pasid)
+			    ),
+	    TP_fast_assign(
+			   __entry->pasid = p->pasid;
+			   ),
+	    TP_printk("pasid=%u", __entry->pasid)
+);
+
+TRACE_EVENT(kfd_restore_process_worker_end,
+	    TP_PROTO(struct kfd_process *p, char *pStatusMsg),
+	    TP_ARGS(p, pStatusMsg),
+	    TP_STRUCT__entry(
+				__field(unsigned int, pasid)
+				__string(pStatusMsg, pStatusMsg)
+			    ),
+	    TP_fast_assign(
+				 entry->pasid = p->pasid;
+				__assign_str(pStatusMsg, pStatusMsg);
+			   ),
+	    TP_printk("pasid=%u, StatusMsg=%s",
+			    __entry->pasid, __get_str(pStatusMsg))
+);
+
+#endif
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH ../../drivers/gpu/drm/amd/amdkfd
+#include <trace/define_trace.h>
diff --git a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
old mode 100644
new mode 100755
index 98b9533e672b..46c76476929b
--- a/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
+++ b/drivers/gpu/drm/amd/include/kgd_kfd_interface.h
@@ -31,6 +31,9 @@
 #include <linux/types.h>
 #include <linux/bitmap.h>
 #include <linux/dma-fence.h>
+#include <linux/dma-buf.h>
+#include <linux/mm_types.h>
+#include <linux/scatterlist.h>
 
 struct pci_dev;
 
@@ -38,6 +41,7 @@ struct pci_dev;
 
 struct kfd_dev;
 struct kgd_dev;
+struct drm_device;
 
 struct kgd_mem;
 
@@ -309,6 +313,19 @@ struct kfd2kgd_calls {
 	uint32_t (*read_vmid_from_vmfault_reg)(struct kgd_dev *kgd);
 	uint64_t (*get_hive_id)(struct kgd_dev *kgd);
 
+	uint32_t (*enable_debug_trap)(struct kgd_dev *kgd,
+					uint32_t trap_debug_wave_launch_mode,
+					uint32_t vmid);
+	uint32_t (*disable_debug_trap)(struct kgd_dev *kgd);
+	uint32_t (*set_debug_trap_data)(struct kgd_dev *kgd,
+					int trap_data0,
+					int trap_data1);
+	uint32_t (*set_wave_launch_trap_override)(struct kgd_dev *kgd,
+						uint32_t trap_override,
+						uint32_t trap_mask);
+	uint32_t (*set_wave_launch_mode)(struct kgd_dev *kgd,
+					uint8_t wave_launch_mode,
+					uint32_t vmid);
 };
 
 #endif	/* KGD_KFD_INTERFACE_H_INCLUDED */
diff --git a/include/drm/amd_rdma.h b/include/drm/amd_rdma.h
new file mode 100755
index 000000000000..b0cab3c2b03c
--- /dev/null
+++ b/include/drm/amd_rdma.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright 2015 Advanced Micro Devices, Inc.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR
+ * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+ * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+ * OTHER DEALINGS IN THE SOFTWARE.
+ */
+
+/* @file This file defined kernel interfaces to communicate with amdkfd */
+
+#ifndef AMD_RDMA_H_
+#define AMD_RDMA_H_
+
+
+/**
+ * Structure describing information needed to P2P access from another device
+ * to specific location of GPU memory
+ */
+struct amd_p2p_info {
+	uint64_t	va;		/**< Specify user virt. address
+					  * which this page table
+					  * described
+					  */
+	uint64_t	size;		/**< Specify total size of
+					  * allocation
+					  */
+	struct pid	*pid;		/**< Specify process pid to which
+					  * virtual address belongs
+					  */
+	struct sg_table *pages;		/**< Specify DMA/Bus addresses */
+	void		*priv;		/**< Pointer set by AMD kernel
+					  * driver
+					  */
+};
+
+/**
+ * Structure providing function pointers to support rdma/p2p requirements.
+ * to specific location of GPU memory
+ */
+struct amd_rdma_interface {
+	int (*get_pages)(uint64_t address, uint64_t length, struct pid *pid,
+				struct amd_p2p_info  **amd_p2p_data,
+				void  (*free_callback)(void *client_priv),
+				void  *client_priv);
+	int (*put_pages)(struct amd_p2p_info **amd_p2p_data);
+	int (*is_gpu_address)(uint64_t address, struct pid *pid);
+	int (*get_page_size)(uint64_t address, uint64_t length, struct pid *pid,
+				unsigned long *page_size);
+};
+
+
+int amdkfd_query_rdma_interface(const struct amd_rdma_interface **rdma);
+
+
+#endif /* AMD_RDMA_H_ */
+
diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
index 8f063c0654c2..f27473a8c74c 100644
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -528,6 +528,9 @@ static inline void mmu_notifier_range_init(struct mmu_notifier_range *range,
 	set_pte_at(___mm, ___address, __ptep, ___pte);			\
 })
 
+extern void mmu_notifier_call_srcu(struct rcu_head *rcu,
+                                   void (*func)(struct rcu_head *rcu));
+
 #else /* CONFIG_MMU_NOTIFIER */
 
 struct mmu_notifier_range {
diff --git a/include/uapi/drm/amdgpu_drm.h b/include/uapi/drm/amdgpu_drm.h
old mode 100644
new mode 100755
index 30f2781fb6e2..0e4752af4109
--- a/include/uapi/drm/amdgpu_drm.h
+++ b/include/uapi/drm/amdgpu_drm.h
@@ -1003,6 +1003,8 @@ struct drm_amdgpu_info_firmware {
 #define AMDGPU_VRAM_TYPE_DDR4  8
 #define AMDGPU_VRAM_TYPE_GDDR6 9
 
+#define AMDGPU_VRAM_TYPE_HBM_WIDTH 4096
+
 struct drm_amdgpu_info_device {
 	/** PCI Device ID */
 	__u32 device_id;
diff --git a/include/uapi/linux/kfd_ioctl.h b/include/uapi/linux/kfd_ioctl.h
old mode 100644
new mode 100755
index 20917c59f39c..8ec65f1732b3
--- a/include/uapi/linux/kfd_ioctl.h
+++ b/include/uapi/linux/kfd_ioctl.h
@@ -23,11 +23,11 @@
 #ifndef KFD_IOCTL_H_INCLUDED
 #define KFD_IOCTL_H_INCLUDED
 
-#include <drm/drm.h>
+#include <linux/types.h>
 #include <linux/ioctl.h>
 
 #define KFD_IOCTL_MAJOR_VERSION 1
-#define KFD_IOCTL_MINOR_VERSION 1
+#define KFD_IOCTL_MINOR_VERSION 2
 
 struct kfd_ioctl_get_version_args {
 	__u32 major_version;	/* from KFD */
@@ -187,6 +187,64 @@ struct kfd_ioctl_dbg_wave_control_args {
 	__u32 buf_size_in_bytes;	/*including gpu_id and buf_size */
 };
 
+/* KFD_IOC_DBG_TRAP_ENABLE:
+ * ptr:   unused
+ * data1: 0=disable, 1=enable
+ * data2: queue ID (for future use)
+ * data3: unused
+ */
+#define KFD_IOC_DBG_TRAP_ENABLE 0
+
+/* KFD_IOC_DBG_TRAP_SET_TRAP_DATA:
+ * ptr:   unused
+ * data1: SPI_GDBG_TRAP_DATA0
+ * data2: SPI_GDBG_TRAP_DATA1
+ * data3: unused
+ */
+#define KFD_IOC_DBG_TRAP_SET_TRAP_DATA 1
+
+/* KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_OVERRIDE:
+ * ptr:   unused
+ * data1: override mode: 0=OR, 1=REPLACE
+ * data2: mask
+ * data3: unused
+ */
+#define KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_OVERRIDE 2
+
+/* KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE:
+ * ptr:   unused
+ * data1: 0=normal, 1=halt, 2=kill, 3=singlestep, 4=disable
+ * data2: unused
+ * data3: unused
+ */
+#define KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE 3
+
+/* KFD_IOC_DBG_TRAP_NODE_SUSPEND:
+ * ptr:   pointer to an array of Queues IDs
+ * data1: flags
+ * data2: number of queues
+ * data3: grace period
+ */
+#define KFD_IOC_DBG_TRAP_NODE_SUSPEND 4
+
+/* KFD_IOC_DBG_TRAP_NODE_RESUME:
+ * ptr:   pointer to an array of Queues IDs
+ * data1: flags
+ * data2: number of queues
+ * data3: unused
+ */
+#define KFD_IOC_DBG_TRAP_NODE_RESUME 5
+
+struct kfd_ioctl_dbg_trap_args {
+	__u64 ptr;     /* to KFD -- used for pointer arguments: queue arrays */
+	__u32 pid;     /* to KFD */
+	__u32 gpu_id;  /* to KFD */
+	__u32 op;      /* to KFD */
+	__u32 data1;   /* to KFD */
+	__u32 data2;   /* to KFD */
+	__u32 data3;   /* to KFD */
+};
+
 /* Matching HSA_EVENTTYPE */
 #define KFD_IOC_EVENT_SIGNAL			0
 #define KFD_IOC_EVENT_NODECHANGE		1
@@ -251,7 +309,7 @@ struct kfd_memory_exception_failure {
 	__u32 imprecise;	/* Can't determine the	exact fault address */
 };
 
-/* memory exception data*/
+/* memory exception data */
 struct kfd_hsa_memory_exception_data {
 	struct kfd_memory_exception_failure failure;
 	__u64 va;
@@ -410,6 +468,20 @@ struct kfd_ioctl_unmap_memory_from_gpu_args {
 	__u32 n_success;		/* to/from KFD */
 };
 
+/* Allocate GWS for specific queue
+ *
+ * @queue_id:    queue's id that GWS is allocated for
+ * @num_gws:     how many GWS to allocate
+ * @first_gws:   index of the first GWS allocated.
+ *               only support contiguous GWS allocation
+ */
+struct kfd_ioctl_alloc_queue_gws_args {
+	__u32 queue_id;		/* to KFD */
+	__u32 num_gws;		/* to KFD */
+	__u32 first_gws;	/* from KFD */
+	__u32 pad;
+};
+
 struct kfd_ioctl_get_dmabuf_info_args {
 	__u64 size;		/* from KFD */
 	__u64 metadata_ptr;	/* to KFD */
@@ -435,6 +507,53 @@ enum kfd_mmio_remap {
 	KFD_MMIO_REMAP_HDP_REG_FLUSH_CNTL = 4,
 };
 
+struct kfd_ioctl_ipc_export_handle_args {
+	__u64 handle;		/* to KFD */
+	__u32 share_handle[4];	/* from KFD */
+	__u32 gpu_id;		/* to KFD */
+	__u32 pad;
+};
+
+struct kfd_ioctl_ipc_import_handle_args {
+	__u64 handle;		/* from KFD */
+	__u64 va_addr;		/* to KFD */
+	__u64 mmap_offset;		/* from KFD */
+	__u32 share_handle[4];	/* to KFD */
+	__u32 gpu_id;		/* to KFD */
+	__u32 pad;
+};
+
+struct kfd_memory_range {
+	__u64 va_addr;
+	__u64 size;
+};
+
+/* flags definitions
+ * BIT0: 0: read operation, 1: write operation.
+ * This also identifies if the src or dst array belongs to remote process
+ */
+#define KFD_CROSS_MEMORY_RW_BIT (1 << 0)
+#define KFD_SET_CROSS_MEMORY_READ(flags) (flags &= ~KFD_CROSS_MEMORY_RW_BIT)
+#define KFD_SET_CROSS_MEMORY_WRITE(flags) (flags |= KFD_CROSS_MEMORY_RW_BIT)
+#define KFD_IS_CROSS_MEMORY_WRITE(flags) (flags & KFD_CROSS_MEMORY_RW_BIT)
+
+struct kfd_ioctl_cross_memory_copy_args {
+	/* to KFD: Process ID of the remote process */
+	__u32 pid;
+	/* to KFD: See above definition */
+	__u32 flags;
+	/* to KFD: Source GPU VM range */
+	__u64 src_mem_range_array;
+	/* to KFD: Size of above array */
+	__u64 src_mem_array_size;
+	/* to KFD: Destination GPU VM range */
+	__u64 dst_mem_range_array;
+	/* to KFD: Size of above array */
+	__u64 dst_mem_array_size;
+	/* from KFD: Total amount of bytes copied */
+	__u64 bytes_copied;
+};
+
 #define AMDKFD_IOCTL_BASE 'K'
 #define AMDKFD_IO(nr)			_IO(AMDKFD_IOCTL_BASE, nr)
 #define AMDKFD_IOR(nr, type)		_IOR(AMDKFD_IOCTL_BASE, nr, type)
@@ -529,7 +648,22 @@ enum kfd_mmio_remap {
 #define AMDKFD_IOC_IMPORT_DMABUF		\
 		AMDKFD_IOWR(0x1D, struct kfd_ioctl_import_dmabuf_args)
 
+#define AMDKFD_IOC_ALLOC_QUEUE_GWS		\
+		AMDKFD_IOWR(0x1E, struct kfd_ioctl_alloc_queue_gws_args)
+
+#define AMDKFD_IOC_IPC_IMPORT_HANDLE                                    \
+		AMDKFD_IOWR(0x1F, struct kfd_ioctl_ipc_import_handle_args)
+
+#define AMDKFD_IOC_IPC_EXPORT_HANDLE		\
+		AMDKFD_IOWR(0x20, struct kfd_ioctl_ipc_export_handle_args)
+
+#define AMDKFD_IOC_DBG_TRAP			\
+		AMDKFD_IOWR(0x21, struct kfd_ioctl_dbg_trap_args)
+
+#define AMDKFD_IOC_CROSS_MEMORY_COPY		\
+		AMDKFD_IOWR(0x22, struct kfd_ioctl_cross_memory_copy_args)
+
 #define AMDKFD_COMMAND_START		0x01
-#define AMDKFD_COMMAND_END		0x1E
+#define AMDKFD_COMMAND_END		0x23
 
 #endif
diff --git a/kernel/fork.c b/kernel/fork.c
index 6cabc124378c..38a7884a4328 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1235,6 +1235,7 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 
 	return mm;
 }
+EXPORT_SYMBOL_GPL(mm_access);
 
 static void complete_vfork_done(struct task_struct *tsk)
 {
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index 252bf4e3e00c..d46a7a2c7fb4 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -27,6 +27,18 @@ struct lockdep_map __mmu_notifier_invalidate_range_start_map = {
 };
 #endif
 
+/*
+ * This function allows mmu_notifier::release callback to delay a call to
+ * a function that will free appropriate resources. The function must be
+ * quick and must not block.
+ */
+void mmu_notifier_call_srcu(struct rcu_head *rcu,
+                            void (*func)(struct rcu_head *rcu))
+{
+        call_srcu(&srcu, rcu, func);
+}
+EXPORT_SYMBOL_GPL(mmu_notifier_call_srcu);
+
 /*
  * This function can't run concurrently against mmu_notifier_register
  * because mm->mm_users > 0 during mmu_notifier_register and exit_mmap
diff --git a/security/device_cgroup.c b/security/device_cgroup.c
index 725674f3276d..8492af9a3e67 100644
--- a/security/device_cgroup.c
+++ b/security/device_cgroup.c
@@ -824,3 +824,4 @@ int __devcgroup_check_permission(short type, u32 major, u32 minor,
 
 	return 0;
 }
+EXPORT_SYMBOL(__devcgroup_check_permission);
-- 
2.17.1

