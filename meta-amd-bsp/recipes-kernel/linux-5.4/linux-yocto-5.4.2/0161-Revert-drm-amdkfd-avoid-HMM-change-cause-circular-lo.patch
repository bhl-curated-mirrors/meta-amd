From b523f2bcb742c5869e069f781ba89c67bee29f9c Mon Sep 17 00:00:00 2001
From: Flora Cui <flora.cui@amd.com>
Date: Wed, 25 Sep 2019 10:37:38 +0800
Subject: [PATCH 0161/1453] Revert "drm/amdkfd: avoid HMM change cause circular
 lock"

This reverts commit 89cd9d23e9a74d94f0db5bbbaf2ef1f6ede36ae5.
---
 .../drm/amd/amdkfd/kfd_device_queue_manager.c | 29 +++++++++----------
 1 file changed, 13 insertions(+), 16 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index 5e88c0fa7a0d..f8c400287fb4 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -1135,20 +1135,22 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 	int retval;
 	struct mqd_manager *mqd_mgr;
 
+	retval = 0;
+
+	dqm_lock(dqm);
+
 	if (dqm->total_queue_count >= max_num_of_queues_per_device) {
 		pr_warn("Can't create new usermode queue because %d queues were already created\n",
 				dqm->total_queue_count);
 		retval = -EPERM;
-		goto out;
+		goto out_unlock;
 	}
 
 	if (q->properties.type == KFD_QUEUE_TYPE_SDMA ||
 		q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {
-		dqm_lock(dqm);
 		retval = allocate_sdma_queue(dqm, q);
-		dqm_unlock(dqm);
 		if (retval)
-			goto out;
+			goto out_unlock;
 	}
 
 	retval = allocate_doorbell(qpd, q);
@@ -1169,7 +1171,6 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 		goto out_deallocate_doorbell;
 	}
 
-	dqm_lock(dqm);
 	/*
 	 * Eviction state logic: mark all queues as evicted, even ones
 	 * not currently active. Restoring inactive queues later only
@@ -1208,11 +1209,11 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 out_deallocate_sdma_queue:
 	if (q->properties.type == KFD_QUEUE_TYPE_SDMA ||
 		q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {
-		dqm_lock(dqm);
 		deallocate_sdma_queue(dqm, q);
-		dqm_unlock(dqm);
 	}
-out:
+out_unlock:
+	dqm_unlock(dqm);
+
 	return retval;
 }
 
@@ -1379,6 +1380,8 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 			qpd->reset_wavefronts = true;
 	}
 
+	mqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);
+
 	/*
 	 * Unconditionally decrement this counter, regardless of the queue's
 	 * type
@@ -1389,9 +1392,6 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 
 	dqm_unlock(dqm);
 
-	/* Do free_mqd after dqm_unlock(dqm) to avoid circular locking */
-	mqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);
-
 	return retval;
 
 failed_try_destroy_debugged_queue:
@@ -1622,17 +1622,13 @@ static int process_termination_cpsch(struct device_queue_manager *dqm,
 		qpd->reset_wavefronts = false;
 	}
 
-	dqm_unlock(dqm);
-
 	/* Outside the DQM lock because under the DQM lock we can't do
 	 * reclaim or take other locks that others hold while reclaiming.
 	 */
 	if (found)
 		kfd_dec_compute_active(dqm->dev);
 
-	/* Lastly, free mqd resources.
-	 * Do free_mqd() after dqm_unlock to avoid circular locking.
-	 */
+	/* lastly, free mqd resources */
 	list_for_each_entry_safe(q, next, &qpd->queues_list, list) {
 		mqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(
 				q->properties.type)];
@@ -1641,6 +1637,7 @@ static int process_termination_cpsch(struct device_queue_manager *dqm,
 		mqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);
 	}
 
+	dqm_unlock(dqm);
 	return retval;
 }
 
-- 
2.17.1

