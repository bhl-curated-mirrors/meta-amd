From 6ec64ebb840522918c418b7e0a3c2ace9814780e Mon Sep 17 00:00:00 2001
From: shaoyunl <shaoyun.liu@amd.com>
Date: Fri, 22 Feb 2019 16:20:38 -0500
Subject: [PATCH 1570/2940] drm/amdgpu: Enable XGMI mapping for peer device
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Adjust vram base offset for XGMI mapping when update the PT entry so
the address will fall into correct XGMI aperture for peer device

Change-Id: I78bdf244da699d2559481ef5afe9663b3e752236
Signed-off-by: shaoyunl <shaoyun.liu@amd.com>
Reviewed-by: Christian KÃ¶nig <christian.koenig@amd.com>
Signed-off-by: Chaudhary Amit Kumar <Chaudharyamit.Kumar@amd.com>
---
 drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c | 81 ++++++++++----------------
 1 file changed, 31 insertions(+), 50 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
index e3c71d15f8bc..bf5b4f6ef3df 100644
--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c
@@ -1842,6 +1842,7 @@ static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev,
  * @vm: requested vm
  * @mapping: mapped range and flags to use for the update
  * @flags: HW flags for the mapping
+ * @bo_adev: amdgpu_device pointer that bo actually been allocated
  * @nodes: array of drm_mm_nodes with the MC addresses
  * @fence: optional resulting fence
  *
@@ -1856,12 +1857,11 @@ static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 				      dma_addr_t *pages_addr,
 				      struct amdgpu_vm *vm,
 				      struct amdgpu_bo_va_mapping *mapping,
-				      uint64_t vram_base_offset,
 				      uint64_t flags,
-				      struct ttm_mem_reg *mem,
+				      struct amdgpu_device *bo_adev,
+				      struct drm_mm_node *nodes,
 				      struct dma_fence **fence)
 {
-	struct drm_mm_node *nodes = mem ? mem->mm_node : NULL;
 	unsigned min_linear_pages = 1 << adev->vm_manager.fragment_size;
 	uint64_t pfn, start = mapping->start;
 	int r;
@@ -1906,46 +1906,34 @@ static int amdgpu_vm_bo_split_mapping(struct amdgpu_device *adev,
 			addr = nodes->start << PAGE_SHIFT;
 			max_entries = (nodes->size - pfn) *
 				(PAGE_SIZE / AMDGPU_GPU_PAGE_SIZE);
-		switch (mem->mem_type) {
-			case TTM_PL_TT:
-				max_entries = min(max_entries, 16ull * 1024ull);
-			for (count = 1;
-				 count < max_entries / AMDGPU_GPU_PAGES_IN_CPU_PAGE;
-				 ++count){
-				uint64_t idx = pfn + count;
+		} else {
+                        addr = 0;
+                        max_entries = S64_MAX;
+                }
+
+                if (pages_addr) {
+                        uint64_t count;
+
+                        for (count = 1;
+                        	count < max_entries / AMDGPU_GPU_PAGES_IN_CPU_PAGE;
+                             	++count) {
+                                	uint64_t idx = pfn + count;
+
 					if (pages_addr[idx] !=
 						(pages_addr[idx - 1] + PAGE_SIZE))
 					break;
 				}
-				if (count < min_linear_pages) {
-					addr = pfn << PAGE_SHIFT;
-					dma_addr = pages_addr;
-				} else {
-					addr = pages_addr[pfn];
-					max_entries = count;
-				}
-				break;
-			case AMDGPU_PL_DGMA_IMPORT:
-				addr = 0;
-				max_entries = min(max_entries, 16ull * 1024ull);
+			if (count < min_linear_pages) {
+                                addr = pfn << PAGE_SHIFT;
 				dma_addr = pages_addr;
-				break;
-			case AMDGPU_PL_DGMA:
-				addr += vram_base_offset +
-					adev->mman.bdev.man[mem->mem_type].gpu_offset -
-					adev->mman.bdev.man[TTM_PL_VRAM].gpu_offset;
-				addr += pfn << PAGE_SHIFT;
-				break;
-			case TTM_PL_VRAM:
-				addr += vram_base_offset;
-				addr += pfn << PAGE_SHIFT;
-				break;
-			default:
-				break;
+			} else {
+                                addr = pages_addr[pfn];
+                                max_entries = count * AMDGPU_GPU_PAGES_IN_CPU_PAGE;	
 			}
-		} else {
-			addr = 0;
-			max_entries = S64_MAX;
+
+		} else if (flags & AMDGPU_PTE_VALID) {
+			addr += bo_adev->vm_manager.vram_base_offset;
+			addr += pfn << PAGE_SHIFT;
 		}
 
 		last = min((uint64_t)mapping->last, start + max_entries - 1);
@@ -1991,11 +1979,10 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 	struct drm_mm_node *nodes;
 	struct dma_fence *exclusive, **last_update;
 	uint64_t flags;
-	uint64_t vram_base_offset = adev->vm_manager.vram_base_offset;
-	struct amdgpu_device *bo_adev;
+	struct amdgpu_device *bo_adev = adev;
 	int r;
 
-	if (clear || !bo_va->base.bo) {
+	if (clear || !bo) {
 		mem = NULL;
 		nodes = NULL;
 		exclusive = NULL;
@@ -2007,8 +1994,6 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 		if (mem->mem_type == TTM_PL_TT) {
 			ttm = container_of(bo->tbo.ttm, struct ttm_dma_tt, ttm);
 			pages_addr = ttm->dma_address;
-		} else if (mem->mem_type == AMDGPU_PL_DGMA_IMPORT) {
-			pages_addr = (dma_addr_t *)bo_va->base.bo->tbo.mem.bus.addr;
 		}
 		exclusive = reservation_object_get_excl(bo->tbo.resv);
 	}
@@ -2016,13 +2001,9 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 	if (bo) {
 		flags = amdgpu_ttm_tt_pte_flags(adev, bo->tbo.ttm, mem);
 		bo_adev = amdgpu_ttm_adev(bo->tbo.bdev);
-		if (mem && mem->mem_type == TTM_PL_VRAM &&
-			adev != bo_adev) {
-			flags |= AMDGPU_PTE_SYSTEM;
-			vram_base_offset = bo_adev->gmc.aper_base;
-		}
-	} else
+	} else {
 		flags = 0x0;
+	}
 
 	if (clear || (bo && bo->tbo.resv == vm->root.base.bo->tbo.resv))
 		last_update = &vm->last_update;
@@ -2039,8 +2020,8 @@ int amdgpu_vm_bo_update(struct amdgpu_device *adev,
 
 	list_for_each_entry(mapping, &bo_va->invalids, list) {
 		r = amdgpu_vm_bo_split_mapping(adev, exclusive, pages_addr, vm,
-					       mapping, vram_base_offset, flags,
-					       mem, last_update);
+						mapping, flags, bo_adev, nodes,
+						last_update);
 		if (r)
 			return r;
 	}
-- 
2.17.1

