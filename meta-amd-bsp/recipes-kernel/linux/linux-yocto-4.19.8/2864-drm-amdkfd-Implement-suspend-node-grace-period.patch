From 9f91bb641d12975a65f051fd707fad68f1d11a30 Mon Sep 17 00:00:00 2001
From: Philip Cox <Philip.Cox@amd.com>
Date: Thu, 30 May 2019 09:50:59 -0400
Subject: [PATCH 2864/2940] drm/amdkfd: Implement suspend node grace period.

Add the grace period for the KFD debugger when suspending waves.

Change-Id: I63bd1a6f23194ce7abe3a9417ad1ccfe843f38c1
Signed-off-by: Philip Cox <Philip.Cox@amd.com>
---
 .../drm/amd/amdkfd/kfd_device_queue_manager.c | 73 +++++++++++++------
 .../drm/amd/amdkfd/kfd_device_queue_manager.h |  2 +
 .../gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c  |  3 +
 .../gpu/drm/amd/amdkfd/kfd_packet_manager.c   | 32 ++++++++
 drivers/gpu/drm/amd/amdkfd/kfd_priv.h         |  2 +
 5 files changed, 91 insertions(+), 21 deletions(-)

diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index fceb8fdf01b9..603672d63f22 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -45,10 +45,12 @@ static int set_pasid_vmid_mapping(struct device_queue_manager *dqm,
 
 static int execute_queues_cpsch(struct device_queue_manager *dqm,
 								enum kfd_unmap_queues_filter filter,
-								uint32_t filter_param);
+								uint32_t filter_param,
+								uint32_t grace_period);
 static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 							  enum kfd_unmap_queues_filter filter,
-							  uint32_t filter_param);
+							  uint32_t filter_param,
+							  uint32_t grace_period);
 
 static int map_queues_cpsch(struct device_queue_manager *dqm);
 
@@ -541,7 +543,8 @@ static int update_queue(struct device_queue_manager *dqm, struct queue *q)
   /* Make sure the queue is unmapped before updating the MQD */
   if (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS) {
 	retval = unmap_queues_cpsch(dqm,
-								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+								USE_DEFAULT_GRACE_PERIOD);
 	if (retval) {
 	  pr_err("unmap queue failed\n");
 	  goto out_unlock;
@@ -724,7 +727,8 @@ static int evict_process_queues_cpsch(struct device_queue_manager *dqm,
   retval = execute_queues_cpsch(dqm,
 								qpd->is_debug ?
 								KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES :
-								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+								USE_DEFAULT_GRACE_PERIOD);
 
 out:
   dqm_unlock(dqm);
@@ -842,7 +846,8 @@ static int restore_process_queues_cpsch(struct device_queue_manager *dqm,
 	dqm->queue_count++;
   }
   retval = execute_queues_cpsch(dqm,
-								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+								KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+								USE_DEFAULT_GRACE_PERIOD);
   qpd->evicted = 0;
 out:
   dqm_unlock(dqm);
@@ -1139,7 +1144,8 @@ static int start_cpsch(struct device_queue_manager *dqm)
   dqm_lock(dqm);
   /* clear hang status when driver try to start the hw scheduler */
   dqm->is_hws_hang = false;
-  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+					   USE_DEFAULT_GRACE_PERIOD);
   dqm_unlock(dqm);
 
   return 0;
@@ -1153,7 +1159,8 @@ static int start_cpsch(struct device_queue_manager *dqm)
 static int stop_cpsch(struct device_queue_manager *dqm)
 {
   dqm_lock(dqm);
-  unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+  unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,
+					 USE_DEFAULT_GRACE_PERIOD);
   dqm_unlock(dqm);
 
   kfd_gtt_sa_free(dqm->dev, dqm->fence_mem);
@@ -1185,7 +1192,8 @@ static int create_kernel_queue_cpsch(struct device_queue_manager *dqm,
   list_add(&kq->list, &qpd->priv_queue_list);
   dqm->queue_count++;
   qpd->is_debug = true;
-  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+					   USE_DEFAULT_GRACE_PERIOD);
   dqm_unlock(dqm);
 
   return 0;
@@ -1199,7 +1207,8 @@ static void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,
   list_del(&kq->list);
   dqm->queue_count--;
   qpd->is_debug = false;
-  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+  execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,
+					   USE_DEFAULT_GRACE_PERIOD);
   /*
    * Unconditionally decrement this counter, regardless of the queue's
    * type.
@@ -1266,7 +1275,8 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 	if (q->properties.is_active) {
 		dqm->queue_count++;
 		retval = execute_queues_cpsch(dqm,
-				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+				USE_DEFAULT_GRACE_PERIOD);
 	}
 
 	if (q->properties.type == KFD_QUEUE_TYPE_SDMA)
@@ -1361,7 +1371,8 @@ static int map_queues_cpsch(struct device_queue_manager *dqm)
 /* dqm->lock mutex has to be locked before calling this function */
 static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 				enum kfd_unmap_queues_filter filter,
-				uint32_t filter_param)
+				uint32_t filter_param,
+				uint32_t grace_period)
 {
 	int retval = 0;
 
@@ -1373,6 +1384,12 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 	pr_debug("Before destroying queues, sdma queue count is : %u, xgmi sdma queue count is : %u\n",
 		dqm->sdma_queue_count, dqm->xgmi_sdma_queue_count);
 
+	if (grace_period != USE_DEFAULT_GRACE_PERIOD) {
+		retval = pm_update_grace_period(&dqm->packets, grace_period);
+		if (retval)
+			return retval;
+	}
+
 	if (dqm->sdma_queue_count > 0 || dqm->xgmi_sdma_queue_count)
 		unmap_sdma_queues(dqm);
 
@@ -1380,7 +1397,6 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 			filter, filter_param, false, 0);
 	if (retval)
 		return retval;
-
 	*dqm->fence_addr = KFD_FENCE_INIT;
 	pm_send_query_status(&dqm->packets, dqm->fence_gpu_addr,
 				KFD_FENCE_COMPLETED);
@@ -1390,6 +1406,13 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 	if (retval)
 		return retval;
 
+	/* We need to reset the grace period value for this device */
+	if (grace_period != USE_DEFAULT_GRACE_PERIOD) {
+		if (pm_update_grace_period(&dqm->packets,
+					USE_DEFAULT_GRACE_PERIOD))
+			pr_err("Failed to reset grace period\n");
+	}
+
 	pm_release_ib(&dqm->packets);
 	dqm->active_runlist = false;
 
@@ -1399,13 +1422,14 @@ static int unmap_queues_cpsch(struct device_queue_manager *dqm,
 /* dqm->lock mutex has to be locked before calling this function */
 static int execute_queues_cpsch(struct device_queue_manager *dqm,
 				enum kfd_unmap_queues_filter filter,
-				uint32_t filter_param)
+				uint32_t filter_param,
+				uint32_t grace_period)
 {
 	int retval;
 
 	if (dqm->is_hws_hang)
 		return -EIO;
-	retval = unmap_queues_cpsch(dqm, filter, filter_param);
+	retval = unmap_queues_cpsch(dqm, filter, filter_param, grace_period);
 	if (retval) {
 		pr_err("The cp might be in an unrecoverable state due to an unsuccessful queues preemption\n");
 		dqm->is_hws_hang = true;
@@ -1456,7 +1480,8 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 	if (q->properties.is_active) {
 		dqm->queue_count--;
 		retval = execute_queues_cpsch(dqm,
-				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+				USE_DEFAULT_GRACE_PERIOD);
 		if (retval == -ETIME)
 			qpd->reset_wavefronts = true;
 	}
@@ -1688,7 +1713,8 @@ static int process_termination_cpsch(struct device_queue_manager *dqm,
 		}
 	}
 
-	retval = execute_queues_cpsch(dqm, filter, 0);
+	retval = execute_queues_cpsch(dqm, filter, 0,
+			USE_DEFAULT_GRACE_PERIOD);
 	if ((!dqm->is_hws_hang) && (retval || qpd->reset_wavefronts)) {
 		pr_warn("Resetting wave fronts (cpsch) on dev %p\n", dqm->dev);
 		dbgdev_wave_reset_wavefronts(dqm->dev, qpd->pqm->process);
@@ -1939,7 +1965,8 @@ int reserve_debug_trap_vmid(struct device_queue_manager *dqm)
 		goto out_unlock;
 	}
 
-	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,
+			USE_DEFAULT_GRACE_PERIOD);
 	if (r)
 		goto out_unlock;
 
@@ -1985,7 +2012,8 @@ int release_debug_trap_vmid(struct device_queue_manager *dqm)
 		goto out_unlock;
 	}
 
-	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+	r = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,
+			USE_DEFAULT_GRACE_PERIOD);
 	if (r)
 		goto out_unlock;
 
@@ -2116,7 +2144,8 @@ int suspend_queues(struct kfd_process *p,
 
 		if (queues_suspended_on_device) {
 			r = execute_queues_cpsch(dqm,
-				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0);
+				KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,
+				grace_period);
 			if (r) {
 				pr_err("Failed to suspend process queues.\n");
 				dqm_unlock(dqm);
@@ -2187,7 +2216,8 @@ int resume_queues(struct kfd_process *p,
 		if (queues_resumed_on_device) {
 			r = execute_queues_cpsch(dqm,
 					KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES,
-					0);
+					0,
+					USE_DEFAULT_GRACE_PERIOD);
 			if (r) {
 				pr_err("Failed to resume process queues\n");
 				dqm_unlock(dqm);
@@ -2290,7 +2320,8 @@ int dqm_debugfs_execute_queues(struct device_queue_manager *dqm)
 
 	dqm_lock(dqm);
 	dqm->active_runlist = true;
-	r = execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0);
+	r = execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,
+			USE_DEFAULT_GRACE_PERIOD);
 	dqm_unlock(dqm);
 
 	return r;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
index 90339e5a6fa9..fcab7ad80512 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.h
@@ -32,6 +32,8 @@
 #include "kfd_mqd_manager.h"
 
 
+#define USE_DEFAULT_GRACE_PERIOD 0xffffffff
+
 struct device_process_node {
 	struct qcm_process_device *qpd;
 	struct list_head list;
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
index 9da582786657..d315f4dc2f8d 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_kernel_queue_v9.c
@@ -223,6 +223,9 @@ static int pm_set_grace_period_v9(struct packet_manager *pm,
 			&reg_offset,
 			&reg_data);
 
+	if (grace_period == USE_DEFAULT_GRACE_PERIOD)
+		reg_data = pm->dqm->wait_times;
+
 	packet = (struct pm4_mec_write_data_mmio *)buffer;
 	memset(buffer, 0, sizeof(struct pm4_mec_write_data_mmio));
 
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
index ccf6b2310316..73b93286af3b 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c
@@ -369,6 +369,38 @@ int pm_send_query_status(struct packet_manager *pm, uint64_t fence_address,
 	return retval;
 }
 
+int pm_update_grace_period(struct packet_manager *pm, uint32_t grace_period)
+{
+	int retval = 0;
+	uint32_t *buffer, size;
+
+	size = pm->pmf->set_grace_period_size;
+
+	mutex_lock(&pm->lock);
+
+	if (size) {
+		pm->priv_queue->ops.acquire_packet_buffer(pm->priv_queue,
+			size / sizeof(uint32_t),
+			(unsigned int **)&buffer);
+
+		if (!buffer) {
+			pr_err("Failed to allocate buffer on kernel queue\n");
+			retval = -ENOMEM;
+			goto out;
+		}
+
+		retval = pm->pmf->set_grace_period(pm, buffer, grace_period);
+		if (!retval)
+			pm->priv_queue->ops.submit_packet(pm->priv_queue);
+		else
+			pm->priv_queue->ops.rollback_packet(pm->priv_queue);
+	}
+
+out:
+	mutex_unlock(&pm->lock);
+	return retval;
+}
+
 int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
 			enum kfd_unmap_queues_filter filter,
 			uint32_t filter_param, bool reset,
diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
index afbe9b3d7fda..b227f3b2ba9c 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_priv.h
@@ -1112,6 +1112,8 @@ int pm_send_unmap_queue(struct packet_manager *pm, enum kfd_queue_type type,
 
 void pm_release_ib(struct packet_manager *pm);
 
+int pm_update_grace_period(struct packet_manager *pm, uint32_t grace_period);
+
 /* Following PM funcs can be shared among VI and AI */
 unsigned int pm_build_pm4_header(unsigned int opcode, size_t packet_size);
 int pm_set_resources_vi(struct packet_manager *pm, uint32_t *buffer,
-- 
2.17.1

